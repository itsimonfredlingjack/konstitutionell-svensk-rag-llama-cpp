[Unit]
Description=Constitutional AI LLM (llama-server with Ministral-3-14B)
After=network.target

[Service]
Type=simple
WorkingDirectory=/home/ai-server/AN-FOR-NO-ASSHOLES/09_CONSTITUTIONAL-AI
Environment=LD_LIBRARY_PATH=/home/ai-server/AN-FOR-NO-ASSHOLES/09_CONSTITUTIONAL-AI/llama.cpp/build/bin
ExecStart=/home/ai-server/AN-FOR-NO-ASSHOLES/09_CONSTITUTIONAL-AI/llama.cpp/build/bin/llama-server \
    -m models/Ministral-3-14B-Instruct-2512-Q4_K_M.gguf \
    -c 16384 \
    -ngl 99 \
    -ctk q8_0 -ctv q8_0 \
    --port 8080 \
    --host 0.0.0.0 \
    --ctx-size 16384 \
    --parallel 2 \
    -fa on \
    --spec-type ngram-simple --draft-max 64
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

# Give model time to unload on stop
TimeoutStopSec=30
KillMode=mixed

[Install]
WantedBy=default.target
