# ğŸ”„ Ã–verlÃ¤mning: VÃ¤ck liv i AI-modellerna

**Datum:** 2025-12-02
**FÃ¶regÃ¥ende session:** Polish features fÃ¶r Kiosk Dashboard (auto-rotate, ripple, ljud, lazy-load)

---

## ğŸ“Š NulÃ¤ge

### Backend (FastAPI) - FUNGERAR âœ…
- **URL:** http://192.168.86.32:8900
- **Docs:** http://192.168.86.32:8900/docs
- **WebSocket:** ws://192.168.86.32:8900/api/chat

### Ollama-modeller installerade
```
sven-gpt:latest    13 GB   â† GPT-OSS 20B (custom Modelfile) - anvÃ¤nds i Ollama
gpt-oss:20b        13 GB   â† Basmodell
devstral:24b       14 GB   â† Mistral kodmodell
```

**OBS:** Profile-IDs i backend/frontend Ã¤r `gpt-oss` och `devstral`, men Ollama-modellnamnet fÃ¶r GPT-OSS Ã¤r `sven-gpt`.

### Frontend (Vite) - FUNGERAR âœ…
- **URL:** http://192.168.86.32:5173
- **Kiosk:** http://192.168.86.32:5173/kiosk

---

## ğŸ—ï¸ Arkitektur

```
Frontend (React)
    â†“ WebSocket
Backend_Chat_Stream.py  â† Huvud-WebSocket endpoint
    â†“
Backend_Fraga_Router.py â† Routar till rÃ¤tt agent/modell
    â†“
Backend_Agent_Prompts.py â† System prompts per agent
    â†“
ollama_client.py â† Ollama API-anrop
```

### Nyckelfilar
| Fil | Ansvar |
|-----|--------|
| `app/api/Backend_Chat_Stream.py` | WebSocket, streaming, GPU telemetry |
| `app/services/Backend_Fraga_Router.py` | Agent routing logic |
| `app/models/Backend_Agent_Prompts.py` | Profile definitions & prompts |
| `app/services/ollama_client.py` | Ollama HTTP client |
| `app/services/gpu_monitor.py` | nvidia-smi parsing |

---

## ğŸ¤– Agenter (Profiles)

### Nuvarande konfiguration
```python
# FrÃ¥n /api/profiles endpoint:
"gpt-oss"    â†’ model: "sven-gpt"   (13GB)  # Arkitekten (GPT-OSS 20B)
"devstral"   â†’ model: "devstral"   (14GB)  # Kodaren (Devstral 24B)
"qwen"       â†’ model: "sven-gpt"   (Legacy)
```

### Kiosk Dashboard visar
- **GPT-OSS** (cyan) - id: `gpt-oss`
- **Devstral** (gold) - id: `devstral`

âœ… **Status:** Frontend och backend anvÃ¤nder nu samma profile-IDs (gpt-oss, devstral)

---

## ğŸ”Œ API Endpoints att testa

```bash
# Health check
curl http://192.168.86.32:8900/health

# GPU stats
curl http://192.168.86.32:8900/api/gpu/stats

# Lista profiler
curl http://192.168.86.32:8900/api/profiles

# Warmup en modell (ladda i VRAM)
curl -X POST http://192.168.86.32:8900/api/profiles/gpt-oss/warmup
curl -X POST http://192.168.86.32:8900/api/profiles/devstral/warmup

# Unload modeller (frigÃ¶r VRAM)
curl -X POST http://192.168.86.32:8900/api/system/unload-models

# WebSocket test (behÃ¶ver wscat eller liknande)
wscat -c ws://192.168.86.32:8900/api/chat
```

---

## ğŸ¯ Uppgifter fÃ¶r nÃ¤sta session

### 1. Synka Frontend â†” Backend profiles
- Kolla `frontend/src/config/KioskConfig.ts`
- Matcha agent IDs med backend `/api/profiles`
- Uppdatera `KIOSK_AGENTS` om nÃ¶dvÃ¤ndigt

### 2. Verifiera warmup-flÃ¶det
```
Kiosk "Tap to Load" â†’ handleAgentSelect()
  â†’ POST /api/profiles/{id}/warmup  (ex: gpt-oss eller devstral)
  â†’ ollama_client.warmup_model()
  â†’ Ollama laddar modell i VRAM (sven-gpt eller devstral)
```

### 3. Testa chat-streaming
```
Frontend skickar via WebSocket:
{
  "type": "chat_message",
  "content": "Hej!",
  "profile": "gpt-oss"
}

Backend svarar med streaming tokens:
{
  "type": "stream_token",
  "token": "Hej",
  "agent_id": "gpt-oss"
}
```

### 4. Koppla modeller i Kiosk
- Testa att trycka pÃ¥ agent i Kiosk
- Verifiera att modell faktiskt laddas (`ollama ps`)
- Kolla att `is_active` uppdateras i frontend

---

## ğŸ› ï¸ Debug-kommandon

```bash
# Se vad som kÃ¶rs i Ollama
ollama ps

# Se alla modeller
ollama list

# Testa modeller direkt i Ollama
ollama run sven-gpt "Hej, vem Ã¤r du?"  # GPT-OSS 20B
ollama run devstral "Write a Python function"  # Devstral 24B

# Backend logs
journalctl -u simons-ai -f

# Frontend dev server
cd frontend && npm run dev
```

---

## ğŸ“ Projektstruktur

```
/home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/
â”œâ”€â”€ app/                    # FastAPI backend
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ Backend_Chat_Stream.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ Backend_Agent_Prompts.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ Backend_Fraga_Router.py
â”‚   â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â”‚   â””â”€â”€ gpu_monitor.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/               # Vite React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ KioskDashboard.tsx      # â† Kiosk huvudfil
â”‚   â”‚   â”œâ”€â”€ config/KioskConfig.ts   # â† Agent definitions
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ SystemControlCard.tsx
â”‚   â””â”€â”€ dist/               # Built files
â””â”€â”€ docs/
    â””â”€â”€ HANDOVER_AI_MODELS.md  # â† Denna fil
```

---

## ğŸ’¡ Tips

1. **KÃ¶r alltid `ollama ps`** fÃ¶r att se vilka modeller som Ã¤r laddade
2. **VRAM Ã¤r 12GB** - bara en 13-14GB modell Ã¥t gÃ¥ngen
3. **WebSocket reconnect** - Frontend har auto-reconnect efter 3s
4. **Kiosk auto-rotate** - StÃ¤ngs av vid touch, Ã¥teraktiveras efter 30s

Lycka till! ğŸš€
