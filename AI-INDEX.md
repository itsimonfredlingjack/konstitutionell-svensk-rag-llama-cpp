# Constitutional AI - AI Index

> Denna fil Ã¤r designad fÃ¶r AI-modeller att fÃ¶rstÃ¥ projektstrukturen snabbt.

## Projektets Syfte

Constitutional AI Ã¤r ett RAG-system (Retrieval-Augmented Generation) fÃ¶r svenska myndighetsdokument med:
- 1.37M+ dokument (538K legal/gov + 829K DiVA research)
- ChromaDB som vector database
- llama-server (llama.cpp) fÃ¶r lokal LLM-inferens (Ollama som fallback)
- FastAPI backend + React frontend

## Viktiga Filer fÃ¶r AI-fÃ¶rstÃ¥else

### 1. SystemÃ¶versikt (START HÃ„R)
**Fil**: `docs/system-overview.md`
**InnehÃ¥ll**: Arkitektur, services, collections, key files

### 2. Backend Status
**Fil**: `docs/BACKEND_STATUS.md`
**InnehÃ¥ll**: Service status, endpoints, system commands

### 3. API Dokumentation
**Fil**: `apps/constitutional-dashboard/CONSTITUTIONAL_API.md`
**InnehÃ¥ll**: Alla API endpoints med exempel

### 4. Modelloptimering
**Fil**: `docs/MODEL_OPTIMIZATION.md`
**InnehÃ¥ll**: System prompts, modellparametrar, optimering

### 5. Agent Guardrails
**Fil**: `docs/guardrails.md`
**InnehÃ¥ll**: Regler fÃ¶r AI-agenter som arbetar med projektet

## Kodstruktur

### Backend (`backend/`)
- `app/main.py` - FastAPI application entry point
- `app/api/constitutional_routes.py` - API routes (550+ lines)
- `app/services/orchestrator_service.py` - RAG orchestration
- `app/services/retrieval_service.py` - ChromaDB retrieval
- `app/services/llm_service.py` - llama-server integration (Ollama fallback)

### Frontend (`apps/`)
- `constitutional-gpt/` - Main RAG interface (Next.js 16)
- `constitutional-dashboard/` - Metrics dashboard (Vite + React)

### Scrapers (`scrapers/`)
- ~100 Python-filer fÃ¶r web scraping
- Riksdagen, myndigheter, kommuner

## Data Flow

```
User Query â†’ Frontend â†’ Backend API â†’ Orchestrator
    â†“
Retrieval Service â†’ ChromaDB (1.37M+ docs)
    â†“
LLM Service â†’ llama-server (Mistral-Nemo-Instruct-2407-Q5_K_M.gguf)
    â†“
Response â†’ Frontend â†’ User
```

## Viktiga Konfigurationer

- **ChromaDB Path**: Konfigureras i `backend/app/config.py` (data exkluderas frÃ¥n git)
- **LLM Models**: Mistral-Nemo-Instruct-2407-Q5_K_M.gguf via llama-server (port 8080), gpt-sw3 (fallback)
- **Embedding Model**: BAAI/bge-m3 (1024 dimensions)
- **Reranker**: BAAI/bge-reranker-v2-m3
- **API Port**: 8900
- **Systemd Service**: `constitutional-ai-backend`
- **CRAG**: Enabled (self-reflection + grading active)

## FÃ¶r AI-modeller som ska arbeta med projektet

1. **LÃ¤s fÃ¶rst**: `docs/system-overview.md` och `docs/BACKEND_STATUS.md`
2. **FÃ¶r API-Ã¤ndringar**: Se `docs/guardrails.md` â†’ Route Discovery
3. **FÃ¶r modellÃ¤ndringar**: Se `docs/MODEL_OPTIMIZATION.md`
4. **FÃ¶r kodstil**: Se `CONTRIBUTING.md`

## Vanliga Uppgifter

- **LÃ¤gg till endpoint**: Se `docs/guardrails.md` â†’ Route Discovery
- **Ã„ndra modellparametrar**: Se `docs/MODEL_OPTIMIZATION.md`
- **Uppdatera dokumentation**: Uppdatera relevant fil i `docs/`
- **Testa backend**: `curl http://localhost:8900/api/constitutional/health`

## Projektstruktur (High-Level)

```
09_CONSTITUTIONAL-AI/
â”œâ”€â”€ backend/              # FastAPI backend (port 8900)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # API routes
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic (12 services)
â”‚   â”‚   â”œâ”€â”€ core/         # Exceptions, error handlers
â”‚   â”‚   â””â”€â”€ main.py        # FastAPI entry point
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ constitutional-gpt/      # Main RAG interface
â”‚   â””â”€â”€ constitutional-dashboard/ # Metrics dashboard
â”œâ”€â”€ docs/                 # Dokumentation
â”‚   â”œâ”€â”€ system-overview.md
â”‚   â”œâ”€â”€ BACKEND_STATUS.md
â”‚   â”œâ”€â”€ guardrails.md
â”‚   â””â”€â”€ MODEL_OPTIMIZATION.md
â”œâ”€â”€ scrapers/            # Web scrapers (~100 files)
â”œâ”€â”€ indexers/            # ChromaDB indexing scripts
â””â”€â”€ AI-INDEX.md          # Denna fil
```

## Services & Ports

| Service | Port | Status | Purpose |
|--------|------|--------|---------|
| Constitutional AI Backend | 8900 | ðŸŸ¢ Active | FastAPI RAG API |
| llama-server | 8080 | ðŸŸ¢ Running | Local LLM inference (Mistral-Nemo) |
| Ollama (fallback) | 11434 | Optional | Fallback LLM inference |

## API Endpoints (Key)

- `GET /api/constitutional/health` - Health check
- `POST /api/constitutional/agent/query` - RAG query
- `GET /api/constitutional/stats/overview` - Statistics
- `GET /api/constitutional/collections` - List collections

## Teknisk Stack

- **Backend**: FastAPI (Python 3.14)
- **Frontend**: React + TypeScript + Vite / Next.js 16
- **Vector DB**: ChromaDB (1.37M+ dokument: 538K legal/gov + 829K DiVA research, exkluderas frÃ¥n git)
- **LLM**: Mistral-Nemo-Instruct-2407-Q5_K_M.gguf via llama-server (port 8080), gpt-sw3 (fallback)
- **Embeddings**: BAAI/bge-m3 (1024 dimensions)
- **Reranker**: BAAI/bge-reranker-v2-m3

## Viktiga Noteringar

- **Data exkluderas**: `chromadb_data/`, `pdf_cache/`, `backups/` Ã¤r stora (16GB+) och exkluderas frÃ¥n git
- **Secrets**: AnvÃ¤nd environment variables, aldrig hardcode API keys
- **Systemd**: Backend kÃ¶rs som `constitutional-ai-backend` service
- **Dokumentation**: Alla viktiga filer finns i `docs/` mappen
