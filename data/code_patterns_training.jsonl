{"instruction": "Implementera en Python-funktion: Get cached settings instance", "output": "```python\ndef get_settings() -> Settings:\n    \"\"\"Get cached settings instance\"\"\"\n    return Settings()\n```\n\nDenna funktion finns i `app/config.py`."}
{"instruction": "Implementera en Python-klass: Application settings with environment variable support", "output": "```python\nclass Settings(BaseSettings):\n    \"\"\"Application settings with environment variable support\"\"\"\n\n    # Application\n    app_name: str = \"Simons AI Backend\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n\n    # Server\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n\n    # Ollama (local)\n    ollama_base_url: str = \"http://localhost:11434\"\n    ollama_timeout_seconds: int = 120\n\n    # Gemini (cloud - FAST mode)\n    gemini_api_key: Optional[str] = None\n    gemini_model: str = \"gemini-2.0-flash-exp\"\n\n    # Hybrid Orchestrator\n    default_mode: str = \"auto\"  # auto, fast, deep\n\n    # CORS - explicit origins needed when allow_credentials=True\n    cors_origins: list[str] = [\n        \"http://localhost:5173\",\n        \"http://127.0.0.1:5173\",\n        \"http://192.168.86.32:5173\",\n        \"http://localhost:8000\",\n        \"http://127.0.0.1:8000\",\n        \"http://192.168.86.32:8000\",\n    ]\n    cors_allow_credentials: bool = True\n\n    # Logging\n    log_level: str = \"INFO\"\n    log_json: bool = False\n    log_file: Optional[str] = None\n\n    # WebSocket\n    ws_heartbeat_interval: int = 30  # seconds\n    ws_max_message_size: int = 65536  # bytes\n\n    # Model defaults\n    default_profile: str = \"chill\"\n    warmup_on_startup: bool = False\n    warmup_profile: str = \"chill\"\n\n    class Config:\n        env_prefix = \"LLM_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n```\n\nKlassen finns i `app/config.py`."}
{"instruction": "Skapa en Python-klass för config", "output": "```python\n    class Config:\n        env_prefix = \"LLM_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n```\n\nKlassen finns i `app/config.py`."}
{"instruction": "Implementera en Python-funktion: Application lifespan manager", "output": "```python\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Application lifespan manager.\n    Handles startup and shutdown events.\n    \"\"\"\n    # Startup\n    logger.info(\"=\" * 60)\n    logger.info(f\"  {settings.app_name} v{settings.app_version}\")\n    logger.info(\"=\" * 60)\n\n    # Check Ollama connection\n    ollama_ok = await ollama_client.is_connected()\n    if ollama_ok:\n        logger.info(\"Ollama connection: OK\")\n        models = await ollama_client.list_models()\n        logger.info(f\"Available models: {', '.join(models) if models else 'None'}\")\n    else:\n        logger.warning(\"Ollama connection: FAILED - Start with 'ollama serve'\")\n\n    # Check GPU\n    gpu_ok = await gpu_monitor.is_gpu_available()\n    if gpu_ok:\n        stats = await gpu_monitor.get_stats()\n        logger.info(f\"GPU: {stats.name}\")\n        logger.info(f\"VRAM: {stats.vram_used_gb:.1f}/{stats.vram_total_gb:.1f} GB\")\n    else:\n        logger.warning(\"GPU monitoring: FAILED - nvidia-smi not available\")\n\n    # Optional warmup\n    if settings.warmup_on_startup and ollama_ok:\n        from .models.Backend_Agent_Prompts import get_profile\n        profile = get_profile(settings.warmup_profile)\n        logger.info(f\"Warming up model: {profile.model}\")\n        await ollama_client.warmup_model(profile)\n\n    logger.info(f\"Server starting on http://{settings.host}:{settings.port}\")\n    logger.info(\"=\" * 60)\n\n    yield\n\n    # Shutdown\n    logger.info(\"Shutting down...\")\n    stop_gpu_broadcast()  # Stop GPU telemetry broadcast\n    stop_status_pulse()   # Stop status pulse\n    await ollama_client.close()\n```\n\nDenna funktion finns i `app/main.py`."}
{"instruction": "Implementera en Python-funktion: Root endpoint - basic info", "output": "```python\nasync def root():\n    \"\"\"Root endpoint - basic info\"\"\"\n    return {\n        \"name\": settings.app_name,\n        \"version\": settings.app_version,\n        \"docs\": \"/docs\",\n        \"health\": \"/api/health\",\n        \"profiles\": \"/api/profiles\",\n        \"gpu\": \"/api/gpu/stats\",\n        \"websocket\": \"ws://localhost:8000/api/chat\",\n        \"cascade\": \"ws://localhost:8000/api/cascade\",\n    }\n```\n\nDenna funktion finns i `app/main.py`."}
{"instruction": "Implementera en Python-funktion: Configure logging for the application", "output": "```python\ndef setup_logging(\n    level: str = \"INFO\",\n    json_output: bool = False,\n    log_file: Optional[str] = None\n) -> None:\n    \"\"\"\n    Configure logging for the application.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR)\n        json_output: Use JSON format instead of colored console\n        log_file: Optional file path for logging\n    \"\"\"\n    root_logger = logging.getLogger()\n    root_logger.setLevel(getattr(logging, level.upper()))\n\n    # Clear existing handlers\n    root_logger.handlers.clear()\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    if json_output:\n        console_handler.setFormatter(JSONFormatter())\n    else:\n        console_handler.setFormatter(ColoredFormatter(\n            fmt=\"%(timestamp)s │ %(levelname)-8s │ %(module_name)-15s │ %(message)s\"\n        ))\n\n    root_logger.addHandler(console_handler)\n\n    # File handler (if specified)\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(JSONFormatter())\n        root_logger.addHandler(file_handler)\n\n    # Reduce noise from third-party libraries\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n    logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-funktion: Get a logger instance with the given name", "output": "```python\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"\n    Get a logger instance with the given name.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som format", "output": "```python\n    def format(self, record: logging.LogRecord) -> str:\n        # Add color to level name\n        level_color = self.COLORS.get(record.levelname, \"\")\n        record.levelname = f\"{level_color}{record.levelname}{self.RESET}\"\n\n        # Add timestamp\n        record.timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Format module name\n        record.module_name = record.name.split(\".\")[-1]\n\n        return super().format(record)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som debug", "output": "```python\n    def debug(self, msg: str, **kwargs) -> None:\n        self._log(logging.DEBUG, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som info", "output": "```python\n    def info(self, msg: str, **kwargs) -> None:\n        self._log(logging.INFO, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som warning", "output": "```python\n    def warning(self, msg: str, **kwargs) -> None:\n        self._log(logging.WARNING, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som error", "output": "```python\n    def error(self, msg: str, **kwargs) -> None:\n        self._log(logging.ERROR, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: Custom formatter with colors for console output", "output": "```python\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Custom formatter with colors for console output\"\"\"\n\n    # ANSI color codes\n    COLORS = {\n        \"DEBUG\": \"\\033[36m\",    # Cyan\n        \"INFO\": \"\\033[32m\",     # Green\n        \"WARNING\": \"\\033[33m\",  # Yellow\n        \"ERROR\": \"\\033[31m\",    # Red\n        \"CRITICAL\": \"\\033[35m\", # Magenta\n    }\n    RESET = \"\\033[0m\"\n    BOLD = \"\\033[1m\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        # Add color to level name\n        level_color = self.COLORS.get(record.levelname, \"\")\n        record.levelname = f\"{level_color}{record.levelname}{self.RESET}\"\n\n        # Add timestamp\n        record.timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Format module name\n        record.module_name = record.name.split(\".\")[-1]\n\n        return super().format(record)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: JSON formatter for production logging", "output": "```python\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for production logging\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        import json\n\n        log_data = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n        }\n\n        # Add extra fields if present\n        if hasattr(record, \"request_id\"):\n            log_data[\"request_id\"] = record.request_id\n        if hasattr(record, \"profile\"):\n            log_data[\"profile\"] = record.profile\n        if hasattr(record, \"model\"):\n            log_data[\"model\"] = record.model\n\n        # Add exception info if present\n        if record.exc_info:\n            log_data[\"exception\"] = self.formatException(record.exc_info)\n\n        return json.dumps(log_data)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: Context manager for request-scoped logging", "output": "```python\nclass RequestLogger:\n    \"\"\"Context manager for request-scoped logging\"\"\"\n\n    def __init__(self, logger: logging.Logger, request_id: str, **extra):\n        self.logger = logger\n        self.request_id = request_id\n        self.extra = extra\n\n    def _log(self, level: int, msg: str, **kwargs) -> None:\n        extra = {\"request_id\": self.request_id, **self.extra, **kwargs}\n        self.logger.log(level, f\"[{self.request_id[:8]}] {msg}\", extra=extra)\n\n    def debug(self, msg: str, **kwargs) -> None:\n        self._log(logging.DEBUG, msg, **kwargs)\n\n    def info(self, msg: str, **kwargs) -> None:\n        self._log(logging.INFO, msg, **kwargs)\n\n    def warning(self, msg: str, **kwargs) -> None:\n        self._log(logging.WARNING, msg, **kwargs)\n\n    def error(self, msg: str, **kwargs) -> None:\n        self._log(logging.ERROR, msg, **kwargs)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-funktion: Get profile by ID with fallback to QWEN-SONNET", "output": "```python\ndef get_profile(profile_id: str) -> ModelProfile:\n    \"\"\"Get profile by ID with fallback to QWEN-SONNET.\"\"\"\n    try:\n        return PROFILES[ProfileId(profile_id.lower())]\n    except (ValueError, KeyError):\n        return PROFILES[ProfileId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get list of all available profiles", "output": "```python\ndef get_all_profiles() -> list[ModelProfile]:\n    \"\"\"Get list of all available profiles.\"\"\"\n    return list(PROFILES.values())\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Find profile by Ollama model name", "output": "```python\ndef get_profile_by_model(model_name: str) -> Optional[ModelProfile]:\n    \"\"\"Find profile by Ollama model name.\"\"\"\n    for profile in PROFILES.values():\n        if profile.model == model_name:\n            return profile\n    return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get the default profile", "output": "```python\ndef get_default_profile() -> ModelProfile:\n    \"\"\"Get the default profile.\"\"\"\n    return PROFILES[ProfileId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Calculate total VRAM needed for all profiles loaded", "output": "```python\ndef calculate_total_vram() -> float:\n    \"\"\"Calculate total VRAM needed for all profiles loaded.\"\"\"\n    return sum(p.estimated_vram_gb for p in PROFILES.values())\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Skapa en Python-funktion som active profile", "output": "```python\n    def active_profile(self) -> Optional[ModelProfile]:\n        if self._active_profile:\n            return PROFILES.get(self._active_profile)\n        return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Resolve a profile ID to a ProfileId enum, or None if not found", "output": "```python\n    def _resolve_profile_id(self, profile_id) -> Optional[ProfileId]:\n        \"\"\"Resolve a profile ID to a ProfileId enum, or None if not found.\"\"\"\n        if isinstance(profile_id, ProfileId):\n            return profile_id\n        try:\n            return ProfileId(str(profile_id).lower())\n        except ValueError:\n            return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Mark a profile as loaded in VRAM", "output": "```python\n    def set_loaded(self, profile_id, loaded: bool = True) -> None:\n        \"\"\"Mark a profile as loaded in VRAM.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loaded = loaded\n        self._status[pid].is_loading = False\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get status for a specific profile", "output": "```python\n    def get_status(self, profile_id: ProfileId) -> ProfileStatus:\n        \"\"\"Get status for a specific profile.\"\"\"\n        return self._status[profile_id]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get status for all profiles", "output": "```python\n    def get_all_status(self) -> dict[ProfileId, ProfileStatus]:\n        \"\"\"Get status for all profiles.\"\"\"\n        return self._status.copy()\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get list of currently loaded profiles", "output": "```python\n    def get_loaded_profiles(self) -> list[ProfileId]:\n        \"\"\"Get list of currently loaded profiles.\"\"\"\n        return [pid for pid, status in self._status.items() if status.is_loaded]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Available profile identifiers", "output": "```python\nclass ProfileId(str, Enum):\n    \"\"\"Available profile identifiers\"\"\"\n    QWEN_SONNET = \"qwen-sonnet\"          # Daily Coder\n    DEVSTRAL_OPENHANDS = \"devstral-openhands\"  # Wave Primary\n    QWEN_OPENHANDS = \"qwen-openhands\"    # Wave Backup\n    # Legacy aliases\n    GPT_OSS = \"gpt-oss\"\n    DEVSTRAL = \"devstral\"\n    QWEN = \"qwen\"\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Configuration for an AI model profile (UI metadata only)", "output": "```python\nclass ModelProfile:\n    \"\"\"Configuration for an AI model profile (UI metadata only)\"\"\"\n    id: ProfileId\n    name: str\n    display_name: str\n    description: str\n    model: str  # Ollama model name\n    temperature: float\n    max_tokens: int\n    context_length: int\n    estimated_vram_gb: float\n    top_p: float = 0.9\n    repeat_penalty: float = 1.1\n    icon: str = \"robot\"\n    color: str = \"#FF4FD8\"\n    strengths: list[str] = None\n\n    def __post_init__(self):\n        if self.strengths is None:\n            self.strengths = []\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Runtime status for a profile", "output": "```python\nclass ProfileStatus:\n    \"\"\"Runtime status for a profile\"\"\"\n    profile_id: ProfileId\n    is_loaded: bool = False\n    is_loading: bool = False\n    last_used: Optional[str] = None\n    load_time_ms: Optional[int] = None\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Manages profile state for dual-model setup", "output": "```python\nclass ProfileManager:\n    \"\"\"Manages profile state for dual-model setup.\"\"\"\n\n    def __init__(self):\n        self._status: dict[ProfileId, ProfileStatus] = {\n            pid: ProfileStatus(profile_id=pid)\n            for pid in ProfileId\n        }\n        self._active_profile: Optional[ProfileId] = None\n\n    @property\n    def active_profile(self) -> Optional[ModelProfile]:\n        if self._active_profile:\n            return PROFILES.get(self._active_profile)\n        return None\n\n    def _resolve_profile_id(self, profile_id) -> Optional[ProfileId]:\n        \"\"\"Resolve a profile ID to a ProfileId enum, or None if not found.\"\"\"\n        if isinstance(profile_id, ProfileId):\n            return profile_id\n        try:\n            return ProfileId(str(profile_id).lower())\n        except ValueError:\n            return None\n\n    def set_loaded(self, profile_id, loaded: bool = True) -> None:\n        \"\"\"Mark a profile as loaded in VRAM.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loaded = loaded\n        self._status[pid].is_loading = False\n\n    def set_active(self, profile_id) -> None:\n        \"\"\"Mark a profile as currently active (being used).\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._active_profile = pid\n        self._status[pid].is_loaded = True\n        self._status[pid].is_loading = False\n\n    def set_loading(self, profile_id) -> None:\n        \"\"\"Mark a profile as currently loading.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loading = True\n\n    def get_status(self, profile_id: ProfileId) -> ProfileStatus:\n        \"\"\"Get status for a specific profile.\"\"\"\n        return self._status[profile_id]\n\n    def get_all_status(self) -> dict[ProfileId, ProfileStatus]:\n        \"\"\"Get status for all profiles.\"\"\"\n        return self._status.copy()\n\n    def get_loaded_profiles(self) -> list[ProfileId]:\n        \"\"\"Get list of currently loaded profiles.\"\"\"\n        return [pid for pid, status in self._status.items() if status.is_loaded]\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: WebSocket message types", "output": "```python\nclass WSMessageType(str, Enum):\n    \"\"\"WebSocket message types\"\"\"\n    # Client -> Server\n    CHAT = \"chat\"\n    PING = \"ping\"\n    SWITCH_PROFILE = \"switch_profile\"\n\n    # Server -> Client\n    START = \"start\"\n    TOKEN = \"token\"\n    DONE = \"done\"\n    ERROR = \"error\"\n    WARMUP = \"warmup\"\n    PONG = \"pong\"\n    PROFILE_CHANGED = \"profile_changed\"\n    GPU_UPDATE = \"gpu_update\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single chat message", "output": "```python\nclass ChatMessage(BaseModel):\n    \"\"\"Single chat message\"\"\"\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Chat request from client", "output": "```python\nclass ChatRequest(BaseModel):\n    \"\"\"Chat request from client\"\"\"\n    profile: str = Field(default=\"qwen\", description=\"Profile ID: qwen or gemma\")\n    mode: str = Field(default=\"auto\", description=\"Inference mode: auto, fast, deep\")\n    messages: list[ChatMessage] = Field(default_factory=list)\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"profile\": \"qwen\",\n                \"mode\": \"auto\",\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Förklara async/await i Python\"}\n                ],\n                \"request_id\": \"550e8400-e29b-41d4-a716-446655440000\"\n            }\n        }\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Statistics for completed chat response", "output": "```python\nclass ChatStats(BaseModel):\n    \"\"\"Statistics for completed chat response\"\"\"\n    tokens_generated: int\n    tokens_per_second: float\n    total_duration_ms: int\n    prompt_eval_count: Optional[int] = None\n    prompt_eval_duration_ms: Optional[int] = None\n    model: str\n    profile: str\n    provider: str = \"ollama\"  # ollama or gemini\n    mode: str = \"auto\"  # auto, fast, deep\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single token in streaming response", "output": "```python\nclass TokenChunk(BaseModel):\n    \"\"\"Single token in streaming response\"\"\"\n    type: Literal[\"token\"] = \"token\"\n    content: str\n    request_id: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Complete chat response (non-streaming)", "output": "```python\nclass ChatResponse(BaseModel):\n    \"\"\"Complete chat response (non-streaming)\"\"\"\n    type: Literal[\"done\"] = \"done\"\n    request_id: str\n    content: str\n    stats: ChatStats\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Sent when chat generation starts", "output": "```python\nclass WSStartMessage(BaseModel):\n    \"\"\"Sent when chat generation starts\"\"\"\n    type: Literal[\"start\"] = \"start\"\n    request_id: str\n    profile: str\n    model: str\n    display_name: str  # e.g., \"QWEN_NEXUS\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single streamed token", "output": "```python\nclass WSTokenMessage(BaseModel):\n    \"\"\"Single streamed token\"\"\"\n    type: Literal[\"token\"] = \"token\"\n    content: str\n    request_id: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Sent when generation completes", "output": "```python\nclass WSDoneMessage(BaseModel):\n    \"\"\"Sent when generation completes\"\"\"\n    type: Literal[\"done\"] = \"done\"\n    request_id: str\n    stats: ChatStats\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Error message", "output": "```python\nclass WSErrorMessage(BaseModel):\n    \"\"\"Error message\"\"\"\n    type: Literal[\"error\"] = \"error\"\n    request_id: str\n    code: str\n    message: str\n    retry_after_ms: Optional[int] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Model loading/warmup status", "output": "```python\nclass WSWarmupMessage(BaseModel):\n    \"\"\"Model loading/warmup status\"\"\"\n    type: Literal[\"warmup\"] = \"warmup\"\n    request_id: str\n    status: Literal[\"loading\", \"ready\", \"unloading\"]\n    model: str\n    progress_percent: Optional[int] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Generic WebSocket message wrapper", "output": "```python\nclass WSMessage(BaseModel):\n    \"\"\"Generic WebSocket message wrapper\"\"\"\n    type: WSMessageType\n    request_id: Optional[str] = None\n    data: Optional[dict] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Profile information for API response", "output": "```python\nclass ProfileInfo(BaseModel):\n    \"\"\"Profile information for API response\"\"\"\n    id: str\n    name: str\n    display_name: str\n    description: str\n    model: str\n    estimated_vram_gb: float\n    icon: str\n    color: str\n    strengths: list[str] = []\n    is_active: bool = False\n    is_loading: bool = False\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Response for GET /api/profiles", "output": "```python\nclass ProfilesResponse(BaseModel):\n    \"\"\"Response for GET /api/profiles\"\"\"\n    profiles: list[ProfileInfo]\n    active_profile: Optional[str] = None\n    default_profile: str = \"chill\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: GPU statistics from nvidia-smi", "output": "```python\nclass GPUStats(BaseModel):\n    \"\"\"GPU statistics from nvidia-smi\"\"\"\n    name: str = \"NVIDIA GeForce RTX 4070\"\n    vram_total_gb: float = 12.0\n    vram_used_gb: float = 0.0\n    vram_free_gb: float = 12.0\n    vram_percent: float = 0.0\n    temperature_c: int = 0\n    power_draw_w: int = 0\n    power_limit_w: int = 200\n    gpu_util_percent: int = 0\n    memory_util_percent: int = 0\n    fan_speed_percent: Optional[int] = None\n    is_available: bool = True\n    driver_version: Optional[str] = None\n    cuda_version: Optional[str] = None\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"name\": \"NVIDIA GeForce RTX 4070\",\n                \"vram_total_gb\": 12.0,\n                \"vram_used_gb\": 10.2,\n                \"vram_free_gb\": 1.8,\n                \"vram_percent\": 85.0,\n                \"temperature_c\": 58,\n                \"power_draw_w\": 185,\n                \"power_limit_w\": 200,\n                \"gpu_util_percent\": 95,\n                \"memory_util_percent\": 85,\n                \"is_available\": True\n            }\n        }\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Ollama service status", "output": "```python\nclass OllamaStatus(BaseModel):\n    \"\"\"Ollama service status\"\"\"\n    connected: bool = False\n    version: Optional[str] = None\n    models_available: list[str] = Field(default_factory=list)\n    models_loaded: list[str] = Field(default_factory=list)\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: System information for header display", "output": "```python\nclass SystemInfo(BaseModel):\n    \"\"\"System information for header display\"\"\"\n    ram_total_gb: float\n    ram_used_gb: float\n    cpu_percent: float\n    cpu_freq_ghz: Optional[float] = None\n    uptime_seconds: int\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-funktion: Extract JSON tool call from QWEN's response text", "output": "```python\ndef extract_tool_call(text: str) -> Optional[dict]:\n    \"\"\"\n    Extract JSON tool call from QWEN's response text.\n\n    QWEN will include tool calls like:\n        {\"tool\": \"docker_ps\"}\n        {\"tool\": \"docker_logs\", \"container\": \"nginx\", \"lines\": 50}\n\n    Returns:\n        dict with tool name and params, or None if no tool call found\n    \"\"\"\n    match = TOOL_JSON_PATTERN.search(text)\n    if not match:\n        return None\n\n    try:\n        data = json.loads(match.group())\n        if \"tool\" in data:\n            return data\n    except json.JSONDecodeError:\n        pass\n\n    return None\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Returns last 3 Q&A exchanges for dashboard display", "output": "```python\ndef get_active_context() -> list[dict]:\n    \"\"\"Returns last 3 Q&A exchanges for dashboard display\"\"\"\n    return [ex.to_dict() for ex in reversed(_mobile_context)]\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Add a completed Q&A exchange to the active context buffer", "output": "```python\ndef add_mobile_exchange(\n    request_id: str,\n    question: str,\n    answer: str,\n    profile: str,\n    stats: dict = None\n) -> None:\n    \"\"\"Add a completed Q&A exchange to the active context buffer\"\"\"\n    exchange = MobileExchange(\n        request_id=request_id,\n        question=question,\n        answer=answer,\n        profile=profile,\n        timestamp=datetime.utcnow().isoformat() + \"Z\",\n        stats=stats or {},\n    )\n    _mobile_context.append(exchange)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast mobile activity to all connected dashboard clients", "output": "```python\nasync def emit_mobile_activity(\n    event_type: str,\n    request_id: str,\n    data: dict\n) -> None:\n    \"\"\"\n    Broadcast mobile activity to all connected dashboard clients.\n\n    Event types:\n    - mobile_request_start: Mobile request initiated\n    - mobile_generation: Token generation progress\n    - mobile_response: Complete Q&A response\n    - mobile_shell: Shell command executed\n    \"\"\"\n    packet = {\n        \"type\": \"mobile_activity\",\n        \"event\": event_type,\n        \"request_id\": request_id,\n        \"source\": \"mobile\",\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n        **data\n    }\n    await manager.broadcast(packet)\n    logger.debug(f\"Mobile activity broadcast: {event_type} ({request_id})\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast War Room events to all dashboard clients for real-time sync", "output": "```python\nasync def emit_war_room_event(\n    event_type: str,\n    data: dict\n) -> None:\n    \"\"\"\n    Broadcast War Room events to all dashboard clients for real-time sync.\n\n    Event types:\n    - start: Mobile/WebSocket request initiated (shows user prompt)\n    - stream: Batched tokens (100ms intervals for smooth display)\n    - end: Response complete (shows stats)\n\n    Used by KioskDashboard to display mobile sessions in real-time.\n    \"\"\"\n    packet = {\n        \"type\": \"war_room_event\",\n        \"event\": event_type,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n        **data\n    }\n    await manager.broadcast(packet)\n    logger.debug(f\"War Room broadcast: {event_type}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Emit a system log event to all connected clients", "output": "```python\nasync def emit_system_log(\n    event: str,\n    message: str,\n    level: str = \"info\",\n    agent_id: str | None = None\n) -> None:\n    \"\"\"\n    Emit a system log event to all connected clients.\n\n    Args:\n        event: Event type (e.g., 'model_load', 'generation_start', 'error')\n        message: Human-readable log message\n        level: Log level ('info', 'warn', 'error', 'success')\n        agent_id: Optional agent that triggered this log\n    \"\"\"\n    global _system_logs\n\n    log_entry = {\n        \"type\": \"system_log\",\n        \"event\": event,\n        \"message\": message,\n        \"level\": level,\n        \"agent_id\": agent_id,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n\n    # Add to buffer (circular)\n    _system_logs.append(log_entry)\n    if len(_system_logs) > MAX_LOG_BUFFER:\n        _system_logs = _system_logs[-MAX_LOG_BUFFER:]\n\n    # Broadcast to all connections\n    await manager.broadcast(log_entry)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Get recent system logs for new connections", "output": "```python\ndef get_recent_logs(count: int = 20) -> list[dict]:\n    \"\"\"Get recent system logs for new connections\"\"\"\n    return _system_logs[-count:]\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Background task that broadcasts GPU stats to all connected clients", "output": "```python\nasync def gpu_telemetry_loop():\n    \"\"\"\n    Background task that broadcasts GPU stats to all connected clients.\n    Runs every GPU_BROADCAST_INTERVAL seconds.\n\n    Sends status_update packets with:\n    - GPU VRAM usage, temperature, utilization\n    - Number of active models in Ollama\n    \"\"\"\n    logger.info(\"GPU telemetry broadcast started\")\n\n    while True:\n        try:\n            await asyncio.sleep(GPU_BROADCAST_INTERVAL)\n\n            # Skip if no connections\n            if not manager.active_connections:\n                continue\n\n            # Fetch GPU stats\n            gpu_stats = await gpu_monitor.get_stats()\n\n            # Fetch active models count\n            running_models = await ollama_client.list_running_models()\n            active_models_count = len(running_models)\n\n            # Build status_update packet (matches frontend expectations)\n            status_packet = {\n                \"type\": \"status_update\",\n                \"gpu\": {\n                    \"vram_used_gb\": round(gpu_stats.vram_used_gb, 2),\n                    \"vram_total_gb\": round(gpu_stats.vram_total_gb, 2),\n                    \"vram_percent\": round(gpu_stats.vram_percent, 1),\n                    \"temperature_c\": gpu_stats.temperature_c,\n                    \"utilization_percent\": gpu_stats.gpu_util_percent,\n                    \"power_draw_w\": gpu_stats.power_draw_w,\n                    \"power_limit_w\": gpu_stats.power_limit_w,\n                    \"name\": gpu_stats.name,\n                    \"is_available\": gpu_stats.is_available,\n                },\n                \"active_models_count\": active_models_count,\n                \"active_models\": [m.get(\"name\", \"\") for m in running_models],\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n            # Broadcast to all connections\n            await manager.broadcast(status_packet)\n\n        except asyncio.CancelledError:\n            logger.info(\"GPU telemetry broadcast stopped\")\n            break\n        except Exception as e:\n            logger.error(f\"GPU telemetry error: {e}\")\n            # Continue loop despite errors\n            await asyncio.sleep(GPU_BROADCAST_INTERVAL)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Start the GPU telemetry background task", "output": "```python\ndef start_gpu_broadcast():\n    \"\"\"Start the GPU telemetry background task\"\"\"\n    global _gpu_broadcast_task\n    if _gpu_broadcast_task is None or _gpu_broadcast_task.done():\n        _gpu_broadcast_task = asyncio.create_task(gpu_telemetry_loop())\n        logger.info(\"GPU telemetry broadcast task created\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Stop the GPU telemetry background task", "output": "```python\ndef stop_gpu_broadcast():\n    \"\"\"Stop the GPU telemetry background task\"\"\"\n    global _gpu_broadcast_task\n    if _gpu_broadcast_task and not _gpu_broadcast_task.done():\n        _gpu_broadcast_task.cancel()\n        logger.info(\"GPU telemetry broadcast task cancelled\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Start the status pulse background task", "output": "```python\ndef start_status_pulse():\n    \"\"\"Start the status pulse background task\"\"\"\n    global _status_pulse_task\n    if _status_pulse_task is None or _status_pulse_task.done():\n        _status_pulse_task = asyncio.create_task(status_pulse_loop())\n        logger.info(\"Status pulse task created\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Stop the status pulse background task", "output": "```python\ndef stop_status_pulse():\n    \"\"\"Stop the status pulse background task\"\"\"\n    global _status_pulse_task\n    if _status_pulse_task and not _status_pulse_task.done():\n        _status_pulse_task.cancel()\n        logger.info(\"Status pulse task cancelled\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send error message to client", "output": "```python\nasync def send_error(\n    websocket: WebSocket,\n    request_id: str,\n    code: str,\n    message: str,\n    retry_after_ms: Optional[int] = None\n) -> None:\n    \"\"\"Send error message to client\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.ERROR.value,\n        \"request_id\": request_id,\n        \"code\": code,\n        \"message\": message,\n        \"retry_after_ms\": retry_after_ms,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send model warmup status", "output": "```python\nasync def send_warmup(\n    websocket: WebSocket,\n    request_id: str,\n    model: str,\n    status: str,\n    progress: Optional[int] = None\n) -> None:\n    \"\"\"Send model warmup status\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.WARMUP.value,\n        \"request_id\": request_id,\n        \"model\": model,\n        \"status\": status,\n        \"progress_percent\": progress,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle incoming chat request with streaming response", "output": "```python\nasync def handle_chat_message(\n    websocket: WebSocket,\n    data: dict\n) -> None:\n    \"\"\"\n    Handle incoming chat request with streaming response.\n    Uses 4-Agent Orchestrator for profile-based routing.\n\n    Frontend sends profile: qwen/gemma/cloud/nerdy\n    Backend routes to correct model with appropriate system prompt.\n    Every packet includes agent_id for frontend panel highlighting.\n    \"\"\"\n    # Parse request\n    try:\n        request = ChatRequest(**data)\n    except Exception as e:\n        logger.error(f\"Invalid chat request: {e}\")\n        await send_error(\n            websocket,\n            data.get(\"request_id\", \"unknown\"),\n            \"INVALID_REQUEST\",\n            f\"Invalid request format: {str(e)}\"\n        )\n        return\n\n    # Get agent config for display info\n    agent = get_agent(request.profile)\n\n    req_logger = RequestLogger(logger, request.request_id, profile=request.profile)\n    req_logger.info(f\"Chat request - profile: {request.profile} → agent: {agent.display_name}\")\n\n    # Build messages list\n    messages = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n\n    # Start timing for latency tracking\n    request_start_time = time.time()\n    first_token_sent = False\n    latency_ms = None\n\n    # Send start message with agent_id for highlighting\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.START.value,\n        \"request_id\": request.request_id,\n        \"profile\": request.profile,\n        \"agent_id\": agent.id.value,  # For panel highlighting\n        \"model\": agent.model,\n        \"sender\": \"agent\",\n    })\n\n    try:\n        # Stream response via 4-agent orchestrator\n        final_stats = None\n        token_count = 0\n        active_agent_id = agent.id.value\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request.request_id,\n            profile=request.profile,  # Route by profile, not mode\n            temperature=None,  # Use agent's default\n            max_tokens=4096\n        ):\n            active_agent_id = agent_id\n\n            if token:\n                token_count += 1\n\n                # Calculate latency on first token\n                if not first_token_sent:\n                    latency_ms = int((time.time() - request_start_time) * 1000)\n                    first_token_sent = True\n                    req_logger.info(f\"TTFT: {latency_ms}ms\")\n\n                # Send token with agent_id for highlighting\n                token_packet = {\n                    \"type\": WSMessageType.TOKEN.value,\n                    \"content\": token,\n                    \"request_id\": request.request_id,\n                    \"agent_id\": active_agent_id,  # For panel highlighting\n                    \"sender\": \"agent\",\n                    \"text\": token,  # Antigravity format compatibility\n                    \"is_finished\": False,\n                }\n\n                # Include latency only on first token\n                if latency_ms is not None:\n                    token_packet[\"latency_ms\"] = latency_ms\n                    latency_ms = None  # Only send once\n\n                await manager.send_json(websocket, token_packet)\n\n            if stats:\n                final_stats = stats\n\n        # Send completion with full stats and agent_id\n        done_packet = {\n            \"type\": WSMessageType.DONE.value,\n            \"request_id\": request.request_id,\n            \"agent_id\": final_stats.agent_id if final_stats else active_agent_id,\n            \"sender\": \"agent\",\n            \"text\": \"\",\n            \"is_finished\": True,\n            \"model\": final_stats.model if final_stats else agent.model,\n            \"provider\": final_stats.provider if final_stats else agent.provider.value,\n        }\n\n        if final_stats:\n            done_packet[\"stats\"] = {\n                \"tokens_generated\": final_stats.tokens_generated,\n                \"tokens_per_second\": round(final_stats.tokens_per_second, 1),\n                \"total_duration_ms\": final_stats.total_duration_ms,\n                \"prompt_tokens\": final_stats.prompt_tokens,\n                \"model\": final_stats.model,\n                \"provider\": final_stats.provider,\n                \"agent_id\": final_stats.agent_id,\n                \"profile\": request.profile,\n            }\n\n        await manager.send_json(websocket, done_packet)\n\n        req_logger.info(\n            f\"Completed via {active_agent_id}: {token_count} tokens \"\n            f\"in {final_stats.total_duration_ms if final_stats else 0}ms\"\n        )\n\n    except OllamaConnectionError as e:\n        req_logger.error(f\"Connection error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"PROVIDER_DISCONNECTED\",\n            str(e),\n            retry_after_ms=5000\n        )\n\n    except OllamaModelNotFoundError as e:\n        req_logger.error(f\"Model not found: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"MODEL_NOT_FOUND\",\n            str(e)\n        )\n\n    except XAIError as e:\n        req_logger.error(f\"xAI error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"XAI_ERROR\",\n            str(e)\n        )\n\n    except OllamaTimeoutError as e:\n        req_logger.error(f\"Timeout: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"TIMEOUT\",\n            str(e),\n            retry_after_ms=10000\n        )\n\n    except Exception as e:\n        req_logger.error(f\"Unexpected error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"INTERNAL_ERROR\",\n            f\"An unexpected error occurred: {str(e)}\"\n        )\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle ping message", "output": "```python\nasync def handle_ping(websocket: WebSocket, data: dict) -> None:\n    \"\"\"Handle ping message\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.PONG.value,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle profile switch request", "output": "```python\nasync def handle_switch_profile(websocket: WebSocket, data: dict) -> None:\n    \"\"\"Handle profile switch request\"\"\"\n    profile_id = data.get(\"profile\", \"chill\")\n    profile = get_profile(profile_id)\n\n    # Optionally warmup the model\n    if data.get(\"warmup\", False):\n        await send_warmup(websocket, \"switch\", profile.model, \"loading\")\n        success = await ollama_client.warmup_model(profile)\n        status = \"ready\" if success else \"failed\"\n        await send_warmup(websocket, \"switch\", profile.model, status)\n\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.PROFILE_CHANGED.value,\n        \"profile\": profile.id.value,\n        \"display_name\": profile.display_name,\n        \"model\": profile.model,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle Antigravity/Robot Unicorn frontend protocol (legacy simple format)", "output": "```python\nasync def handle_antigravity_message(websocket: WebSocket, data: dict) -> None:\n    \"\"\"\n    Handle Antigravity/Robot Unicorn frontend protocol (legacy simple format).\n\n    Incoming format:\n        {\"text\": \"user message\", \"profile\": \"qwen\"|\"gemma\"|\"cloud\"|\"nerdy\"}\n\n    Outgoing format (streamed):\n        {\"sender\": \"agent\", \"text\": \"token...\", \"is_finished\": false, \"agent_id\": \"qwen\"}\n        {\"sender\": \"agent\", \"text\": \"\", \"is_finished\": true, \"stats\": {...}}\n\n    OPERATION VITAL SIGNS features:\n        - agent_id: Identifies which agent panel should highlight\n        - latency_ms: Time to first token (TTFT), sent with first token\n    \"\"\"\n    text = data.get(\"text\", \"\")\n    profile = data.get(\"profile\", \"qwen\")\n\n    if not text:\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": \"Error: No text provided\",\n            \"is_finished\": True,\n            \"error\": True\n        })\n        return\n\n    request_id = str(uuid.uuid4())\n    agent = get_agent(profile)\n    logger.info(f\"[{request_id}] Antigravity request - profile: {profile} → {agent.display_name}\")\n\n    # Emit system log for terminal\n    await emit_system_log(\n        \"request_start\",\n        f\"[{agent.display_name}] Processing request...\",\n        \"info\",\n        agent.id.value\n    )\n\n    # Build messages list (simple format - just the user message)\n    messages = [{\"role\": \"user\", \"content\": text}]\n\n    # Start timing for latency tracking\n    request_start_time = time.time()\n    first_token_sent = False\n\n    # WAR ROOM: Broadcast request start to all dashboards\n    await emit_war_room_event(\"start\", {\n        \"agent\": profile,\n        \"user_prompt\": text[:100],\n        \"request_id\": request_id,\n    })\n\n    try:\n        # Stream response via 4-agent orchestrator\n        full_response = []\n        final_stats = None\n        active_agent_id = agent.id.value\n        latency_ms = None\n\n        # WAR ROOM: 100ms token batching for smooth dashboard display\n        war_room_buffer = []\n        last_war_room_flush = time.time()\n        WAR_ROOM_FLUSH_INTERVAL = 0.1  # 100ms\n        WAR_ROOM_BUFFER_MAX = 50  # chars\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request_id,\n            profile=profile\n        ):\n            active_agent_id = agent_id\n\n            if token:\n                full_response.append(token)\n\n                # Calculate latency on first token\n                if not first_token_sent:\n                    latency_ms = int((time.time() - request_start_time) * 1000)\n                    first_token_sent = True\n                    logger.info(f\"[{request_id}] TTFT: {latency_ms}ms via {agent_id}\")\n\n                    # Emit system log for TTFT\n                    await emit_system_log(\n                        \"generation_start\",\n                        f\"[{agent.display_name}] Streaming response (TTFT: {latency_ms}ms)\",\n                        \"success\",\n                        agent_id\n                    )\n\n                # Stream each token to frontend with agent_id and latency\n                token_packet = {\n                    \"sender\": \"agent\",\n                    \"text\": token,\n                    \"is_finished\": False,\n                    \"model\": agent.model,\n                    \"provider\": agent.provider.value,\n                    \"agent_id\": active_agent_id,  # For dynamic highlighting\n                }\n\n                # Include latency only on first token\n                if latency_ms is not None:\n                    token_packet[\"latency_ms\"] = latency_ms\n                    latency_ms = None  # Only send once\n\n                await manager.send_json(websocket, token_packet)\n\n                # WAR ROOM: Add token to buffer\n                war_room_buffer.append(token)\n                buffer_text = \"\".join(war_room_buffer)\n                now = time.time()\n\n                # Flush if 100ms elapsed OR buffer > 50 chars\n                if (now - last_war_room_flush >= WAR_ROOM_FLUSH_INTERVAL) or len(buffer_text) > WAR_ROOM_BUFFER_MAX:\n                    await emit_war_room_event(\"stream\", {\n                        \"tokens\": buffer_text,\n                        \"request_id\": request_id,\n                    })\n                    war_room_buffer = []\n                    last_war_room_flush = now\n\n            if stats:\n                final_stats = stats\n\n        # WAR ROOM: Flush any remaining tokens in buffer\n        if war_room_buffer:\n            await emit_war_room_event(\"stream\", {\n                \"tokens\": \"\".join(war_room_buffer),\n                \"request_id\": request_id,\n            })\n\n        # WAR ROOM: Broadcast completion with stats\n        duration_ms = int((time.time() - request_start_time) * 1000)\n        await emit_war_room_event(\"end\", {\n            \"request_id\": request_id,\n            \"stats\": {\n                \"tokens\": len(full_response),\n                \"duration_ms\": duration_ms,\n            }\n        })\n\n        # =================================================================\n        # TOOL CALL DETECTION - Check if QWEN wants to run a tool\n        # =================================================================\n        full_text = \"\".join(full_response)\n        tool_call = extract_tool_call(full_text)\n\n        if tool_call and profile == \"qwen\":\n            tool_name = tool_call.get(\"tool\")\n            params = {k: v for k, v in tool_call.items() if k != \"tool\"}\n\n            tool_def = TOOLS.get(tool_name)\n\n            if tool_def:\n                logger.info(f\"[{request_id}] Tool call detected: {tool_name} ({tool_def.safety.value})\")\n\n                await emit_system_log(\n                    \"tool_call\",\n                    f\"[QWEN] Verktygsanrop: {tool_name}\",\n                    \"info\",\n                    \"qwen\"\n                )\n\n                if tool_def.safety == SafetyLevel.SAFE:\n                    # Execute SAFE tool directly\n                    result = await system_tools.execute(tool_name, params)\n\n                    # Send tool result to frontend\n                    await manager.send_json(websocket, {\n                        \"sender\": \"system\",\n                        \"tool_result\": {\n                            \"tool\": tool_name,\n                            \"success\": result.success,\n                            \"output\": result.output[:2000] if result.output else \"\",\n                            \"error\": result.error,\n                        },\n                        \"is_finished\": False\n                    })\n\n                    await emit_system_log(\n                        \"tool_executed\",\n                        f\"[SYSTEM] {tool_name}: {'OK' if result.success else 'FEL'}\",\n                        \"success\" if result.success else \"error\",\n                        \"qwen\"\n                    )\n\n                    # Let QWEN interpret the result\n                    follow_up_messages = [\n                        {\"role\": \"user\", \"content\": text},\n                        {\"role\": \"assistant\", \"content\": full_text},\n                        {\"role\": \"user\", \"content\": f\"Verktygsresultat för {tool_name}:\\n```\\n{result.output[:2000] if result.output else result.error}\\n```\\nTolka resultatet och ge en sammanfattning.\"}\n                    ]\n\n                    # Stream QWEN's interpretation\n                    async for token, stats, agent_id in orchestrator.chat_stream(\n                        messages=follow_up_messages,\n                        request_id=request_id + \"-followup\",\n                        profile=\"qwen\"\n                    ):\n                        if token:\n                            await manager.send_json(websocket, {\n                                \"sender\": \"agent\",\n                                \"text\": token,\n                                \"is_finished\": False,\n                                \"agent_id\": \"qwen\",\n                            })\n                        if stats:\n                            final_stats = stats\n\n                else:\n                    # DANGEROUS tool - require confirmation\n                    _pending_tool_confirmations[request_id] = {\n                        \"tool\": tool_name,\n                        \"params\": params,\n                        \"original_question\": text,\n                        \"original_response\": full_text,\n                        \"websocket\": websocket,\n                    }\n\n                    await manager.send_json(websocket, {\n                        \"sender\": \"system\",\n                        \"confirmation_required\": {\n                            \"request_id\": request_id,\n                            \"tool\": tool_name,\n                            \"params\": params,\n                            \"description\": tool_def.description,\n                            \"message\": f\"⚠️ QWEN vill köra: {tool_name}({params}). Godkänn?\"\n                        },\n                        \"is_finished\": False\n                    })\n\n                    await emit_system_log(\n                        \"tool_confirmation\",\n                        f\"[VÄNTANDE] {tool_name} kräver bekräftelse\",\n                        \"warn\",\n                        \"qwen\"\n                    )\n\n                    # Don't send final done message yet - wait for confirmation\n                    return\n\n        # Send completion message with full stats\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": \"\",\n            \"is_finished\": True,\n            \"model\": final_stats.model if final_stats else agent.model,\n            \"provider\": final_stats.provider if final_stats else agent.provider.value,\n            \"agent_id\": final_stats.agent_id if final_stats else active_agent_id,\n            \"stats\": {\n                \"tokens\": final_stats.tokens_generated if final_stats else 0,\n                \"speed\": round(final_stats.tokens_per_second, 1) if final_stats else 0,\n                \"duration_ms\": final_stats.total_duration_ms if final_stats else 0,\n                \"provider\": final_stats.provider if final_stats else agent.provider.value,\n                \"model\": final_stats.model if final_stats else agent.model,\n                \"agent_id\": final_stats.agent_id if final_stats else active_agent_id\n            }\n        })\n\n        logger.info(f\"[{request_id}] Antigravity complete via {active_agent_id}\")\n\n        # Emit completion log\n        tokens = final_stats.tokens_generated if final_stats else 0\n        speed = round(final_stats.tokens_per_second, 1) if final_stats else 0\n        await emit_system_log(\n            \"generation_complete\",\n            f\"[{agent.display_name}] Complete: {tokens} tokens @ {speed} tok/s\",\n            \"success\",\n            active_agent_id\n        )\n\n    except Exception as e:\n        logger.error(f\"[{request_id}] Antigravity error: {e}\")\n\n        # Emit error log\n        await emit_system_log(\n            \"error\",\n            f\"[ERROR] {str(e)[:50]}\",\n            \"error\",\n            agent.id.value if agent else None\n        )\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": f\"Error: {str(e)}\",\n            \"is_finished\": True,\n            \"error\": True\n        })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Main WebSocket endpoint for chat communication", "output": "```python\nasync def websocket_endpoint(websocket: WebSocket) -> None:\n    \"\"\"\n    Main WebSocket endpoint for chat communication.\n\n    Supports two protocols:\n    1. Legacy: {\"type\": \"chat\", \"messages\": [...], \"mode\": \"auto\"}\n    2. Antigravity: {\"text\": \"message\", \"mode\": \"auto\"}\n\n    Server responds with streaming tokens and status updates.\n\n    OPERATION VITAL SIGNS:\n    - Starts GPU telemetry broadcast when first client connects\n    - Broadcasts status_update packets every 2 seconds with GPU stats\n    \"\"\"\n    await manager.connect(websocket)\n\n    # Start GPU telemetry broadcast if not already running\n    start_gpu_broadcast()\n\n    # Start status pulse for Kiosk dashboard\n    start_status_pulse()\n\n    try:\n        while True:\n            # Receive message\n            try:\n                raw_data = await websocket.receive_text()\n                data = json.loads(raw_data)\n            except json.JSONDecodeError:\n                logger.warning(\"Received invalid JSON\")\n                continue\n\n            # Detect protocol format\n            if \"text\" in data and \"type\" not in data:\n                # Antigravity protocol: {\"text\": \"...\", \"mode\": \"...\"}\n                await handle_antigravity_message(websocket, data)\n            else:\n                # Legacy protocol with \"type\" field\n                message_type = data.get(\"type\", \"chat\")\n\n                if message_type == \"chat\":\n                    await handle_chat_message(websocket, data)\n                elif message_type == \"ping\":\n                    await handle_ping(websocket, data)\n                elif message_type == \"switch_profile\":\n                    await handle_switch_profile(websocket, data)\n                else:\n                    logger.warning(f\"Unknown message type: {message_type}\")\n\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: WebSocket endpoint för Multi-Agent Cascade (Planner → Coder → Reviewer)", "output": "```python\nasync def cascade_websocket_endpoint(websocket: WebSocket) -> None:\n    \"\"\"\n    WebSocket endpoint för Multi-Agent Cascade (Planner → Coder → Reviewer).\n\n    Protocol:\n        Send: {\"task\": \"Skriv en REST API...\", \"skip_review\": false}\n        Receive: {\"phase\": \"planning|coding|reviewing|complete\", \"agent_id\": \"...\", \"token\": \"...\"}\n    \"\"\"\n    from ..services.cascade_orchestrator import cascade_orchestrator, CascadePhase\n\n    await manager.connect(websocket)\n    logger.info(\"Cascade WebSocket connected\")\n\n    try:\n        while True:\n            raw_data = await websocket.receive_text()\n            data = json.loads(raw_data)\n\n            task = data.get(\"task\", \"\")\n            skip_review = data.get(\"skip_review\", False)\n\n            if not task:\n                await manager.send_json(websocket, {\n                    \"phase\": \"error\",\n                    \"error\": \"No task provided\"\n                })\n                continue\n\n            request_id = str(uuid.uuid4())\n            logger.info(f\"[{request_id}] Cascade started: {task[:100]}...\")\n\n            # Stream cascade phases\n            async for result in cascade_orchestrator.execute_cascade(\n                task=task,\n                request_id=request_id,\n                skip_review=skip_review\n            ):\n                packet = {\n                    \"phase\": result.phase.value,\n                    \"agent_id\": result.agent_id,\n                    \"token\": result.content,\n                }\n\n                if result.stats:\n                    packet[\"stats\"] = {\n                        \"tokens\": result.stats.tokens_generated,\n                        \"tokens_per_sec\": result.stats.tokens_per_second,\n                        \"duration_ms\": result.stats.total_duration_ms,\n                    }\n\n                await manager.send_json(websocket, packet)\n\n            logger.info(f\"[{request_id}] Cascade complete\")\n\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n    except Exception as e:\n        logger.error(f\"Cascade WebSocket error: {e}\")\n        manager.disconnect(websocket)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en Python-funktion som to dict", "output": "```python\n    def to_dict(self) -> dict:\n        return {\n            \"request_id\": self.request_id,\n            \"question\": self.question[:100] + \"...\" if len(self.question) > 100 else self.question,\n            \"answer\": self.answer[:200] + \"...\" if len(self.answer) > 200 else self.answer,\n            \"profile\": self.profile,\n            \"timestamp\": self.timestamp,\n            \"stats\": self.stats,\n        }\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en async Python-funktion som connect", "output": "```python\n    async def connect(self, websocket: WebSocket) -> None:\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        logger.info(f\"WebSocket connected. Active: {len(self.active_connections)}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en Python-funktion som disconnect", "output": "```python\n    def disconnect(self, websocket: WebSocket) -> None:\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n        logger.info(f\"WebSocket disconnected. Active: {len(self.active_connections)}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send JSON message to a specific connection", "output": "```python\n    async def send_json(self, websocket: WebSocket, data: dict) -> None:\n        \"\"\"Send JSON message to a specific connection\"\"\"\n        try:\n            await websocket.send_json(data)\n        except Exception as e:\n            logger.error(f\"Failed to send message: {e}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast message to all connections", "output": "```python\n    async def broadcast(self, data: dict) -> None:\n        \"\"\"Broadcast message to all connections\"\"\"\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(data)\n            except Exception:\n                pass\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-klass: Represents a mobile Q&A exchange for the Active Context buffer", "output": "```python\nclass MobileExchange:\n    \"\"\"Represents a mobile Q&A exchange for the Active Context buffer\"\"\"\n    request_id: str\n    question: str\n    answer: str\n    profile: str\n    timestamp: str\n    stats: dict = field(default_factory=dict)\n\n    def to_dict(self) -> dict:\n        return {\n            \"request_id\": self.request_id,\n            \"question\": self.question[:100] + \"...\" if len(self.question) > 100 else self.question,\n            \"answer\": self.answer[:200] + \"...\" if len(self.answer) > 200 else self.answer,\n            \"profile\": self.profile,\n            \"timestamp\": self.timestamp,\n            \"stats\": self.stats,\n        }\n```\n\nKlassen finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-klass: Manages WebSocket connections", "output": "```python\nclass ConnectionManager:\n    \"\"\"Manages WebSocket connections\"\"\"\n\n    def __init__(self):\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket) -> None:\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        logger.info(f\"WebSocket connected. Active: {len(self.active_connections)}\")\n\n    def disconnect(self, websocket: WebSocket) -> None:\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n        logger.info(f\"WebSocket disconnected. Active: {len(self.active_connections)}\")\n\n    async def send_json(self, websocket: WebSocket, data: dict) -> None:\n        \"\"\"Send JSON message to a specific connection\"\"\"\n        try:\n            await websocket.send_json(data)\n        except Exception as e:\n            logger.error(f\"Failed to send message: {e}\")\n\n    async def broadcast(self, data: dict) -> None:\n        \"\"\"Broadcast message to all connections\"\"\"\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(data)\n            except Exception:\n                pass\n```\n\nKlassen finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Health check endpoint", "output": "```python\nasync def health_check(\n    ollama: OllamaClient = Depends(get_ollama_client),\n    gpu: GPUMonitor = Depends(get_gpu_monitor),\n) -> HealthResponse:\n    \"\"\"\n    Health check endpoint.\n    Returns system status and component health.\n    \"\"\"\n    # Check Ollama\n    ollama_connected = await ollama.is_connected()\n    ollama_version = await ollama.get_version() if ollama_connected else None\n    models_available = await ollama.list_models() if ollama_connected else []\n    running_models = await ollama.list_running_models() if ollama_connected else []\n\n    # Check GPU\n    gpu_available = await gpu.is_gpu_available()\n\n    # Check required models (QWEN for local inference)\n    qwen_model = PROFILES[ProfileId.QWEN].model\n    qwen_available = qwen_model in models_available or any(qwen_model in m for m in models_available)\n\n    # Get orchestrator status for xAI\n    orch_status = await orchestrator.get_status()\n    xai_ok = orch_status.get(\"providers\", {}).get(\"xai\", {}).get(\"connected\", False)\n\n    # Determine overall status\n    checks = {\n        \"ollama\": ollama_connected,\n        \"gpu\": gpu_available,\n        \"qwen_model\": qwen_available,\n        \"xai\": xai_ok,\n    }\n\n    # Healthy if at least one provider works\n    if ollama_connected and qwen_available:\n        status = \"healthy\"\n    elif xai_ok:\n        status = \"degraded\"  # Cloud-only mode\n    else:\n        status = \"unhealthy\"\n\n    return HealthResponse(\n        status=status,\n        version=settings.app_version,\n        timestamp=datetime.utcnow(),\n        ollama=OllamaStatus(\n            connected=ollama_connected,\n            version=ollama_version,\n            models_available=models_available,\n            models_loaded=[m.get(\"name\", \"\") for m in running_models],\n        ),\n        gpu_available=gpu_available,\n        checks=checks,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: List available AI profiles (THINK/CHILL)", "output": "```python\nasync def get_profiles() -> ProfilesResponse:\n    \"\"\"\n    List available AI profiles (THINK/CHILL).\n    Returns profile configurations and current status.\n    \"\"\"\n    profiles = []\n    active_profile = profile_manager.active_profile\n\n    for profile in get_all_profiles():\n        status = profile_manager.get_status(profile.id)\n        profiles.append(ProfileInfo(\n            id=profile.id.value,\n            name=profile.name,\n            display_name=profile.display_name,\n            description=profile.description,\n            model=profile.model,\n            estimated_vram_gb=profile.estimated_vram_gb,\n            icon=profile.icon,\n            color=profile.color,\n            strengths=profile.strengths or [],\n            is_active=status.is_loaded,\n            is_loading=status.is_loading,\n        ))\n\n    return ProfilesResponse(\n        profiles=profiles,\n        active_profile=active_profile.id.value if active_profile else None,\n        default_profile=settings.default_profile,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get a specific profile by ID", "output": "```python\nasync def get_profile_by_id(profile_id: str) -> ProfileInfo:\n    \"\"\"\n    Get a specific profile by ID.\n    \"\"\"\n    try:\n        pid = ProfileId(profile_id.lower())\n        profile = PROFILES[pid]\n    except (ValueError, KeyError):\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Profile '{profile_id}' not found. Available: qwen, gemma\"\n        )\n\n    status = profile_manager.get_status(profile.id)\n\n    return ProfileInfo(\n        id=profile.id.value,\n        name=profile.name,\n        display_name=profile.display_name,\n        description=profile.description,\n        model=profile.model,\n        estimated_vram_gb=profile.estimated_vram_gb,\n        icon=profile.icon,\n        color=profile.color,\n        strengths=profile.strengths or [],\n        is_active=status.is_loaded,\n        is_loading=status.is_loading,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Pre-load a model into GPU memory", "output": "```python\nasync def warmup_profile(\n    profile_id: str,\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> dict:\n    \"\"\"\n    Pre-load a model into GPU memory.\n    This reduces first-response latency.\n    \"\"\"\n    try:\n        pid = ProfileId(profile_id.lower())\n        profile = PROFILES[pid]\n    except (ValueError, KeyError):\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Profile '{profile_id}' not found\"\n        )\n\n    logger.info(f\"Warming up profile: {profile_id}\")\n    success = await ollama.warmup_model(profile)\n\n    if not success:\n        raise HTTPException(\n            status_code=503,\n            detail=f\"Failed to warm up model {profile.model}. Is Ollama running?\"\n        )\n\n    return {\n        \"status\": \"ready\",\n        \"profile\": profile_id,\n        \"model\": profile.model,\n        \"message\": f\"Model {profile.display_name} is now loaded and ready\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get current GPU statistics", "output": "```python\nasync def get_gpu_stats(\n    gpu: GPUMonitor = Depends(get_gpu_monitor),\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> GPUStatsResponse:\n    \"\"\"\n    Get current GPU statistics.\n    Used by frontend GPU monitor component.\n    \"\"\"\n    stats = await gpu.get_stats()\n    running = await ollama.list_running_models()\n\n    return GPUStatsResponse(\n        gpu=stats,\n        timestamp=datetime.utcnow(),\n        ollama_models_loaded=[m.get(\"name\", \"\") for m in running],\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: List all available Ollama models", "output": "```python\nasync def list_models(\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> dict:\n    \"\"\"\n    List all available Ollama models.\n    \"\"\"\n    if not await ollama.is_connected():\n        raise HTTPException(\n            status_code=503,\n            detail=\"Ollama not available. Start with: ollama serve\"\n        )\n\n    models = await ollama.list_models()\n    running = await ollama.list_running_models()\n\n    return {\n        \"available\": models,\n        \"loaded\": [m.get(\"name\", \"\") for m in running],\n        \"required\": {\n            \"qwen\": PROFILES[ProfileId.QWEN].model,\n            \"gemma\": PROFILES[ProfileId.GEMMA].model,\n        }\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get status of Hybrid Orchestrator providers", "output": "```python\nasync def get_orchestrator_status() -> dict:\n    \"\"\"\n    Get status of Hybrid Orchestrator providers.\n\n    Returns status of:\n    - xAI Grok (cloud) - for FAST mode\n    - Ollama (local) - for DEEP mode\n    \"\"\"\n    status = await orchestrator.get_status()\n    return {\n        \"providers\": status,\n        \"modes\": {\n            \"auto\": \"Intelligent routing based on query\",\n            \"fast\": \"Grok API (instant, cloud)\",\n            \"deep\": \"Qwen 14B (max quality, local GPU)\",\n        },\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get project structure context for frontend display", "output": "```python\nasync def get_project_context(refresh: bool = False) -> dict:\n    \"\"\"\n    Get project structure context for frontend display.\n\n    Args:\n        refresh: Force refresh of cached context\n    \"\"\"\n    context = intelligence.get_project_context(force_refresh=refresh)\n    return {\n        \"root_path\": context.root_path,\n        \"file_tree\": context.file_tree,\n        \"file_count\": context.file_count,\n        \"directory_count\": context.directory_count,\n        \"languages\": context.languages_detected,\n        \"key_files\": context.key_files,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Validate code syntax (currently Python only)", "output": "```python\nasync def validate_code(body: dict) -> dict:\n    \"\"\"\n    Validate code syntax (currently Python only).\n\n    Body:\n        code: str - Code to validate\n        language: str - Language (default: python)\n    \"\"\"\n    code = body.get(\"code\", \"\")\n    language = body.get(\"language\", \"python\").lower()\n\n    if language != \"python\":\n        return {\n            \"valid\": None,\n            \"supported\": False,\n            \"message\": f\"Validation not supported for {language}. Only Python is supported.\"\n        }\n\n    from ..services.intelligence import validate_python_syntax, build_correction_prompt\n\n    result = validate_python_syntax(code)\n\n    response = {\n        \"valid\": result.is_valid,\n        \"supported\": True,\n        \"language\": result.language\n    }\n\n    if not result.is_valid:\n        response[\"error\"] = {\n            \"message\": result.error_message,\n            \"line\": result.error_line\n        }\n        response[\"correction_prompt\"] = build_correction_prompt(result)\n\n    return response\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Extract and validate JSON from text", "output": "```python\nasync def extract_json(body: dict) -> dict:\n    \"\"\"\n    Extract and validate JSON from text.\n\n    Body:\n        text: str - Text containing JSON\n        required_fields: list[str] - Optional required field names\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    required_fields = body.get(\"required_fields\", [])\n\n    from ..services.intelligence import extract_json_from_response, validate_json_against_schema\n\n    success, data, raw = extract_json_from_response(text)\n\n    response = {\n        \"found\": success,\n        \"data\": data if success else None,\n        \"raw\": raw if success else None\n    }\n\n    if success and required_fields:\n        valid, missing = validate_json_against_schema(data, required_fields)\n        response[\"fields_valid\"] = valid\n        if not valid:\n            response[\"missing_fields\"] = missing\n\n    if not success:\n        response[\"error\"] = data  # Contains error message\n\n    return response\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Warm up the system by pre-loading the default model", "output": "```python\nasync def warmup_system() -> dict:\n    \"\"\"\n    Warm up the system by pre-loading the default model.\n    This reduces first-response latency.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating system warmup\")\n        await emit_system_log(\"info\", \"System warmup initiated\")\n\n        result = await warmup_model()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", f\"System warmup completed: {result.get('message', 'Model loaded')}\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Model warmed up successfully\"),\n                \"model\": result.get(\"model\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"System warmup failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to warm up model\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"System warmup error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart the backend service", "output": "```python\nasync def restart_backend_service() -> dict:\n    \"\"\"\n    Restart the backend service.\n    This will reload all configurations and services.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating backend restart\")\n        await emit_system_log(\"warning\", \"Backend restart initiated\")\n\n        result = await restart_backend()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", \"Backend restart completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Backend restart initiated\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Backend restart failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to restart backend\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Backend restart error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart the frontend service", "output": "```python\nasync def restart_frontend_service() -> dict:\n    \"\"\"\n    Restart the frontend service.\n    This will reload the Next.js development server.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating frontend restart\")\n        await emit_system_log(\"warning\", \"Frontend restart initiated\")\n\n        result = await restart_frontend()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", \"Frontend restart completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Frontend restart initiated\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Frontend restart failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to restart frontend\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Frontend restart error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Unload all models from GPU memory", "output": "```python\nasync def unload_models() -> dict:\n    \"\"\"\n    Unload all models from GPU memory.\n    This frees up VRAM and allows for model switching.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating model unload\")\n        await emit_system_log(\"info\", \"Unloading all models from GPU memory\")\n\n        result = await unload_all_models()\n\n        if result.get(\"success\"):\n            models_unloaded = result.get(\"models_unloaded\", 0)\n            await emit_system_log(\"success\", f\"Successfully unloaded {models_unloaded} model(s)\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"All models unloaded\"),\n                \"models_unloaded\": models_unloaded,\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Model unload failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to unload models\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Model unload error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get today's system statistics", "output": "```python\nasync def get_stats_today() -> dict:\n    \"\"\"\n    Get today's system statistics.\n    Returns usage metrics, request counts, and performance data.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Fetching today's statistics\")\n\n        result = await get_today_stats()\n\n        if result.get(\"success\"):\n            stats = result.get(\"stats\", {})\n            return {\n                \"success\": True,\n                \"stats\": stats,\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Failed to fetch stats: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to retrieve statistics\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Stats retrieval error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Voice command endpoint for mobile app", "output": "```python\nasync def voice_command(body: dict) -> dict:\n    \"\"\"\n    Voice command endpoint for mobile app.\n    Receives transcribed text and returns AI response.\n\n    WAR ROOM SYNC: Broadcasts mobile_activity events to all connected dashboards.\n\n    Request body:\n        text: str - User message\n        profile: str - Agent profile (\"qwen\", \"nerdy\", \"gemma\", default: \"qwen\")\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    profile = body.get(\"profile\", \"qwen\").lower()\n\n    if not text:\n        return {\"success\": False, \"error\": \"No text provided\"}\n\n    # Validate profile\n    valid_profiles = [\"qwen\", \"nerdy\", \"gemma\"]\n    if profile not in valid_profiles:\n        profile = \"qwen\"\n\n    try:\n        # Skapa meddelande\n        messages = [{\"role\": \"user\", \"content\": text}]\n        request_id = str(uuid.uuid4())[:8]\n\n        # WAR ROOM: Emit request start to dashboards\n        await emit_mobile_activity(\"mobile_request_start\", request_id, {\n            \"profile\": profile,\n            \"question_preview\": text[:50] + \"...\" if len(text) > 50 else text,\n        })\n\n        # Samla hela svaret från stream\n        full_response = \"\"\n        token_count = 0\n        last_emit_time = datetime.utcnow()\n        final_stats = None\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request_id,\n            profile=profile,\n        ):\n            if token:\n                full_response += token\n                token_count += 1\n\n            if stats:\n                final_stats = stats\n\n            # WAR ROOM: Emit generation progress every 500ms\n            now = datetime.utcnow()\n            if (now - last_emit_time).total_seconds() >= 0.5:\n                tps = final_stats.tokens_per_second if final_stats else 0\n                await emit_mobile_activity(\"mobile_generation\", request_id, {\n                    \"tokens_generated\": token_count,\n                    \"tokens_per_second\": round(tps, 1),\n                })\n                last_emit_time = now\n\n        # Build stats dict for response\n        stats_dict = {}\n        if final_stats:\n            stats_dict = {\n                \"tokens_generated\": final_stats.tokens_generated,\n                \"tokens_per_second\": round(final_stats.tokens_per_second, 1),\n                \"total_duration_ms\": final_stats.total_duration_ms,\n            }\n\n        # WAR ROOM: Add to active context buffer\n        add_mobile_exchange(\n            request_id=request_id,\n            question=text,\n            answer=full_response,\n            profile=profile,\n            stats=stats_dict,\n        )\n\n        # WAR ROOM: Emit complete response with active context\n        await emit_mobile_activity(\"mobile_response\", request_id, {\n            \"question\": text,\n            \"answer\": full_response[:300] + \"...\" if len(full_response) > 300 else full_response,\n            \"stats\": stats_dict,\n            \"active_context\": get_active_context(),\n        })\n\n        return {\n            \"success\": True,\n            \"response\": full_response or \"Inget svar\"\n        }\n\n    except Exception as e:\n        logger.error(f\"Voice command error: {e}\")\n        # WAR ROOM: Emit error\n        await emit_mobile_activity(\"mobile_error\", request_id, {\n            \"error\": str(e)[:100],\n        })\n        return {\n            \"success\": False,\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: SSE streaming endpoint for mobile app", "output": "```python\nasync def voice_command_stream(body: dict) -> StreamingResponse:\n    \"\"\"\n    SSE streaming endpoint for mobile app.\n    Receives transcribed text and streams AI response token by token.\n\n    Request body:\n        text: str - User message\n        profile: str - Agent profile (\"qwen\" or \"nerdy\", default: \"qwen\")\n        history: list - Optional conversation history\n\n    Response: Server-Sent Events stream\n        - Each token: data: {\"token\": \"...\"}\\n\\n\n        - On completion: data: {\"done\": true, \"agent_id\": \"qwen\"}\\n\\n\n        - On error: data: {\"error\": \"message\"}\\n\\n\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    profile = body.get(\"profile\", \"qwen\").lower()\n    history = body.get(\"history\", [])\n\n    if not text:\n        # Return immediate error response\n        async def error_stream() -> AsyncGenerator[str, None]:\n            error_data = {\"error\": \"No text provided\"}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n        return StreamingResponse(\n            error_stream(),\n            media_type=\"text/event-stream\",\n            headers={\n                \"Cache-Control\": \"no-cache\",\n                \"Connection\": \"keep-alive\",\n                \"X-Accel-Buffering\": \"no\"\n            }\n        )\n\n    # Validate profile\n    valid_profiles = [\"qwen\", \"nerdy\", \"gemma\"]\n    if profile not in valid_profiles:\n        profile = \"qwen\"\n        logger.warning(f\"Invalid profile requested, defaulting to qwen. Valid: {valid_profiles}\")\n\n    async def event_stream() -> AsyncGenerator[str, None]:\n        \"\"\"Generate SSE stream from orchestrator\"\"\"\n        try:\n            # Build messages array with history\n            messages = []\n\n            # Add history if provided\n            if history and isinstance(history, list):\n                for msg in history:\n                    if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n                        messages.append({\n                            \"role\": msg[\"role\"],\n                            \"content\": msg[\"content\"]\n                        })\n\n            # Add current user message\n            messages.append({\"role\": \"user\", \"content\": text})\n\n            request_id = str(uuid.uuid4())[:8]\n            logger.info(f\"SSE stream started: request_id={request_id}, profile={profile}, messages={len(messages)}\")\n\n            # Stream tokens from orchestrator\n            agent_id = profile\n            async for token, stats, returned_agent_id in orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n            ):\n                if returned_agent_id:\n                    agent_id = returned_agent_id\n\n                if token:\n                    token_data = {\"token\": token}\n                    yield f\"data: {json.dumps(token_data)}\\n\\n\"\n\n            # Send completion event\n            completion_data = {\n                \"done\": True,\n                \"agent_id\": agent_id\n            }\n            yield f\"data: {json.dumps(completion_data)}\\n\\n\"\n\n            logger.info(f\"SSE stream completed: request_id={request_id}, agent_id={agent_id}\")\n\n        except Exception as e:\n            logger.error(f\"SSE stream error: {e}\", exc_info=True)\n            error_data = {\"error\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get complete system health statistics", "output": "```python\nasync def get_system_health() -> dict:\n    \"\"\"\n    Get complete system health statistics.\n    Returns CPU, RAM, Disk, GPU stats for the admin dashboard.\n    \"\"\"\n    try:\n        stats = await system_monitor.get_stats()\n        return {\n            \"success\": True,\n            **stats.to_dict()\n        }\n    except Exception as e:\n        logger.error(f\"System health error: {e}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get minimal system health for frequent polling", "output": "```python\nasync def get_system_health_quick() -> dict:\n    \"\"\"\n    Get minimal system health for frequent polling.\n    Returns just percentages for CPU, RAM, Disk, GPU.\n    \"\"\"\n    try:\n        summary = await system_monitor.get_quick_summary()\n        return {\n            \"success\": True,\n            **summary,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n    except Exception as e:\n        logger.error(f\"Quick health error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Execute a shell command", "output": "```python\nasync def execute_shell_command(body: dict) -> dict:\n    \"\"\"\n    Execute a shell command.\n\n    WAR ROOM SYNC: Broadcasts mobile_shell event to dashboards.\n\n    Request body:\n        command: str - Command to execute (with or without $ prefix)\n        confirmed: bool - Whether dangerous commands are confirmed\n\n    Response:\n        success, stdout, stderr, exit_code, duration_ms\n    \"\"\"\n    command = body.get(\"command\", \"\")\n    confirmed = body.get(\"confirmed\", False)\n\n    if not command:\n        return {\n            \"success\": False,\n            \"error\": \"No command provided\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    try:\n        result = await shell_executor.execute(command, confirmed=confirmed)\n        result_dict = result.to_dict()\n\n        # WAR ROOM: Emit shell command to dashboards\n        shell_id = f\"shell_{datetime.utcnow().strftime('%H%M%S')}\"\n        await emit_mobile_activity(\"mobile_shell\", shell_id, {\n            \"command\": command[:80],\n            \"stdout\": result_dict.get(\"stdout\", \"\")[:500],\n            \"exit_code\": result_dict.get(\"exit_code\", -1),\n            \"success\": result_dict.get(\"success\", False),\n        })\n\n        return result_dict\n    except Exception as e:\n        logger.error(f\"Shell execute error: {e}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get list of quick commands for the UI", "output": "```python\nasync def get_shell_commands() -> dict:\n    \"\"\"\n    Get list of quick commands for the UI.\n    Returns predefined safe commands for easy access.\n    \"\"\"\n    try:\n        commands = await shell_executor.get_quick_commands()\n        return {\n            \"success\": True,\n            \"commands\": commands\n        }\n    except Exception as e:\n        logger.error(f\"Get commands error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Execute a QWEN SysAdmin tool", "output": "```python\nasync def execute_tool(body: dict) -> dict:\n    \"\"\"\n    Execute a QWEN SysAdmin tool.\n\n    This endpoint is used for:\n    1. Confirming and executing DANGEROUS tools after user approval\n    2. Directly executing tools from admin interface\n\n    Request body:\n        tool: str - Tool name (e.g., \"docker_restart\")\n        params: dict - Tool parameters\n        confirmed: bool - Whether user has confirmed (required for DANGEROUS tools)\n        request_id: str - Optional request ID for pending confirmations\n\n    Response:\n        success, output, error\n    \"\"\"\n    tool_name = body.get(\"tool\", \"\")\n    params = body.get(\"params\", {})\n    confirmed = body.get(\"confirmed\", False)\n    request_id = body.get(\"request_id\")\n\n    if not tool_name:\n        return {\n            \"success\": False,\n            \"error\": \"No tool specified\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # Check if tool exists\n    tool_def = TOOLS.get(tool_name)\n    if not tool_def:\n        return {\n            \"success\": False,\n            \"error\": f\"Unknown tool: {tool_name}. Use GET /api/tools for available tools.\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # Check safety level\n    if tool_def.safety == SafetyLevel.DANGEROUS and not confirmed:\n        return {\n            \"success\": False,\n            \"error\": f\"Tool '{tool_name}' is DANGEROUS and requires confirmation. Set confirmed=true.\",\n            \"requires_confirmation\": True,\n            \"tool\": tool_name,\n            \"description\": tool_def.description,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # If this is a pending confirmation, remove it from pending\n    if request_id and request_id in _pending_tool_confirmations:\n        pending = _pending_tool_confirmations.pop(request_id)\n        # Use params from pending if not overridden\n        if not params:\n            params = pending.get(\"params\", {})\n\n    try:\n        await emit_system_log(\n            \"tool_execute\",\n            f\"[SYSTEM] Executing {tool_name}...\",\n            \"info\",\n            \"qwen\"\n        )\n\n        # Execute the tool\n        result = await system_tools.execute(tool_name, params)\n\n        # Log result\n        if result.success:\n            await emit_system_log(\n                \"tool_success\",\n                f\"[SYSTEM] {tool_name}: OK\",\n                \"success\",\n                \"qwen\"\n            )\n        else:\n            await emit_system_log(\n                \"tool_error\",\n                f\"[SYSTEM] {tool_name}: {result.error}\",\n                \"error\",\n                \"qwen\"\n            )\n\n        return {\n            \"success\": result.success,\n            \"tool\": tool_name,\n            \"output\": result.output,\n            \"error\": result.error,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    except Exception as e:\n        error_msg = f\"Tool execution error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"tool_error\", error_msg, \"error\", \"qwen\")\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Confirm and execute a pending DANGEROUS tool", "output": "```python\nasync def confirm_tool(request_id: str) -> dict:\n    \"\"\"\n    Confirm and execute a pending DANGEROUS tool.\n\n    This is called when user clicks \"Godkänn\" in the frontend after\n    QWEN requests a dangerous operation.\n\n    Path params:\n        request_id: The request_id from confirmation_required message\n    \"\"\"\n    if request_id not in _pending_tool_confirmations:\n        return {\n            \"success\": False,\n            \"error\": f\"No pending tool confirmation for request_id: {request_id}\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    pending = _pending_tool_confirmations.pop(request_id)\n    tool_name = pending[\"tool\"]\n    params = pending[\"params\"]\n\n    await emit_system_log(\n        \"tool_confirmed\",\n        f\"[USER] Confirmed: {tool_name}\",\n        \"warn\",\n        \"qwen\"\n    )\n\n    try:\n        result = await system_tools.execute(tool_name, params)\n\n        if result.success:\n            await emit_system_log(\n                \"tool_success\",\n                f\"[SYSTEM] {tool_name}: OK\",\n                \"success\",\n                \"qwen\"\n            )\n        else:\n            await emit_system_log(\n                \"tool_error\",\n                f\"[SYSTEM] {tool_name}: {result.error}\",\n                \"error\",\n                \"qwen\"\n            )\n\n        return {\n            \"success\": result.success,\n            \"tool\": tool_name,\n            \"output\": result.output,\n            \"error\": result.error,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    except Exception as e:\n        error_msg = f\"Tool execution error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Cancel a pending DANGEROUS tool", "output": "```python\nasync def cancel_tool(request_id: str) -> dict:\n    \"\"\"\n    Cancel a pending DANGEROUS tool.\n\n    This is called when user clicks \"Avbryt\" in the frontend.\n    \"\"\"\n    if request_id not in _pending_tool_confirmations:\n        return {\n            \"success\": False,\n            \"error\": f\"No pending tool confirmation for request_id: {request_id}\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    pending = _pending_tool_confirmations.pop(request_id)\n    tool_name = pending[\"tool\"]\n\n    await emit_system_log(\n        \"tool_cancelled\",\n        f\"[USER] Cancelled: {tool_name}\",\n        \"warn\",\n        \"qwen\"\n    )\n\n    return {\n        \"success\": True,\n        \"message\": f\"Tool {tool_name} cancelled\",\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Pull latest changes from git repository", "output": "```python\nasync def deploy_git_pull() -> dict:\n    \"\"\"\n    Pull latest changes from git repository.\n    Uses --ff-only to prevent merge conflicts.\n    \"\"\"\n    try:\n        await emit_system_log(\"info\", \"Git pull initiated from mobile app\")\n        result = await deploy_manager.git_pull()\n\n        if result.success:\n            await emit_system_log(\"success\", f\"Git pull completed: {result.output}\")\n        else:\n            await emit_system_log(\"error\", f\"Git pull failed: {result.error}\")\n\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Git pull error: {e}\")\n        return {\n            \"success\": False,\n            \"operation\": \"git_pull\",\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get current git status (modified files, etc", "output": "```python\nasync def deploy_git_status() -> dict:\n    \"\"\"Get current git status (modified files, etc.)\"\"\"\n    try:\n        result = await deploy_manager.git_status()\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Git status error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart a systemd service", "output": "```python\nasync def deploy_restart_service(service: str, body: dict = None) -> dict:\n    \"\"\"\n    Restart a systemd service.\n\n    Path params:\n        service: \"simons-ai-backend\", \"simons-ai-frontend\", or \"ollama\"\n    \"\"\"\n    try:\n        await emit_system_log(\"warning\", f\"Service restart initiated: {service}\")\n        result = await deploy_manager.restart_service(service)\n\n        if result.success:\n            await emit_system_log(\"success\", f\"Service {service} restarted\")\n        else:\n            await emit_system_log(\"error\", f\"Failed to restart {service}: {result.error}\")\n\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Restart service error: {e}\")\n        return {\n            \"success\": False,\n            \"operation\": f\"restart_{service}\",\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get status of a systemd service", "output": "```python\nasync def deploy_service_status(service: str) -> dict:\n    \"\"\"\n    Get status of a systemd service.\n\n    Path params:\n        service: \"simons-ai-backend\", \"simons-ai-frontend\", or \"ollama\"\n    \"\"\"\n    try:\n        result = await deploy_manager.get_service_status(service)\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Service status error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Full deployment: git pull + restart backend + restart frontend", "output": "```python\nasync def deploy_full() -> StreamingResponse:\n    \"\"\"\n    Full deployment: git pull + restart backend + restart frontend.\n\n    Returns Server-Sent Events with progress updates.\n    \"\"\"\n    async def event_stream() -> AsyncGenerator[str, None]:\n        try:\n            await emit_system_log(\"warning\", \"Full deploy initiated from mobile app\")\n\n            async for progress in deploy_manager.full_deploy():\n                data = {\n                    \"step\": progress.step,\n                    \"status\": progress.status,\n                    \"message\": progress.message,\n                    \"output\": progress.output,\n                }\n                yield f\"data: {json.dumps(data)}\\n\\n\"\n\n            await emit_system_log(\"success\", \"Full deploy completed\")\n\n        except Exception as e:\n            logger.error(f\"Full deploy error: {e}\")\n            error_data = {\"step\": \"error\", \"status\": \"failed\", \"message\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Serve the Android APK file with proper headers for download", "output": "```python\nasync def download_apk():\n    \"\"\"\n    Serve the Android APK file with proper headers for download.\n    Uses StreamingResponse to handle the 72MB file efficiently.\n    \"\"\"\n    apk_path = Path(\"/home/ai-server/simons-ai-v2-nerdy-fix.apk\")\n\n    if not apk_path.exists():\n        raise HTTPException(status_code=404, detail=\"APK file not found\")\n\n    file_size = os.path.getsize(apk_path)\n\n    async def file_iterator():\n        \"\"\"Stream the file in chunks to avoid loading it all into memory\"\"\"\n        chunk_size = 1024 * 1024  # 1MB chunks\n        with open(apk_path, \"rb\") as f:\n            while chunk := f.read(chunk_size):\n                yield chunk\n\n    return StreamingResponse(\n        file_iterator(),\n        media_type=\"application/vnd.android.package-archive\",\n        headers={\n            \"Content-Disposition\": f\"attachment; filename={apk_path.name}\",\n            \"Content-Length\": str(file_size),\n            \"Cache-Control\": \"no-cache\",\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Generate SSE stream from orchestrator", "output": "```python\n    async def event_stream() -> AsyncGenerator[str, None]:\n        \"\"\"Generate SSE stream from orchestrator\"\"\"\n        try:\n            # Build messages array with history\n            messages = []\n\n            # Add history if provided\n            if history and isinstance(history, list):\n                for msg in history:\n                    if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n                        messages.append({\n                            \"role\": msg[\"role\"],\n                            \"content\": msg[\"content\"]\n                        })\n\n            # Add current user message\n            messages.append({\"role\": \"user\", \"content\": text})\n\n            request_id = str(uuid.uuid4())[:8]\n            logger.info(f\"SSE stream started: request_id={request_id}, profile={profile}, messages={len(messages)}\")\n\n            # Stream tokens from orchestrator\n            agent_id = profile\n            async for token, stats, returned_agent_id in orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n            ):\n                if returned_agent_id:\n                    agent_id = returned_agent_id\n\n                if token:\n                    token_data = {\"token\": token}\n                    yield f\"data: {json.dumps(token_data)}\\n\\n\"\n\n            # Send completion event\n            completion_data = {\n                \"done\": True,\n                \"agent_id\": agent_id\n            }\n            yield f\"data: {json.dumps(completion_data)}\\n\\n\"\n\n            logger.info(f\"SSE stream completed: request_id={request_id}, agent_id={agent_id}\")\n\n        except Exception as e:\n            logger.error(f\"SSE stream error: {e}\", exc_info=True)\n            error_data = {\"error\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Skapa en async Python-funktion som event stream", "output": "```python\n    async def event_stream() -> AsyncGenerator[str, None]:\n        try:\n            await emit_system_log(\"warning\", \"Full deploy initiated from mobile app\")\n\n            async for progress in deploy_manager.full_deploy():\n                data = {\n                    \"step\": progress.step,\n                    \"status\": progress.status,\n                    \"message\": progress.message,\n                    \"output\": progress.output,\n                }\n                yield f\"data: {json.dumps(data)}\\n\\n\"\n\n            await emit_system_log(\"success\", \"Full deploy completed\")\n\n        except Exception as e:\n            logger.error(f\"Full deploy error: {e}\")\n            error_data = {\"step\": \"error\", \"status\": \"failed\", \"message\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Stream the file in chunks to avoid loading it all into memory", "output": "```python\n    async def file_iterator():\n        \"\"\"Stream the file in chunks to avoid loading it all into memory\"\"\"\n        chunk_size = 1024 * 1024  # 1MB chunks\n        with open(apk_path, \"rb\") as f:\n            while chunk := f.read(chunk_size):\n                yield chunk\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Skapa en async Python-funktion som error stream", "output": "```python\n        async def error_stream() -> AsyncGenerator[str, None]:\n            error_data = {\"error\": \"No text provided\"}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Proxy till Claude Stats API på dev-maskinen", "output": "```python\nasync def get_claude_stats() -> dict:\n    \"\"\"\n    Proxy till Claude Stats API på dev-maskinen.\n    Returnerar tokens, kostnad, modeller och aktiv status.\n    \"\"\"\n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(f\"{DEV_MACHINE_URL}/api/all\")\n            \n            if response.status_code == 200:\n                return {\n                    \"success\": True,\n                    **response.json(),\n                    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n                }\n            else:\n                return {\n                    \"success\": False,\n                    \"error\": f\"API returned {response.status_code}\",\n                    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n                }\n                \n    except httpx.ConnectError:\n        return {\n            \"success\": False,\n            \"error\": \"Cannot connect to dev machine (192.168.86.27:8765)\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/claude_routes.py`."}
{"instruction": "Implementera en Python-funktion: Dependency injection helper", "output": "```python\nasync def get_shell_executor() -> ShellExecutor:\n    \"\"\"Dependency injection helper\"\"\"\n    return shell_executor\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Check if command requires confirmation", "output": "```python\n    def _is_dangerous(self, command: str) -> bool:\n        \"\"\"Check if command requires confirmation\"\"\"\n        cmd_lower = command.lower()\n        return any(pattern in cmd_lower for pattern in self.DANGEROUS_PATTERNS)\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Return list of quick commands for the UI", "output": "```python\n    async def get_quick_commands(self) -> list[dict]:\n        \"\"\"Return list of quick commands for the UI\"\"\"\n        return [\n            {\"id\": \"uptime\", \"command\": \"uptime\", \"description\": \"Visa systemupptid\", \"dangerous\": False},\n            {\"id\": \"free\", \"command\": \"free -h\", \"description\": \"Visa minnesanvändning\", \"dangerous\": False},\n            {\"id\": \"df\", \"command\": \"df -h\", \"description\": \"Visa diskutrymme\", \"dangerous\": False},\n            {\"id\": \"ps\", \"command\": \"ps aux --sort=-%cpu | head -10\", \"description\": \"Topp 10 processer (CPU)\", \"dangerous\": False},\n            {\"id\": \"nvidia\", \"command\": \"nvidia-smi\", \"description\": \"GPU-status\", \"dangerous\": False},\n            {\"id\": \"ollama_list\", \"command\": \"ollama list\", \"description\": \"Visa Ollama-modeller\", \"dangerous\": False},\n            {\"id\": \"ollama_ps\", \"command\": \"ollama ps\", \"description\": \"Visa körande modeller\", \"dangerous\": False},\n            {\"id\": \"backend_logs\", \"command\": \"journalctl -u simons-ai-backend -n 30 --no-pager\", \"description\": \"Backend-loggar (30 rader)\", \"dangerous\": False},\n            {\"id\": \"frontend_logs\", \"command\": \"journalctl -u simons-ai-frontend -n 30 --no-pager\", \"description\": \"Frontend-loggar (30 rader)\", \"dangerous\": False},\n            {\"id\": \"restart_backend\", \"command\": \"sudo systemctl restart simons-ai-backend\", \"description\": \"Starta om backend\", \"dangerous\": True},\n            {\"id\": \"restart_frontend\", \"command\": \"sudo systemctl restart simons-ai-frontend\", \"description\": \"Starta om frontend\", \"dangerous\": True},\n        ]\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Number of commands executed this session", "output": "```python\n    def execution_count(self) -> int:\n        \"\"\"Number of commands executed this session\"\"\"\n        return self._execution_count\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-klass: Result from shell command execution", "output": "```python\nclass ShellResult:\n    \"\"\"Result from shell command execution\"\"\"\n    success: bool\n    command: str\n    stdout: str\n    stderr: str\n    exit_code: int\n    duration_ms: int\n    requires_confirmation: bool = False\n    message: Optional[str] = None\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"command\": self.command,\n            \"stdout\": self.stdout,\n            \"stderr\": self.stderr,\n            \"exit_code\": self.exit_code,\n            \"duration_ms\": self.duration_ms,\n            \"requires_confirmation\": self.requires_confirmation,\n            \"message\": self.message,\n            \"timestamp\": self.timestamp,\n        }\n```\n\nKlassen finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-klass: Execute shell commands with safety checks", "output": "```python\nclass ShellExecutor:\n    \"\"\"\n    Execute shell commands with safety checks.\n\n    Security model: Trust LAN, require confirmation for dangerous commands.\n    This is NOT secure for public internet - only use on local network.\n    \"\"\"\n\n    # Patterns that require user confirmation before execution\n    DANGEROUS_PATTERNS = [\n        \"restart\",\n        \"stop\",\n        \"kill\",\n        \"rm \",\n        \"rm\\t\",\n        \"reboot\",\n        \"shutdown\",\n        \"systemctl\",\n        \"service \",\n        \"pkill\",\n        \"killall\",\n        \"dd \",\n        \"mkfs\",\n        \"> /\",  # redirect to root\n    ]\n\n    # Commands that are always blocked (even with confirmation)\n    BLOCKED_PATTERNS = [\n        \"rm -rf /\",\n        \"rm -rf /*\",\n        \":(){ :|:& };:\",  # fork bomb\n        \"> /dev/sda\",\n        \"mkfs /dev/sd\",\n    ]\n\n    def __init__(self):\n        self._execution_count = 0\n\n    def _is_dangerous(self, command: str) -> bool:\n        \"\"\"Check if command requires confirmation\"\"\"\n        cmd_lower = command.lower()\n        return any(pattern in cmd_lower for pattern in self.DANGEROUS_PATTERNS)\n\n    def _is_blocked(self, command: str) -> bool:\n        \"\"\"Check if command is always blocked\"\"\"\n        cmd_lower = command.lower().replace(\"  \", \" \")\n        return any(pattern in cmd_lower for pattern in self.BLOCKED_PATTERNS)\n\n    async def execute(\n        self,\n        command: str,\n        confirmed: bool = False,\n        timeout: int = DEFAULT_TIMEOUT\n    ) -> ShellResult:\n        \"\"\"\n        Execute a shell command.\n\n        Args:\n            command: Shell command to execute\n            confirmed: Whether user has confirmed dangerous commands\n            timeout: Timeout in seconds\n\n        Returns:\n            ShellResult with output and status\n        \"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n\n        # Strip the $ prefix if present\n        if command.startswith(\"$ \"):\n            command = command[2:]\n        elif command.startswith(\"$\"):\n            command = command[1:]\n\n        command = command.strip()\n\n        if not command:\n            return ShellResult(\n                success=False,\n                command=command,\n                stdout=\"\",\n                stderr=\"No command provided\",\n                exit_code=-1,\n                duration_ms=0,\n                message=\"Empty command\",\n                timestamp=timestamp,\n            )\n\n        # Check if blocked\n        if self._is_blocked(command):\n            logger.warning(f\"Blocked dangerous command: {command}\")\n            return ShellResult(\n                success=False,\n                command=command,\n                stdout=\"\",\n                stderr=\"This command is blocked for safety reasons\",\n                exit_code=-1,\n                duration_ms=0,\n                message=\"Command blocked\",\n                timestamp=timestamp,\n            )\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Load environment variables from ", "output": "```python\ndef _load_env():\n    \"\"\"Load environment variables from .env and .env.local files\"\"\"\n    for env_file in [\".env\", \".env.local\"]:\n        env_path = Path(__file__).parent.parent.parent / env_file\n        if env_path.exists():\n            with open(env_path) as f:\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith(\"#\") and \"=\" in line:\n                        key, _, value = line.partition(\"=\")\n                        key = key.strip()\n                        value = value.strip().strip('\"').strip(\"'\")\n                        if key and key not in os.environ:\n                            os.environ[key] = value\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Skapa en Python-funktion som total duration ms", "output": "```python\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Skapa en Python-funktion som tokens per second", "output": "```python\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Claude UI URL is configured", "output": "```python\n    def is_configured(self) -> bool:\n        \"\"\"Check if Claude UI URL is configured\"\"\"\n        return bool(self.ui_url)\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Claude Code UI WebSocket is reachable", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Claude Code UI WebSocket is reachable\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Claude UI URL not configured\")\n            return False\n\n        try:\n            # Try to connect to WebSocket\n            ws_url = self.ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n            async with websockets.connect(\n                ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers={\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n            ) as ws:\n                return True\n        except Exception as e:\n            logger.warning(f\"Claude UI connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Extract the latest user message from conversation", "output": "```python\n    def _build_command_from_messages(self, messages: list[dict], system_prompt: Optional[str] = None) -> str:\n        \"\"\"Extract the latest user message from conversation\"\"\"\n        # Find the last user message\n        for msg in reversed(messages):\n            if msg.get(\"role\") == \"user\":\n                return msg.get(\"content\", \"\")\n        \n        # If no user message, use system prompt or empty\n        return system_prompt or \"\"\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Stream chat completion tokens from Claude CLI", "output": "```python\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[ClaudeStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Claude CLI.\n\n        Yields:\n            Tuple of (token_content, stats)\n            - During streaming: (token, None)\n            - Final yield: (\"\", ClaudeStats)\n        \"\"\"\n        if not self.is_configured:\n            raise ClaudeError(\"Claude CLI not found\")\n\n        stats = ClaudeStats(start_time=time.time())\n\n        # Extract command from messages\n        command = self._build_command_from_messages(messages, system_prompt)\n\n        # Build WebSocket message\n        ws_message = {\n            \"type\": \"claude-command\",\n            \"command\": command,\n            \"options\": {\n                \"model\": self.model,\n                \"sessionId\": request_id,  # Use request_id as session ID\n                \"projectPath\": os.getcwd()  # Use current working directory\n            }\n        }\n\n        logger.info(f\"[{request_id}] Starting Claude UI chat with {self.model}\")\n\n        try:\n            # Connect to WebSocket\n            headers = {}\n            if self.token:\n                headers[\"Authorization\"] = f\"Bearer {self.token}\"\n\n            async with websockets.connect(\n                self.ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers=headers\n            ) as ws:\n                # Send command\n                await ws.send(json.dumps(ws_message))\n\n                # Read streaming responses\n                async for message in ws:\n                    try:\n                        data = json.loads(message)\n                        \n                        # Handle different message types from Claude UI\n                        if data.get(\"type\") == \"claude-response\":\n                            # Content chunk\n                            content = data.get(\"data\", {}).get(\"content\", \"\")\n                            if content:\n                                if stats.first_token_time is None:\n                                    stats.first_token_time = time.time()\n                                stats.tokens_generated += len(content) // 4\n                                yield content, None\n                        \n                        elif data.get(\"type\") == \"result\":\n                            # Final result\n                            content = data.get(\"content\", \"\")\n                            if content:\n                                if stats.first_token_time is None:\n                                    stats.first_token_time = time.time()\n                                stats.tokens_generated += len(content) // 4\n                                yield content, None\n                            break\n                        \n                        elif data.get(\"type\") == \"error\":\n                            error_msg = data.get(\"error\", \"Unknown error\")\n                            logger.error(f\"Claude UI error: {error_msg}\")\n                            raise ClaudeError(f\"Claude UI error: {error_msg}\")\n\n                    except json.JSONDecodeError:\n                        # Try to treat as plain text\n                        if message:\n                            if stats.first_token_time is None:\n                                stats.first_token_time = time.time()\n                            stats.tokens_generated += len(message) // 4\n                            yield message, None\n\n        except asyncio.TimeoutError:\n            stats.end_time = time.time()\n            raise ClaudeTimeoutError(f\"Request timed out after {READ_TIMEOUT}s\")\n\n        except Exception as e:\n            stats.end_time = time.time()\n            raise ClaudeError(f\"Claude CLI error: {e}\") from e\n\n        # Final yield with stats\n        stats.end_time = time.time()\n        logger.info(\n            f\"[{request_id}] Claude completed: {stats.tokens_generated} tokens \"\n            f\"in {stats.total_duration_ms}ms ({stats.tokens_per_second:.1f} tok/s)\"\n        )\n        yield \"\", stats\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Close WebSocket connection", "output": "```python\n    async def close(self) -> None:\n        \"\"\"Close WebSocket connection\"\"\"\n        if self._ws:\n            await self._ws.close()\n            self._ws = None\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Claude CLI response", "output": "```python\nclass ClaudeStats:\n    \"\"\"Statistics from Claude CLI response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Claude errors", "output": "```python\nclass ClaudeError(Exception):\n    \"\"\"Base exception for Claude errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Claude CLI failed", "output": "```python\nclass ClaudeConnectionError(ClaudeError):\n    \"\"\"Connection to Claude CLI failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Operation timed out", "output": "```python\nclass ClaudeTimeoutError(ClaudeError):\n    \"\"\"Operation timed out\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Claude CLI with streaming support", "output": "```python\nclass ClaudeClient:\n    \"\"\"\n    Async client for Claude CLI with streaming support.\n\n    Uses Claude Code CLI in --print mode with stream-json format\n    for real-time token streaming.\n    \"\"\"\n\n    def __init__(self, ui_url: str = CLAUDE_UI_URL, token: str = CLAUDE_UI_TOKEN, model: str = CLAUDE_MODEL):\n        self.ui_url = ui_url\n        self.ws_url = ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n        self.token = token\n        self.model = model\n        self._ws: Optional[WebSocketClientProtocol] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if Claude UI URL is configured\"\"\"\n        return bool(self.ui_url)\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Claude Code UI WebSocket is reachable\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Claude UI URL not configured\")\n            return False\n\n        try:\n            # Try to connect to WebSocket\n            ws_url = self.ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n            async with websockets.connect(\n                ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers={\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n            ) as ws:\n                return True\n        except Exception as e:\n            logger.warning(f\"Claude UI connection check failed: {e}\")\n            return False\n\n    def _build_command_from_messages(self, messages: list[dict], system_prompt: Optional[str] = None) -> str:\n        \"\"\"Extract the latest user message from conversation\"\"\"\n        # Find the last user message\n        for msg in reversed(messages):\n            if msg.get(\"role\") == \"user\":\n                return msg.get(\"content\", \"\")\n        \n        # If no user message, use system prompt or empty\n        return system_prompt or \"\"\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[ClaudeStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Claude CLI.\n\n        Yields:\n            Tuple of (token_content, stats)\n            - During streaming: (token, None)\n            - Final yield: (\"\", ClaudeStats)\n        \"\"\"\n        if not self.is_configured:\n            raise ClaudeError(\"Claude CLI not found\")\n\n        stats = ClaudeStats(start_time=time.time())\n\n        # Extract command from messages\n        command = self._build_command_from_messages(messages, system_prompt)\n\n        # Build WebSocket message\n        ws_message = {\n            \"type\": \"claude-command\",\n            \"command\": command,\n            \"options\": {\n                \"model\": self.model,\n                \"sessionId\": request_id,  # Use request_id as session ID\n                \"projectPath\": os.getcwd()  # Use current working directory\n            }\n        }\n\n        logger.info(f\"[{request_id}] Starting Claude UI chat with {self.model}\")\n\n        try:\n            # Connect to WebSocket\n            headers = {}\n            if self.token:\n                headers[\"Authorization\"] = f\"Bearer {self.token}\"\n\n            async with websockets.connect(\n                self.ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers=headers\n            ) as ws:\n                # Send command\n                await ws.send(json.dumps(ws_message))\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Get agent config by ID, defaults to QWEN_SONNET", "output": "```python\ndef get_agent(agent_id: str) -> AgentConfig:\n    \"\"\"Get agent config by ID, defaults to QWEN_SONNET\"\"\"\n    try:\n        normalized = agent_id.lower().replace(\"_\", \"-\")\n        return AGENTS[AgentId(normalized)]\n    except (ValueError, KeyError):\n        legacy_map = {\n            \"qwen\": AgentId.QWEN_SONNET,\n            \"qwen_agent\": AgentId.DEVSTRAL_OPENHANDS,\n            \"architect\": AgentId.QWEN_SONNET,\n            \"coder\": AgentId.DEVSTRAL_OPENHANDS,\n            \"sven-gpt\": AgentId.QWEN_SONNET,\n            \"sven-coder\": AgentId.DEVSTRAL_OPENHANDS,\n            \"gpt-oss\": AgentId.QWEN_SONNET,\n            \"devstral\": AgentId.DEVSTRAL_OPENHANDS,\n        }\n        if agent_id.lower() in legacy_map:\n            return AGENTS[legacy_map[agent_id.lower()]]\n        logger.warning(f\"Unknown agent '{agent_id}', defaulting to QWEN_SONNET\")\n        return AGENTS[AgentId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som from ollama", "output": "```python\n    def from_ollama(cls, stats: StreamStats, model: str, agent_id: str = \"qwen-sonnet\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_eval_count,\n            provider=\"ollama\",\n            model=model,\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som from qwen agent", "output": "```python\n    def from_qwen_agent(cls, stats: QwenAgentStats, agent_id: str = \"devstral-openhands\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=0,\n            provider=\"qwen_agent\",\n            model=MODEL_WAVES_PRIMARY,\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-funktion: Get status of all agents", "output": "```python\n    async def get_status(self) -> dict:\n        \"\"\"Get status of all agents\"\"\"\n        ollama_ok = await self._ollama.is_connected()\n        nerdy_ok = await self._ollama_clients[\"nerdy\"].is_connected()\n\n        return {\n            \"agents\": {\n                \"qwen-sonnet\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_DAILY,\n                    \"display_name\": MODEL_DAILY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_DAILY_DESC\n                },\n                \"devstral-openhands\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_WAVES_PRIMARY,\n                    \"display_name\": MODEL_WAVES_PRIMARY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_WAVES_PRIMARY_DESC\n                },\n                \"qwen-openhands\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_WAVES_BACKUP,\n                    \"display_name\": MODEL_WAVES_BACKUP_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_WAVES_BACKUP_DESC\n                },\n                \"nerdy\": {\n                    \"available\": nerdy_ok,\n                    \"model\": \"qwen2.5:3b-instruct\",\n                    \"display_name\": \"NERDY\",\n                    \"provider\": \"ollama\",\n                    \"server\": \"192.168.86.27:11434\",\n                    \"description\": \"Legal/Compliance\"\n                }\n            },\n            \"providers\": {\n                \"ollama_local\": {\"connected\": ollama_ok, \"server\": \"localhost:11434\"},\n                \"ollama_nerdy\": {\"connected\": nerdy_ok, \"server\": \"192.168.86.27:11434\"},\n            }\n        }\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Agent identifiers matching frontend expectations", "output": "```python\nclass AgentId(str, Enum):\n    \"\"\"Agent identifiers matching frontend expectations\"\"\"\n    QWEN_SONNET = \"qwen-sonnet\"\n    DEVSTRAL_OPENHANDS = \"devstral-openhands\"\n    QWEN_OPENHANDS = \"qwen-openhands\"\n    # Legacy aliases\n    GPT_OSS = \"gpt-oss\"\n    DEVSTRAL = \"devstral\"\n    QWEN = \"qwen\"\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Backend providers", "output": "```python\nclass Provider(str, Enum):\n    \"\"\"Backend providers\"\"\"\n    OLLAMA = \"ollama\"\n    QWEN_AGENT = \"qwen_agent\"\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Unified statistics across providers", "output": "```python\nclass UnifiedStats:\n    \"\"\"Unified statistics across providers\"\"\"\n    tokens_generated: int = 0\n    tokens_per_second: float = 0.0\n    total_duration_ms: int = 0\n    prompt_tokens: int = 0\n    provider: str = \"unknown\"\n    model: str = \"unknown\"\n    agent_id: str = \"qwen-sonnet\"\n\n    @classmethod\n    def from_ollama(cls, stats: StreamStats, model: str, agent_id: str = \"qwen-sonnet\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_eval_count,\n            provider=\"ollama\",\n            model=model,\n            agent_id=agent_id\n        )\n\n    @classmethod\n    def from_qwen_agent(cls, stats: QwenAgentStats, agent_id: str = \"devstral-openhands\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=0,\n            provider=\"qwen_agent\",\n            model=MODEL_WAVES_PRIMARY,\n            agent_id=agent_id\n        )\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Routes requests to the appropriate AI agent", "output": "```python\nclass MultiAgentOrchestrator:\n    \"\"\"\n    Routes requests to the appropriate AI agent.\n    System prompts are in Modelfiles - we just forward messages.\n    \"\"\"\n\n    def __init__(self):\n        self._ollama = ollama_client\n        self._ollama_clients = {\n            \"qwen-sonnet\": ollama_client,\n            \"devstral-openhands\": ollama_client,\n            \"qwen-openhands\": ollama_client,\n            \"gpt-oss\": ollama_client,\n            \"devstral\": ollama_client,\n            \"nerdy\": OllamaClient(\"http://192.168.86.27:11434\"),\n        }\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        profile: str = \"qwen\",\n        temperature: Optional[float] = None,\n        max_tokens: int = 4096,\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str], None]:\n        \"\"\"\n        Stream chat completion to the specified agent.\n        NO system prompt injection - Modelfile has the prompt.\n        \"\"\"\n        agent = get_agent(profile)\n        temp = temperature if temperature is not None else agent.temperature\n\n        logger.info(\n            f\"[{request_id}] Routing to agent: {agent.display_name} \"\n            f\"(model={agent.model}, provider={agent.provider.value})\"\n        )\n\n        try:\n            if agent.provider == Provider.QWEN_AGENT:\n                async for token, stats in qwen_agent_provider.chat_stream(\n                    messages=messages,\n                    system_prompt=\"\",  # Empty - prompt in Modelfile\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_qwen_agent(stats, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n\n            else:  # OLLAMA (local models)\n                # NO system prompt injection - Modelfile has it\n                msgs_clean = [\n                    {\"role\": m.get(\"role\"), \"content\": m.get(\"content\")}\n                    for m in messages if m.get(\"role\") != \"system\"\n                ]\n\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 4096\n\n                profile_obj = SimpleProfile(agent.id.value, agent.model, temp, max_tokens)\n                agent_ollama = self._ollama_clients.get(agent.id.value, self._ollama)\n\n                async for token, stats in agent_ollama.chat_stream(\n                    profile=profile_obj,\n                    messages=msgs_clean,\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_ollama(stats, agent.model, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n\n        except OllamaError as e:\n            logger.error(f\"[{request_id}] Agent {agent.id.value} failed: {e}\")\n            raise\n\n    async def get_status(self) -> dict:\n        \"\"\"Get status of all agents\"\"\"\n        ollama_ok = await self._ollama.is_connected()\n        nerdy_ok = await self._ollama_clients[\"nerdy\"].is_connected()\n\n        return {\n            \"agents\": {\n                \"qwen-sonnet\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_DAILY,\n                    \"display_name\": MODEL_DAILY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_DAILY_DESC\n                },\n                \"devstral-openhands\": {\n                    \"available\": ollama_ok,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-klass för simple profile", "output": "```python\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 4096\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som time to first token ms", "output": "```python\n    def time_to_first_token_ms(self) -> Optional[int]:\n        if self.first_token_time and self.start_time:\n            return int((self.first_token_time - self.start_time) * 1000)\n        return None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get or create HTTP client", "output": "```python\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get model-specific configuration options", "output": "```python\n    def _get_model_options(self, profile: ModelProfile) -> dict:\n        \"\"\"\n        Get model-specific configuration options.\n        Uses optimized configs for the SVEN dual-expert system.\n        \"\"\"\n        # QWEN-SONNET (Daily Coder)\n        if MODEL_DAILY in profile.model or \"qwen-sonnet\" in profile.model:\n            logger.info(f\"Using QWEN_SONNET_CONFIG for {profile.model} with {QWEN_SONNET_CONFIG['num_ctx']} context\")\n            return QWEN_SONNET_CONFIG.copy()\n\n        # DEVSTRAL-OPENHANDS (Waves Primary)\n        if MODEL_WAVES_PRIMARY in profile.model or \"devstral\" in profile.model:\n            logger.info(f\"Using DEVSTRAL_CONFIG for {profile.model} with {DEVSTRAL_CONFIG['num_ctx']} context\")\n            return DEVSTRAL_CONFIG.copy()\n\n        # QWEN-OPENHANDS (Waves Backup)\n        if MODEL_WAVES_BACKUP in profile.model or \"qwen-openhands\" in profile.model:\n            logger.info(f\"Using QWEN_OPENHANDS_CONFIG for {profile.model} with {QWEN_OPENHANDS_CONFIG['num_ctx']} context\")\n            return QWEN_OPENHANDS_CONFIG.copy()\n\n        # Fallback to profile defaults\n        return {\n            \"temperature\": profile.temperature,\n            \"top_p\": profile.top_p,\n            \"repeat_penalty\": profile.repeat_penalty,\n            \"num_predict\": profile.max_tokens,\n            \"num_ctx\": profile.context_length,\n        }\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Close the HTTP client", "output": "```python\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Ollama server is reachable", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Ollama server is reachable\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(\n                f\"{self.base_url}/api/tags\",\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Ollama connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get Ollama version", "output": "```python\n    async def get_version(self) -> Optional[str]:\n        \"\"\"Get Ollama version\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/version\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"version\")\n        except Exception:\n            pass\n        return None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: List available models (downloaded)", "output": "```python\n    async def list_models(self) -> list[str]:\n        \"\"\"List available models (downloaded)\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/tags\")\n            if response.status_code == 200:\n                data = response.json()\n                return [m[\"name\"] for m in data.get(\"models\", [])]\n        except Exception as e:\n            logger.error(f\"Failed to list models: {e}\")\n        return []\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: List currently loaded/running models", "output": "```python\n    async def list_running_models(self) -> list[dict]:\n        \"\"\"List currently loaded/running models\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/ps\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"models\", [])\n        except Exception as e:\n            logger.error(f\"Failed to list running models: {e}\")\n        return []\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if a specific model is downloaded", "output": "```python\n    async def is_model_available(self, model_name: str) -> bool:\n        \"\"\"Check if a specific model is downloaded\"\"\"\n        models = await self.list_models()\n        # Handle both full names and short names\n        return any(\n            model_name in m or m.startswith(model_name.split(\":\")[0])\n            for m in models\n        )\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if model is currently loaded in memory", "output": "```python\n    async def is_model_loaded(self, model_name: str) -> bool:\n        \"\"\"Check if model is currently loaded in memory\"\"\"\n        running = await self.list_running_models()\n        return any(model_name in m.get(\"name\", \"\") for m in running)\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Unload a specific model from VRAM by setting keep_alive=0", "output": "```python\n    async def unload_model(self, model_name: str) -> bool:\n        \"\"\"Unload a specific model from VRAM by setting keep_alive=0\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.post(\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": model_name,\n                    \"prompt\": \"\",\n                    \"keep_alive\": 0\n                },\n                timeout=10.0\n            )\n            logger.info(f\"Unloaded model: {model_name}\")\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Failed to unload {model_name}: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Warm up a model by sending a minimal request", "output": "```python\n    async def warmup_model(self, profile: ModelProfile) -> bool:\n        \"\"\"\n        Warm up a model by sending a minimal request.\n        This pre-loads the model into GPU memory.\n        \"\"\"\n        logger.info(f\"Warming up model: {profile.model}\")\n        profile_manager.set_loading(profile.id)\n\n        # Auto-unload andra modeller först (RTX 4070 kan bara ha en)\n        unloaded = await self.unload_other_models(profile.model)\n        if unloaded:\n            logger.info(f\"Auto-unloaded models to free VRAM: {unloaded}\")\n\n        try:\n            client = await self._get_client()\n\n            # Send minimal generate request to load model\n            async with client.stream(\n                \"POST\",\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": profile.model,\n                    \"prompt\": \"Hi\",\n                    \"options\": {\"num_predict\": 1}\n                },\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=WARMUP_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            ) as response:\n                if response.status_code != 200:\n                    logger.error(f\"Warmup failed: {response.status_code}\")\n                    return False\n\n                # Consume stream to ensure model loads\n                async for _ in response.aiter_lines():\n                    pass\n\n            profile_manager.set_active(profile.id)\n            logger.info(f\"Model warmed up: {profile.model}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Warmup error for {profile.model}: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Non-streaming chat completion", "output": "```python\n    async def chat_complete(\n        self,\n        profile: ModelProfile,\n        messages: list[dict],\n        request_id: str\n    ) -> tuple[str, StreamStats]:\n        \"\"\"\n        Non-streaming chat completion.\n        Returns complete response and stats.\n        \"\"\"\n        full_response = []\n        stats = None\n\n        async for token, final_stats in self.chat_stream(profile, messages, request_id):\n            if token:\n                full_response.append(token)\n            if final_stats:\n                stats = final_stats\n\n        return \"\".join(full_response), stats\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics collected during streaming", "output": "```python\nclass StreamStats:\n    \"\"\"Statistics collected during streaming\"\"\"\n    tokens_generated: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n    prompt_eval_count: int = 0\n    prompt_eval_duration_ns: int = 0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n\n    @property\n    def time_to_first_token_ms(self) -> Optional[int]:\n        if self.first_token_time and self.start_time:\n            return int((self.first_token_time - self.start_time) * 1000)\n        return None\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Ollama errors", "output": "```python\nclass OllamaError(Exception):\n    \"\"\"Base exception for Ollama errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Ollama failed", "output": "```python\nclass OllamaConnectionError(OllamaError):\n    \"\"\"Connection to Ollama failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Requested model not available", "output": "```python\nclass OllamaModelNotFoundError(OllamaError):\n    \"\"\"Requested model not available\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Ollama API with streaming support", "output": "```python\nclass OllamaClient:\n    \"\"\"\n    Async client for Ollama API with streaming support.\n\n    Handles:\n    - Streaming chat completions\n    - Model availability checking\n    - Model loading/unloading tracking\n    - Connection health checks\n    \"\"\"\n\n    def __init__(self, base_url: str = OLLAMA_BASE_URL):\n        self.base_url = base_url\n        self._client: Optional[httpx.AsyncClient] = None\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    def _get_model_options(self, profile: ModelProfile) -> dict:\n        \"\"\"\n        Get model-specific configuration options.\n        Uses optimized configs for the SVEN dual-expert system.\n        \"\"\"\n        # QWEN-SONNET (Daily Coder)\n        if MODEL_DAILY in profile.model or \"qwen-sonnet\" in profile.model:\n            logger.info(f\"Using QWEN_SONNET_CONFIG for {profile.model} with {QWEN_SONNET_CONFIG['num_ctx']} context\")\n            return QWEN_SONNET_CONFIG.copy()\n\n        # DEVSTRAL-OPENHANDS (Waves Primary)\n        if MODEL_WAVES_PRIMARY in profile.model or \"devstral\" in profile.model:\n            logger.info(f\"Using DEVSTRAL_CONFIG for {profile.model} with {DEVSTRAL_CONFIG['num_ctx']} context\")\n            return DEVSTRAL_CONFIG.copy()\n\n        # QWEN-OPENHANDS (Waves Backup)\n        if MODEL_WAVES_BACKUP in profile.model or \"qwen-openhands\" in profile.model:\n            logger.info(f\"Using QWEN_OPENHANDS_CONFIG for {profile.model} with {QWEN_OPENHANDS_CONFIG['num_ctx']} context\")\n            return QWEN_OPENHANDS_CONFIG.copy()\n\n        # Fallback to profile defaults\n        return {\n            \"temperature\": profile.temperature,\n            \"top_p\": profile.top_p,\n            \"repeat_penalty\": profile.repeat_penalty,\n            \"num_predict\": profile.max_tokens,\n            \"num_ctx\": profile.context_length,\n        }\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Ollama server is reachable\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(\n                f\"{self.base_url}/api/tags\",\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Ollama connection check failed: {e}\")\n            return False\n\n    async def get_version(self) -> Optional[str]:\n        \"\"\"Get Ollama version\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/version\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"version\")\n        except Exception:\n            pass\n        return None\n\n    async def list_models(self) -> list[str]:\n        \"\"\"List available models (downloaded)\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/tags\")\n            if response.status_code == 200:\n                data = response.json()\n                return [m[\"name\"] for m in data.get(\"models\", [])]\n        except Exception as e:\n            logger.error(f\"Failed to list models: {e}\")\n        return []\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Convert to dictionary for JSON serialization", "output": "```python\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n        return asdict(self)\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get current system health statistics", "output": "```python\n    async def get_stats(self, force_refresh: bool = False) -> SystemHealth:\n        \"\"\"\n        Get current system health statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            SystemHealth with current system information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.cached_at).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.health\n\n            # Fetch fresh stats\n            health = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedHealth(\n                health=health,\n                cached_at=datetime.utcnow()\n            )\n\n            return health\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Fetch current system statistics", "output": "```python\n    async def _fetch_stats(self) -> SystemHealth:\n        \"\"\"Fetch current system statistics\"\"\"\n        try:\n            # CPU - use interval=None for non-blocking\n            cpu_percent = psutil.cpu_percent(interval=None)\n            cpu_count = psutil.cpu_count()\n\n            # Load average (Unix only)\n            try:\n                load_avg = os.getloadavg()\n            except (AttributeError, OSError):\n                load_avg = (0.0, 0.0, 0.0)\n\n            # RAM\n            ram = psutil.virtual_memory()\n\n            # Disk (root partition)\n            disk = psutil.disk_usage('/')\n\n            # Uptime\n            boot_time = psutil.boot_time()\n            uptime_seconds = int(time.time() - boot_time)\n            boot_time_str = datetime.fromtimestamp(boot_time).isoformat()\n\n            # GPU (from existing monitor)\n            gpu_stats = await gpu_monitor.get_stats()\n            gpu_dict = {\n                \"name\": gpu_stats.name,\n                \"vram_total_gb\": gpu_stats.vram_total_gb,\n                \"vram_used_gb\": gpu_stats.vram_used_gb,\n                \"vram_free_gb\": gpu_stats.vram_free_gb,\n                \"vram_percent\": gpu_stats.vram_percent,\n                \"temperature_c\": gpu_stats.temperature_c,\n                \"gpu_util_percent\": gpu_stats.gpu_util_percent,\n                \"power_draw_w\": gpu_stats.power_draw_w,\n                \"is_available\": gpu_stats.is_available,\n            }\n\n            return SystemHealth(\n                # CPU\n                cpu_percent=round(cpu_percent, 1),\n                cpu_count=cpu_count,\n                load_average=tuple(round(x, 2) for x in load_avg),\n\n                # RAM\n                ram_total_gb=round(ram.total / (1024**3), 2),\n                ram_used_gb=round(ram.used / (1024**3), 2),\n                ram_free_gb=round(ram.available / (1024**3), 2),\n                ram_percent=round(ram.percent, 1),\n\n                # Disk\n                disk_total_gb=round(disk.total / (1024**3), 2),\n                disk_used_gb=round(disk.used / (1024**3), 2),\n                disk_free_gb=round(disk.free / (1024**3), 2),\n                disk_percent=round((disk.used / disk.total) * 100, 1),\n\n                # System\n                uptime_seconds=uptime_seconds,\n                boot_time=boot_time_str,\n\n                # GPU\n                gpu=gpu_dict,\n\n                # Timestamp\n                timestamp=datetime.utcnow().isoformat() + \"Z\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch system stats: {e}\")\n            # Return minimal fallback\n            return SystemHealth(\n                cpu_percent=0.0,\n                cpu_count=0,\n                load_average=(0.0, 0.0, 0.0),\n                ram_total_gb=0.0,\n                ram_used_gb=0.0,\n                ram_free_gb=0.0,\n                ram_percent=0.0,\n                disk_total_gb=0.0,\n                disk_used_gb=0.0,\n                disk_free_gb=0.0,\n                disk_percent=0.0,\n                uptime_seconds=0,\n                boot_time=\"\",\n                gpu=None,\n                timestamp=datetime.utcnow().isoformat() + \"Z\"\n            )\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get minimal stats for WebSocket broadcast", "output": "```python\n    async def get_quick_summary(self) -> dict:\n        \"\"\"Get minimal stats for WebSocket broadcast\"\"\"\n        stats = await self.get_stats()\n        return {\n            \"cpu_percent\": stats.cpu_percent,\n            \"ram_percent\": stats.ram_percent,\n            \"disk_percent\": stats.disk_percent,\n            \"gpu_util_percent\": stats.gpu.get(\"gpu_util_percent\", 0) if stats.gpu else 0,\n            \"gpu_vram_percent\": stats.gpu.get(\"vram_percent\", 0) if stats.gpu else 0,\n        }\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Format uptime as human-readable string", "output": "```python\n    def format_uptime(self, seconds: int) -> str:\n        \"\"\"Format uptime as human-readable string\"\"\"\n        days = seconds // 86400\n        hours = (seconds % 86400) // 3600\n        minutes = (seconds % 3600) // 60\n\n        parts = []\n        if days > 0:\n            parts.append(f\"{days}d\")\n        if hours > 0:\n            parts.append(f\"{hours}h\")\n        if minutes > 0:\n            parts.append(f\"{minutes}m\")\n\n        return \" \".join(parts) if parts else \"< 1m\"\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Complete system health statistics", "output": "```python\nclass SystemHealth:\n    \"\"\"Complete system health statistics\"\"\"\n    # CPU\n    cpu_percent: float\n    cpu_count: int\n    load_average: tuple[float, float, float]  # 1, 5, 15 min\n\n    # RAM\n    ram_total_gb: float\n    ram_used_gb: float\n    ram_free_gb: float\n    ram_percent: float\n\n    # Disk\n    disk_total_gb: float\n    disk_used_gb: float\n    disk_free_gb: float\n    disk_percent: float\n\n    # System\n    uptime_seconds: int\n    boot_time: str\n\n    # GPU (from gpu_monitor)\n    gpu: Optional[dict] = None\n\n    # Timestamp\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n        return asdict(self)\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Cached system health with timestamp", "output": "```python\nclass CachedHealth:\n    \"\"\"Cached system health with timestamp\"\"\"\n    health: SystemHealth\n    cached_at: datetime\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Monitors system resources: CPU, RAM, Disk", "output": "```python\nclass SystemMonitor:\n    \"\"\"\n    Monitors system resources: CPU, RAM, Disk.\n\n    Features:\n    - Async-friendly with caching\n    - Integrates with existing GPU monitor\n    - Low overhead polling\n    \"\"\"\n\n    def __init__(self):\n        self._cache: Optional[CachedHealth] = None\n        self._lock = asyncio.Lock()\n\n    async def get_stats(self, force_refresh: bool = False) -> SystemHealth:\n        \"\"\"\n        Get current system health statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            SystemHealth with current system information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.cached_at).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.health\n\n            # Fetch fresh stats\n            health = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedHealth(\n                health=health,\n                cached_at=datetime.utcnow()\n            )\n\n            return health\n\n    async def _fetch_stats(self) -> SystemHealth:\n        \"\"\"Fetch current system statistics\"\"\"\n        try:\n            # CPU - use interval=None for non-blocking\n            cpu_percent = psutil.cpu_percent(interval=None)\n            cpu_count = psutil.cpu_count()\n\n            # Load average (Unix only)\n            try:\n                load_avg = os.getloadavg()\n            except (AttributeError, OSError):\n                load_avg = (0.0, 0.0, 0.0)\n\n            # RAM\n            ram = psutil.virtual_memory()\n\n            # Disk (root partition)\n            disk = psutil.disk_usage('/')\n\n            # Uptime\n            boot_time = psutil.boot_time()\n            uptime_seconds = int(time.time() - boot_time)\n            boot_time_str = datetime.fromtimestamp(boot_time).isoformat()\n\n            # GPU (from existing monitor)\n            gpu_stats = await gpu_monitor.get_stats()\n            gpu_dict = {\n                \"name\": gpu_stats.name,\n                \"vram_total_gb\": gpu_stats.vram_total_gb,\n                \"vram_used_gb\": gpu_stats.vram_used_gb,\n                \"vram_free_gb\": gpu_stats.vram_free_gb,\n                \"vram_percent\": gpu_stats.vram_percent,\n                \"temperature_c\": gpu_stats.temperature_c,\n                \"gpu_util_percent\": gpu_stats.gpu_util_percent,\n                \"power_draw_w\": gpu_stats.power_draw_w,\n                \"is_available\": gpu_stats.is_available,\n            }\n\n            return SystemHealth(\n                # CPU\n                cpu_percent=round(cpu_percent, 1),\n                cpu_count=cpu_count,\n                load_average=tuple(round(x, 2) for x in load_avg),\n\n                # RAM\n                ram_total_gb=round(ram.total / (1024**3), 2),\n                ram_used_gb=round(ram.used / (1024**3), 2),\n                ram_free_gb=round(ram.available / (1024**3), 2),\n                ram_percent=round(ram.percent, 1),\n\n                # Disk\n                disk_total_gb=round(disk.total / (1024**3), 2),\n                disk_used_gb=round(disk.used / (1024**3), 2),\n                disk_free_gb=round(disk.free / (1024**3), 2),\n                disk_percent=round((disk.used / disk.total) * 100, 1),\n\n                # System\n                uptime_seconds=uptime_seconds,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Analysera planner output för reward signals", "output": "```python\ndef analyze_plan(content: str) -> Dict[str, float]:\n    \"\"\"Analysera planner output för reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter tydliga steg\n    step_patterns = [\n        r'##\\s*IMPLEMENTATION\\s*STEG',\n        r'##\\s*STEG',\n        r'\\d+\\.\\s+\\[?Steg',\n        r'1\\.\\s+\\w+',\n    ]\n    signals['has_clear_steps'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in step_patterns\n    ) else 0.0\n\n    # Kolla efter fillista\n    file_patterns = [\n        r'##\\s*FILER',\n        r'- \\[?\\w+\\.(py|ts|js|tsx|jsx)\\]?',\n        r'`\\w+\\.\\w+`',\n    ]\n    signals['has_file_list'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in file_patterns\n    ) else 0.0\n\n    # Kolla efter utmaningar\n    challenge_patterns = [\n        r'##\\s*(POTENTIELLA\\s*)?UTMANINGAR',\n        r'##\\s*RISKER',\n        r'OBS:',\n        r'Utmaning:',\n    ]\n    signals['has_challenges'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in challenge_patterns\n    ) else 0.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Analysera coder output för reward signals", "output": "```python\ndef analyze_code(content: str) -> Dict[str, float]:\n    \"\"\"Analysera coder output för reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter kodblock\n    code_blocks = re.findall(r'```[\\w]*\\n[\\s\\S]*?\\n```', content)\n    signals['has_code_blocks'] = min(1.0, len(code_blocks) * 0.25)\n\n    # Kolla om koden ser komplett ut\n    completeness_signals = [\n        r'def\\s+\\w+\\s*\\(',  # Funktioner\n        r'class\\s+\\w+',     # Klasser\n        r'return\\s+',       # Return statements\n        r'if\\s+__name__',   # Main guard\n    ]\n    completeness_score = sum(\n        1 for p in completeness_signals if re.search(p, content)\n    ) / len(completeness_signals)\n    signals['code_is_complete'] = completeness_score\n\n    # Kolla efter imports\n    signals['has_imports'] = 1.0 if re.search(\n        r'(import\\s+\\w+|from\\s+\\w+\\s+import)', content\n    ) else 0.0\n\n    # Negativ: placeholders\n    placeholder_patterns = [\n        r'#\\s*\\.\\.\\.',\n        r'//\\s*\\.\\.\\.',\n        r'pass\\s*#\\s*TODO',\n        r'\\.\\.\\.\\s*resten',\n        r'# Add your code here',\n    ]\n    has_placeholders = any(\n        re.search(p, content, re.IGNORECASE) for p in placeholder_patterns\n    )\n    signals['no_placeholders'] = 0.0 if has_placeholders else 1.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Analysera reviewer output för reward signals", "output": "```python\ndef analyze_review(content: str) -> Dict[str, float]:\n    \"\"\"Analysera reviewer output för reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter bug-sektion\n    signals['has_bug_section'] = 1.0 if re.search(\n        r'##\\s*(BUGGAR|BUGS|FEL)', content, re.IGNORECASE\n    ) else 0.0\n\n    # Kolla efter verdict\n    signals['has_verdict'] = 1.0 if re.search(\n        r'##\\s*VERDICT|APPROVED|NEEDS\\s*CHANGES', content, re.IGNORECASE\n    ) else 0.0\n\n    # Kolla om approved\n    if re.search(r'APPROVED\\s*[✓✔]', content, re.IGNORECASE):\n        signals['is_approved'] = 1.0\n    elif re.search(r'NEEDS\\s*CHANGES\\s*[✗✘]', content, re.IGNORECASE):\n        signals['is_approved'] = 0.3  # Partial credit för att hitta problem\n    else:\n        signals['is_approved'] = 0.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Kör en träningsepisode med flera tasks", "output": "```python\nasync def run_training_episode(\n    agent: CascadeLitAgent,\n    tasks: List[Dict[str, Any]],\n    episode_id: str\n) -> List[float]:\n    \"\"\"\n    Kör en träningsepisode med flera tasks.\n\n    Args:\n        agent: CascadeLitAgent instance\n        tasks: Lista med training tasks\n        episode_id: Episode identifier\n\n    Returns:\n        Lista med rewards för varje task\n    \"\"\"\n    rewards = []\n\n    for i, task in enumerate(tasks):\n        rollout_id = f\"{episode_id}-task{i}\"\n        logger.info(f\"Running task {i+1}/{len(tasks)}: {rollout_id}\")\n\n        reward = await agent.training_rollout_async(\n            task=task,\n            rollout_id=rollout_id,\n            resources=None\n        )\n        rewards.append(reward)\n\n        logger.info(f\"Task {i+1} reward: {reward:.2f}\")\n\n    avg_reward = sum(rewards) / len(rewards) if rewards else 0\n    logger.info(f\"Episode {episode_id} complete - avg reward: {avg_reward:.2f}\")\n\n    return rewards\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Ladda training tasks från JSONL fil", "output": "```python\ndef load_training_tasks(path: str) -> List[Dict[str, Any]]:\n    \"\"\"Ladda training tasks från JSONL fil\"\"\"\n    import json\n\n    tasks = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                data = json.loads(line)\n                # Konvertera till task-format\n                tasks.append({\n                    \"question\": data.get(\"instruction\", \"\"),\n                    \"expected_answer\": data.get(\"output\", \"\"),\n                })\n\n    return tasks\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Beräkna total reward (-1 till +3 skala)", "output": "```python\n    def total(self) -> float:\n        \"\"\"Beräkna total reward (-1 till +3 skala)\"\"\"\n        weights = {\n            # Planning (max 0.6)\n            'has_clear_steps': 0.2,\n            'has_file_list': 0.2,\n            'has_challenges': 0.2,\n\n            # Coding (max 1.2)\n            'has_code_blocks': 0.3,\n            'code_is_complete': 0.4,\n            'has_imports': 0.2,\n            'no_placeholders': 0.3,\n\n            # Review (max 0.6)\n            'has_bug_section': 0.2,\n            'has_verdict': 0.2,\n            'is_approved': 0.2,\n\n            # Completion (max 0.6)\n            'task_completed': 0.6,\n        }\n\n        total = sum(\n            getattr(self, key) * weight\n            for key, weight in weights.items()\n        )\n\n        return min(3.0, max(-1.0, total))\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Huvudträningsloop - kör cascade och beräkna reward", "output": "```python\n    async def training_rollout_async(\n        self,\n        task: Dict[str, Any],\n        rollout_id: str,\n        resources: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Huvudträningsloop - kör cascade och beräkna reward.\n\n        Args:\n            task: Training task med \"question\" och optional \"expected_answer\"\n            rollout_id: Unik ID för denna rollout\n            resources: Tillgängliga resurser (ignoreras för nu)\n\n        Returns:\n            reward: Float mellan -1 och +3\n        \"\"\"\n        logger.info(f\"[{rollout_id}] Starting training rollout\")\n\n        # Extrahera task\n        question = task.get(\"question\", task.get(\"task\", \"\"))\n        expected = task.get(\"expected_answer\", task.get(\"expected\", \"\"))\n        skip_review = task.get(\"skip_review\", False)\n\n        if not question:\n            logger.error(f\"[{rollout_id}] Empty question in task\")\n            return -1.0\n\n        # Samla output från cascade\n        plan_content = \"\"\n        code_content = \"\"\n        review_content = \"\"\n        completed = False\n\n        try:\n            async for result in self.cascade.execute_cascade(\n                task=question,\n                request_id=rollout_id,\n                skip_review=skip_review\n            ):\n                if result.phase == CascadePhase.PLANNING:\n                    plan_content += result.content\n                elif result.phase == CascadePhase.CODING:\n                    code_content += result.content\n                elif result.phase == CascadePhase.REVIEWING:\n                    review_content += result.content\n                elif result.phase == CascadePhase.COMPLETE:\n                    completed = True\n\n        except Exception as e:\n            logger.error(f\"[{rollout_id}] Cascade failed: {e}\")\n            return -1.0\n\n        # Beräkna reward signals\n        signals = RewardSignals()\n\n        # Planning signals\n        if plan_content:\n            plan_signals = analyze_plan(plan_content)\n            signals.has_clear_steps = plan_signals.get('has_clear_steps', 0)\n            signals.has_file_list = plan_signals.get('has_file_list', 0)\n            signals.has_challenges = plan_signals.get('has_challenges', 0)\n\n        # Coding signals\n        if code_content:\n            code_signals = analyze_code(code_content)\n            signals.has_code_blocks = code_signals.get('has_code_blocks', 0)\n            signals.code_is_complete = code_signals.get('code_is_complete', 0)\n            signals.has_imports = code_signals.get('has_imports', 0)\n            signals.no_placeholders = code_signals.get('no_placeholders', 0)\n\n        # Review signals\n        if review_content:\n            review_signals = analyze_review(review_content)\n            signals.has_bug_section = review_signals.get('has_bug_section', 0)\n            signals.has_verdict = review_signals.get('has_verdict', 0)\n            signals.is_approved = review_signals.get('is_approved', 0)\n\n        # Completion bonus\n        signals.task_completed = 1.0 if completed else 0.0\n\n        # Extra reward för match mot expected (om tillgängligt)\n        if expected and code_content:\n            # Simple keyword matching\n            expected_keywords = set(re.findall(r'\\w{4,}', expected.lower()))\n            code_keywords = set(re.findall(r'\\w{4,}', code_content.lower()))\n            if expected_keywords:\n                overlap = len(expected_keywords & code_keywords) / len(expected_keywords)\n                signals.task_completed += overlap * 0.5\n\n        # Beräkna total reward\n        reward = signals.total()\n\n        logger.info(\n            f\"[{rollout_id}] Rollout complete - \"\n            f\"reward={reward:.2f}, \"\n            f\"plan={len(plan_content)}, code={len(code_content)}, \"\n            f\"review={len(review_content)}\"\n        )\n\n        return reward\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Inference mode (ej träning) - returnera cascade output", "output": "```python\n    async def inference_async(\n        self,\n        task: Dict[str, Any],\n        request_id: str\n    ) -> Dict[str, str]:\n        \"\"\"\n        Inference mode (ej träning) - returnera cascade output.\n\n        Args:\n            task: Task att köra\n            request_id: Request ID\n\n        Returns:\n            {\"plan\": \"...\", \"code\": \"...\", \"review\": \"...\"}\n        \"\"\"\n        return await self.cascade.quick_cascade(\n            task=task.get(\"question\", task.get(\"task\", \"\")),\n            request_id=request_id\n        )\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-klass: Reward signals från en cascade-körning", "output": "```python\nclass RewardSignals:\n    \"\"\"Reward signals från en cascade-körning\"\"\"\n    # Planning rewards\n    has_clear_steps: float = 0.0\n    has_file_list: float = 0.0\n    has_challenges: float = 0.0\n\n    # Coding rewards\n    has_code_blocks: float = 0.0\n    code_is_complete: float = 0.0\n    has_imports: float = 0.0\n    no_placeholders: float = 0.0\n\n    # Review rewards\n    has_bug_section: float = 0.0\n    has_verdict: float = 0.0\n    is_approved: float = 0.0\n\n    # Overall\n    task_completed: float = 0.0\n\n    def total(self) -> float:\n        \"\"\"Beräkna total reward (-1 till +3 skala)\"\"\"\n        weights = {\n            # Planning (max 0.6)\n            'has_clear_steps': 0.2,\n            'has_file_list': 0.2,\n            'has_challenges': 0.2,\n\n            # Coding (max 1.2)\n            'has_code_blocks': 0.3,\n            'code_is_complete': 0.4,\n            'has_imports': 0.2,\n            'no_placeholders': 0.3,\n\n            # Review (max 0.6)\n            'has_bug_section': 0.2,\n            'has_verdict': 0.2,\n            'is_approved': 0.2,\n\n            # Completion (max 0.6)\n            'task_completed': 0.6,\n        }\n\n        total = sum(\n            getattr(self, key) * weight\n            for key, weight in weights.items()\n        )\n\n        return min(3.0, max(-1.0, total))\n```\n\nKlassen finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-klass: LitAgent wrapper för Cascade Orchestrator", "output": "```python\nclass CascadeLitAgent(LitAgent):\n    \"\"\"\n    LitAgent wrapper för Cascade Orchestrator.\n\n    Möjliggör RL-träning av multi-agent cascade genom att:\n    1. Köra cascade på training tasks\n    2. Samla output från varje fas\n    3. Beräkna reward baserat på output-kvalitet\n    4. Returnera reward för GRPO/PPO training\n\n    Användning:\n        agent = CascadeLitAgent()\n        await agent.training_rollout_async(task, rollout_id, resources)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.cascade = cascade_orchestrator\n        self.ollama = ollama_client\n        logger.info(\"CascadeLitAgent initialized\")\n\n    async def training_rollout_async(\n        self,\n        task: Dict[str, Any],\n        rollout_id: str,\n        resources: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Huvudträningsloop - kör cascade och beräkna reward.\n\n        Args:\n            task: Training task med \"question\" och optional \"expected_answer\"\n            rollout_id: Unik ID för denna rollout\n            resources: Tillgängliga resurser (ignoreras för nu)\n\n        Returns:\n            reward: Float mellan -1 och +3\n        \"\"\"\n        logger.info(f\"[{rollout_id}] Starting training rollout\")\n\n        # Extrahera task\n        question = task.get(\"question\", task.get(\"task\", \"\"))\n        expected = task.get(\"expected_answer\", task.get(\"expected\", \"\"))\n        skip_review = task.get(\"skip_review\", False)\n\n        if not question:\n            logger.error(f\"[{rollout_id}] Empty question in task\")\n            return -1.0\n\n        # Samla output från cascade\n        plan_content = \"\"\n        code_content = \"\"\n        review_content = \"\"\n        completed = False\n\n        try:\n            async for result in self.cascade.execute_cascade(\n                task=question,\n                request_id=rollout_id,\n                skip_review=skip_review\n            ):\n                if result.phase == CascadePhase.PLANNING:\n                    plan_content += result.content\n                elif result.phase == CascadePhase.CODING:\n                    code_content += result.content\n                elif result.phase == CascadePhase.REVIEWING:\n                    review_content += result.content\n                elif result.phase == CascadePhase.COMPLETE:\n                    completed = True\n\n        except Exception as e:\n            logger.error(f\"[{rollout_id}] Cascade failed: {e}\")\n            return -1.0\n\n        # Beräkna reward signals\n        signals = RewardSignals()\n\n        # Planning signals\n        if plan_content:\n            plan_signals = analyze_plan(plan_content)\n            signals.has_clear_steps = plan_signals.get('has_clear_steps', 0)\n            signals.has_file_list = plan_signals.get('has_file_list', 0)\n            signals.has_challenges = plan_signals.get('has_challenges', 0)\n\n        # Coding signals\n        if code_content:\n            code_signals = analyze_code(code_content)\n            signals.has_code_blocks = code_signals.get('has_code_blocks', 0)\n            signals.code_is_complete = code_signals.get('code_is_complete', 0)\n            signals.has_imports = code_signals.get('has_imports', 0)\n            signals.no_placeholders = code_signals.get('no_placeholders', 0)\n\n        # Review signals\n        if review_content:\n            review_signals = analyze_review(review_content)\n            signals.has_bug_section = review_signals.get('has_bug_section', 0)\n            signals.has_verdict = review_signals.get('has_verdict', 0)\n            signals.is_approved = review_signals.get('is_approved', 0)\n\n        # Completion bonus\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Get Docker sandbox instance", "output": "```python\ndef get_docker_sandbox() -> DockerSandbox:\n    \"\"\"Get Docker sandbox instance\"\"\"\n    return docker_sandbox\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Create a fresh Assistant instance", "output": "```python\n    def _create_assistant(self, system_prompt: str) -> Assistant:\n        \"\"\"Create a fresh Assistant instance\"\"\"\n        return Assistant(\n            function_list=self.function_list,\n            llm=self.llm_cfg,\n            name='QWEN AGENT',\n            description='Elite coder med Code Interpreter',\n            system_message=system_prompt\n        )\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Stream response från Qwen-Agent med Code Interpreter support", "output": "```python\n    async def chat_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        system_prompt: str,\n        request_id: str\n    ) -> AsyncGenerator[tuple[str, Optional[QwenAgentStats]], None]:\n        \"\"\"\n        Stream response från Qwen-Agent med Code Interpreter support.\n\n        Args:\n            messages: Lista med {\"role\": \"user/assistant\", \"content\": \"...\"}\n            system_prompt: System message för agenten\n            request_id: Unik request ID för logging\n\n        Yields:\n            (token_text, None) för streaming\n            (\"\", QwenAgentStats) som sista yield med statistik\n        \"\"\"\n        logger.info(f\"[{request_id}] QwenAgentProvider: Starting chat with Code Interpreter\")\n\n        start_time = time.time()\n        token_count = 0\n        tool_calls = 0\n        code_executed = False\n        last_content = \"\"\n\n        try:\n            assistant = self._create_assistant(system_prompt)\n\n            # Konvertera messages till qwen-agent format\n            qwen_messages = []\n            for msg in messages:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue  # System message hanteras separat\n                qwen_messages.append({\"role\": role, \"content\": content})\n\n            # Kör qwen-agent i thread pool (assistant.run() är synkron)\n            loop = asyncio.get_event_loop()\n\n            def run_agent():\n                \"\"\"Synkron körning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n\n            # Kör i bakgrunden och streama resultat\n            all_responses = await loop.run_in_executor(_executor, run_agent)\n\n            # Processa alla responses\n            for response_list in all_responses:\n                if not response_list:\n                    continue\n\n                last_msg = response_list[-1]\n\n                # Kolla om det finns tool calls\n                if last_msg.get(\"function_call\"):\n                    tool_calls += 1\n                    code_executed = True\n                    logger.debug(f\"[{request_id}] Tool call detected: {last_msg.get('function_call', {}).get('name', 'unknown')}\")\n\n                # Extrahera content\n                content = last_msg.get(\"content\", \"\")\n                if isinstance(content, str) and content and content != last_content:\n                    # Streama ny content\n                    new_content = content[len(last_content):]\n                    if new_content:\n                        token_count += len(new_content.split())\n                        yield new_content, None\n                    last_content = content\n\n            # Beräkna statistik\n            duration_ms = int((time.time() - start_time) * 1000)\n            tokens_per_sec = token_count / (duration_ms / 1000) if duration_ms > 0 else 0\n\n            logger.info(\n                f\"[{request_id}] QwenAgentProvider: Completed - \"\n                f\"tokens={token_count}, tool_calls={tool_calls}, \"\n                f\"duration={duration_ms}ms, speed={tokens_per_sec:.1f} t/s\"\n            )\n\n            # Yield final stats\n            yield \"\", QwenAgentStats(\n                tokens_generated=token_count,\n                tokens_per_second=tokens_per_sec,\n                total_duration_ms=duration_ms,\n                code_executed=code_executed,\n                tool_calls=tool_calls\n            )\n\n        except Exception as e:\n            logger.error(f\"[{request_id}] QwenAgentProvider error: {e}\")\n            # Yield error message\n            yield f\"\\n\\n[ERROR] Code Interpreter failed: {str(e)}\", None\n\n            # Yield stats anyway\n            duration_ms = int((time.time() - start_time) * 1000)\n            yield \"\", QwenAgentStats(\n                tokens_generated=token_count,\n                tokens_per_second=0,\n                total_duration_ms=duration_ms,\n                code_executed=code_executed,\n                tool_calls=tool_calls\n            )\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Kör Python-kod i isolerad Docker-container", "output": "```python\n    async def execute(self, code: str) -> Dict[str, Any]:\n        \"\"\"\n        Kör Python-kod i isolerad Docker-container.\n\n        Args:\n            code: Python-kod att köra\n\n        Returns:\n            {\"success\": bool, \"output\": str, \"error\": str, \"duration_ms\": int}\n        \"\"\"\n        import tempfile\n        import time\n\n        start_time = time.time()\n\n        try:\n            # Skriv kod till temporär fil\n            with tempfile.NamedTemporaryFile(\n                mode='w',\n                suffix='.py',\n                delete=False\n            ) as f:\n                f.write(code)\n                script_path = f.name\n\n            logger.info(f\"DockerSandbox: Executing code in {self.image}\")\n\n            # Kör i Docker med säkerhetsbegränsningar\n            proc = await asyncio.wait_for(\n                asyncio.create_subprocess_exec(\n                    \"docker\", \"run\", \"--rm\",\n                    \"--network=none\",              # Ingen internet\n                    f\"--memory={self.memory_limit}\",  # RAM-gräns\n                    f\"--cpus={self.cpu_limit}\",    # CPU-gräns\n                    \"--read-only\",                 # Read-only filesystem\n                    \"--tmpfs=/tmp:size=64m\",       # Tillfällig skrivbar /tmp\n                    \"-v\", f\"{script_path}:/sandbox/script.py:ro\",\n                    self.image,\n                    \"python\", \"/sandbox/script.py\",\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE\n                ),\n                timeout=self.timeout\n            )\n\n            stdout, stderr = await proc.communicate()\n            duration_ms = int((time.time() - start_time) * 1000)\n\n            # Rensa temp-fil\n            import os\n            os.unlink(script_path)\n\n            if proc.returncode == 0:\n                logger.info(f\"DockerSandbox: Success ({duration_ms}ms)\")\n                return {\n                    \"success\": True,\n                    \"output\": stdout.decode()[:10000],  # Max 10KB output\n                    \"error\": \"\",\n                    \"duration_ms\": duration_ms\n                }\n            else:\n                logger.warning(f\"DockerSandbox: Failed with code {proc.returncode}\")\n                return {\n                    \"success\": False,\n                    \"output\": stdout.decode()[:5000],\n                    \"error\": stderr.decode()[:5000],\n                    \"duration_ms\": duration_ms\n                }\n\n        except asyncio.TimeoutError:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Timeout after {self.timeout}s\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": f\"Timeout efter {self.timeout} sekunder\",\n                \"duration_ms\": duration_ms\n            }\n        except Exception as e:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Error - {e}\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": str(e),\n                \"duration_ms\": duration_ms\n            }\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Kolla om Docker och sandbox-image finns", "output": "```python\n    async def is_available(self) -> bool:\n        \"\"\"Kolla om Docker och sandbox-image finns\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"docker\", \"image\", \"inspect\", self.image,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            return proc.returncode == 0\n        except Exception:\n            return False\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Synkron körning av qwen-agent", "output": "```python\n            def run_agent():\n                \"\"\"Synkron körning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Qwen-Agent execution", "output": "```python\nclass QwenAgentStats:\n    \"\"\"Statistics from Qwen-Agent execution\"\"\"\n    tokens_generated: int = 0\n    tokens_per_second: float = 0.0\n    total_duration_ms: int = 0\n    code_executed: bool = False\n    tool_calls: int = 0\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: Provider för Qwen-Agent med Code Interpreter, MCP och Web Search", "output": "```python\nclass QwenAgentProvider:\n    \"\"\"\n    Provider för Qwen-Agent med Code Interpreter, MCP och Web Search.\n\n    Använder Ollama som backend och ger tillgång till:\n    - code_interpreter: Kör Python-kod direkt\n    - web_search: Sök på webben\n    - file tools: Läs/skriv filer\n    - MCP servers: Externa verktyg (filesystem, etc.)\n    \"\"\"\n\n    def __init__(self):\n        self.llm_cfg = {\n            'model': 'devstral:24b',  # Devstral 24B - Code Interpreter\n            'model_server': 'http://localhost:11434/v1',\n            'api_key': 'ollama',\n            'generate_cfg': {\n                'max_input_tokens': 8192,\n                'max_retries': 3,\n            }\n        }\n\n        # Initialize MCP Manager if available\n        self.mcp_manager = None\n        if MCP_AVAILABLE:\n            try:\n                self.mcp_manager = MCPManager()\n                logger.info(\"MCPManager initialized successfully\")\n            except Exception as e:\n                logger.warning(f\"Failed to initialize MCPManager: {e}\")\n\n        # Build function list:\n        # 1. Code Interpreter (built-in)\n        # 2. Web Search\n        # 3. File Tools (custom)\n        self.function_list = [\n            'code_interpreter',\n            WebSearch(),\n        ] + FILE_TOOLS\n\n        logger.info(f\"QwenAgentProvider initialized with {len(self.function_list)} tools\")\n\n    def _create_assistant(self, system_prompt: str) -> Assistant:\n        \"\"\"Create a fresh Assistant instance\"\"\"\n        return Assistant(\n            function_list=self.function_list,\n            llm=self.llm_cfg,\n            name='QWEN AGENT',\n            description='Elite coder med Code Interpreter',\n            system_message=system_prompt\n        )\n\n    async def chat_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        system_prompt: str,\n        request_id: str\n    ) -> AsyncGenerator[tuple[str, Optional[QwenAgentStats]], None]:\n        \"\"\"\n        Stream response från Qwen-Agent med Code Interpreter support.\n\n        Args:\n            messages: Lista med {\"role\": \"user/assistant\", \"content\": \"...\"}\n            system_prompt: System message för agenten\n            request_id: Unik request ID för logging\n\n        Yields:\n            (token_text, None) för streaming\n            (\"\", QwenAgentStats) som sista yield med statistik\n        \"\"\"\n        logger.info(f\"[{request_id}] QwenAgentProvider: Starting chat with Code Interpreter\")\n\n        start_time = time.time()\n        token_count = 0\n        tool_calls = 0\n        code_executed = False\n        last_content = \"\"\n\n        try:\n            assistant = self._create_assistant(system_prompt)\n\n            # Konvertera messages till qwen-agent format\n            qwen_messages = []\n            for msg in messages:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue  # System message hanteras separat\n                qwen_messages.append({\"role\": role, \"content\": content})\n\n            # Kör qwen-agent i thread pool (assistant.run() är synkron)\n            loop = asyncio.get_event_loop()\n\n            def run_agent():\n                \"\"\"Synkron körning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: Säker kodexekvering i Docker-container", "output": "```python\nclass DockerSandbox:\n    \"\"\"\n    Säker kodexekvering i Docker-container.\n    Isolerar kod från värdmaskinen med:\n    - Ingen nätverksåtkomst (--network=none)\n    - Begränsad RAM (--memory)\n    - Begränsad CPU (--cpus)\n    - Icke-root användare\n    \"\"\"\n\n    def __init__(\n        self,\n        image: str = \"ai-sandbox\",\n        memory_limit: str = \"512m\",\n        cpu_limit: float = 0.5,\n        timeout: int = 30\n    ):\n        self.image = image\n        self.memory_limit = memory_limit\n        self.cpu_limit = cpu_limit\n        self.timeout = timeout\n\n    async def execute(self, code: str) -> Dict[str, Any]:\n        \"\"\"\n        Kör Python-kod i isolerad Docker-container.\n\n        Args:\n            code: Python-kod att köra\n\n        Returns:\n            {\"success\": bool, \"output\": str, \"error\": str, \"duration_ms\": int}\n        \"\"\"\n        import tempfile\n        import time\n\n        start_time = time.time()\n\n        try:\n            # Skriv kod till temporär fil\n            with tempfile.NamedTemporaryFile(\n                mode='w',\n                suffix='.py',\n                delete=False\n            ) as f:\n                f.write(code)\n                script_path = f.name\n\n            logger.info(f\"DockerSandbox: Executing code in {self.image}\")\n\n            # Kör i Docker med säkerhetsbegränsningar\n            proc = await asyncio.wait_for(\n                asyncio.create_subprocess_exec(\n                    \"docker\", \"run\", \"--rm\",\n                    \"--network=none\",              # Ingen internet\n                    f\"--memory={self.memory_limit}\",  # RAM-gräns\n                    f\"--cpus={self.cpu_limit}\",    # CPU-gräns\n                    \"--read-only\",                 # Read-only filesystem\n                    \"--tmpfs=/tmp:size=64m\",       # Tillfällig skrivbar /tmp\n                    \"-v\", f\"{script_path}:/sandbox/script.py:ro\",\n                    self.image,\n                    \"python\", \"/sandbox/script.py\",\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE\n                ),\n                timeout=self.timeout\n            )\n\n            stdout, stderr = await proc.communicate()\n            duration_ms = int((time.time() - start_time) * 1000)\n\n            # Rensa temp-fil\n            import os\n            os.unlink(script_path)\n\n            if proc.returncode == 0:\n                logger.info(f\"DockerSandbox: Success ({duration_ms}ms)\")\n                return {\n                    \"success\": True,\n                    \"output\": stdout.decode()[:10000],  # Max 10KB output\n                    \"error\": \"\",\n                    \"duration_ms\": duration_ms\n                }\n            else:\n                logger.warning(f\"DockerSandbox: Failed with code {proc.returncode}\")\n                return {\n                    \"success\": False,\n                    \"output\": stdout.decode()[:5000],\n                    \"error\": stderr.decode()[:5000],\n                    \"duration_ms\": duration_ms\n                }\n\n        except asyncio.TimeoutError:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Timeout after {self.timeout}s\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": f\"Timeout efter {self.timeout} sekunder\",\n                \"duration_ms\": duration_ms\n            }\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Check if nvidia-smi is available", "output": "```python\n    async def _check_nvidia_smi(self) -> bool:\n        \"\"\"Check if nvidia-smi is available\"\"\"\n        if self._nvidia_smi_available is not None:\n            return self._nvidia_smi_available\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--version\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            self._nvidia_smi_available = proc.returncode == 0\n        except FileNotFoundError:\n            self._nvidia_smi_available = False\n        except Exception as e:\n            logger.warning(f\"nvidia-smi check failed: {e}\")\n            self._nvidia_smi_available = False\n\n        return self._nvidia_smi_available\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Run nvidia-smi with arguments and return stdout", "output": "```python\n    async def _run_nvidia_smi(self, args: list[str]) -> Optional[str]:\n        \"\"\"Run nvidia-smi with arguments and return stdout\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", *args,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            stdout, stderr = await proc.communicate()\n\n            if proc.returncode == 0:\n                return stdout.decode(\"utf-8\").strip()\n            else:\n                logger.warning(f\"nvidia-smi failed: {stderr.decode()}\")\n                return None\n\n        except Exception as e:\n            logger.error(f\"Error running nvidia-smi: {e}\")\n            return None\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Fetch current GPU stats from nvidia-smi", "output": "```python\n    async def _fetch_stats(self) -> GPUStats:\n        \"\"\"Fetch current GPU stats from nvidia-smi\"\"\"\n        # Query format: index,name,temp,util,mem_util,mem_total,mem_used,mem_free,power_draw,power_limit,fan\n        query = (\n            \"--query-gpu=\"\n            \"index,name,temperature.gpu,utilization.gpu,utilization.memory,\"\n            \"memory.total,memory.used,memory.free,\"\n            \"power.draw,power.limit,fan.speed,\"\n            \"driver_version\"\n        )\n\n        output = await self._run_nvidia_smi([query, \"--format=csv,noheader,nounits\"])\n\n        if not output:\n            return self._get_fallback_stats()\n\n        try:\n            # Parse CSV output\n            parts = [p.strip() for p in output.split(\",\")]\n\n            if len(parts) < 11:\n                logger.warning(f\"Unexpected nvidia-smi output: {output}\")\n                return self._get_fallback_stats()\n\n            # Parse values with error handling\n            def safe_float(val: str, default: float = 0.0) -> float:\n                try:\n                    return float(val)\n                except (ValueError, TypeError):\n                    return default\n\n            def safe_int(val: str, default: int = 0) -> int:\n                try:\n                    return int(float(val))\n                except (ValueError, TypeError):\n                    return default\n\n            vram_total = safe_float(parts[5]) / 1024  # MiB to GB\n            vram_used = safe_float(parts[6]) / 1024\n            vram_free = safe_float(parts[7]) / 1024\n\n            stats = GPUStats(\n                name=parts[1].strip(),\n                vram_total_gb=round(vram_total, 2),\n                vram_used_gb=round(vram_used, 2),\n                vram_free_gb=round(vram_free, 2),\n                vram_percent=round((vram_used / vram_total) * 100, 1) if vram_total > 0 else 0,\n                temperature_c=safe_int(parts[2]),\n                gpu_util_percent=safe_int(parts[3]),\n                memory_util_percent=safe_int(parts[4]),\n                power_draw_w=safe_int(parts[8]),\n                power_limit_w=safe_int(parts[9]),\n                fan_speed_percent=safe_int(parts[10]) if parts[10] != \"[N/A]\" else None,\n                driver_version=parts[11].strip() if len(parts) > 11 else None,\n                is_available=True\n            )\n\n            return stats\n\n        except Exception as e:\n            logger.error(f\"Failed to parse nvidia-smi output: {e}\")\n            return self._get_fallback_stats()\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Return fallback stats when GPU monitoring unavailable", "output": "```python\n    def _get_fallback_stats(self) -> GPUStats:\n        \"\"\"Return fallback stats when GPU monitoring unavailable\"\"\"\n        return GPUStats(\n            name=\"NVIDIA GeForce RTX 4070\",\n            vram_total_gb=12.0,\n            vram_used_gb=0.0,\n            vram_free_gb=12.0,\n            vram_percent=0.0,\n            temperature_c=0,\n            power_draw_w=0,\n            power_limit_w=200,\n            gpu_util_percent=0,\n            memory_util_percent=0,\n            is_available=False\n        )\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get CUDA version from nvidia-smi", "output": "```python\n    async def get_cuda_version(self) -> Optional[str]:\n        \"\"\"Get CUDA version from nvidia-smi\"\"\"\n        output = await self._run_nvidia_smi([])\n\n        if output:\n            # Parse CUDA version from standard nvidia-smi output\n            match = re.search(r\"CUDA Version:\\s*(\\d+\\.\\d+)\", output)\n            if match:\n                return match.group(1)\n\n        return None\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Check if GPU is available and accessible", "output": "```python\n    async def is_gpu_available(self) -> bool:\n        \"\"\"Check if GPU is available and accessible\"\"\"\n        return await self._check_nvidia_smi()\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Quick method to get just VRAM usage", "output": "```python\n    async def get_vram_usage_gb(self) -> float:\n        \"\"\"Quick method to get just VRAM usage\"\"\"\n        stats = await self.get_stats()\n        return stats.vram_used_gb\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Check if there's enough free VRAM for a model", "output": "```python\n    async def has_sufficient_vram(self, required_gb: float) -> bool:\n        \"\"\"Check if there's enough free VRAM for a model\"\"\"\n        stats = await self.get_stats()\n        return stats.vram_free_gb >= required_gb\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Skapa en Python-funktion som safe float", "output": "```python\n            def safe_float(val: str, default: float = 0.0) -> float:\n                try:\n                    return float(val)\n                except (ValueError, TypeError):\n                    return default\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Skapa en Python-funktion som safe int", "output": "```python\n            def safe_int(val: str, default: int = 0) -> int:\n                try:\n                    return int(float(val))\n                except (ValueError, TypeError):\n                    return default\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-klass: Cached GPU statistics with timestamp", "output": "```python\nclass CachedStats:\n    \"\"\"Cached GPU statistics with timestamp\"\"\"\n    stats: GPUStats\n    timestamp: datetime\n```\n\nKlassen finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-klass: Monitors NVIDIA GPU statistics using nvidia-smi", "output": "```python\nclass GPUMonitor:\n    \"\"\"\n    Monitors NVIDIA GPU statistics using nvidia-smi.\n\n    Features:\n    - Async subprocess calls to nvidia-smi\n    - Caching to avoid excessive calls\n    - Fallback values when GPU unavailable\n    \"\"\"\n\n    def __init__(self):\n        self._cache: Optional[CachedStats] = None\n        self._lock = asyncio.Lock()\n        self._nvidia_smi_available: Optional[bool] = None\n\n    async def _check_nvidia_smi(self) -> bool:\n        \"\"\"Check if nvidia-smi is available\"\"\"\n        if self._nvidia_smi_available is not None:\n            return self._nvidia_smi_available\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--version\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            self._nvidia_smi_available = proc.returncode == 0\n        except FileNotFoundError:\n            self._nvidia_smi_available = False\n        except Exception as e:\n            logger.warning(f\"nvidia-smi check failed: {e}\")\n            self._nvidia_smi_available = False\n\n        return self._nvidia_smi_available\n\n    async def _run_nvidia_smi(self, args: list[str]) -> Optional[str]:\n        \"\"\"Run nvidia-smi with arguments and return stdout\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", *args,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            stdout, stderr = await proc.communicate()\n\n            if proc.returncode == 0:\n                return stdout.decode(\"utf-8\").strip()\n            else:\n                logger.warning(f\"nvidia-smi failed: {stderr.decode()}\")\n                return None\n\n        except Exception as e:\n            logger.error(f\"Error running nvidia-smi: {e}\")\n            return None\n\n    async def get_stats(self, force_refresh: bool = False) -> GPUStats:\n        \"\"\"\n        Get current GPU statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            GPUStats with current GPU information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.timestamp).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.stats\n\n            # Check if nvidia-smi is available\n            if not await self._check_nvidia_smi():\n                return self._get_fallback_stats()\n\n            # Fetch fresh stats\n            stats = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedStats(\n                stats=stats,\n                timestamp=datetime.utcnow()\n            )\n\n            return stats\n\n    async def _fetch_stats(self) -> GPUStats:\n        \"\"\"Fetch current GPU stats from nvidia-smi\"\"\"\n        # Query format: index,name,temp,util,mem_util,mem_total,mem_used,mem_free,power_draw,power_limit,fan\n        query = (\n            \"--query-gpu=\"\n            \"index,name,temperature.gpu,utilization.gpu,utilization.memory,\"\n            \"memory.total,memory.used,memory.free,\"\n            \"power.draw,power.limit,fan.speed,\"\n            \"driver_version\"\n        )\n\n        output = await self._run_nvidia_smi([query, \"--format=csv,noheader,nounits\"])\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Returnera verktygs-definitioner för system prompt", "output": "```python\ndef get_tool_definitions() -> dict[str, dict]:\n    \"\"\"Returnera verktygs-definitioner för system prompt\"\"\"\n    return {\n        name: {\n            \"description\": tool.description,\n            \"safety\": tool.safety.value,\n            \"params\": tool.params\n        }\n        for name, tool in TOOLS.items()\n    }\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kör ett verktyg och returnera resultat", "output": "```python\n    async def execute(self, tool_name: str, params: dict) -> ToolResult:\n        \"\"\"Kör ett verktyg och returnera resultat\"\"\"\n        import time\n        start_time = time.time()\n\n        if tool_name not in TOOLS:\n            return ToolResult(\n                success=False,\n                error=f\"Okänt verktyg: {tool_name}\",\n                tool_name=tool_name\n            )\n\n        tool = TOOLS[tool_name]\n\n        # Validera obligatoriska parametrar\n        missing = [p for p in tool.params if p not in params and p != \"lines\"]\n        if missing:\n            return ToolResult(\n                success=False,\n                error=f\"Saknar obligatoriska parametrar: {', '.join(missing)}\",\n                tool_name=tool_name\n            )\n\n        # Dispatch till rätt metod\n        method = getattr(self, f\"_exec_{tool_name}\", None)\n        if not method:\n            return ToolResult(\n                success=False,\n                error=f\"Verktyget {tool_name} är inte implementerat\",\n                tool_name=tool_name\n            )\n\n        try:\n            result = await asyncio.wait_for(\n                method(params),\n                timeout=self.timeout\n            )\n            result.tool_name = tool_name\n            result.duration_ms = int((time.time() - start_time) * 1000)\n            logger.info(f\"Tool executed: {tool_name} (success={result.success}, {result.duration_ms}ms)\")\n            return result\n\n        except asyncio.TimeoutError:\n            logger.error(f\"Tool timeout: {tool_name}\")\n            return ToolResult(\n                success=False,\n                error=f\"Timeout efter {self.timeout} sekunder\",\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n        except Exception as e:\n            logger.error(f\"Tool error: {tool_name} - {e}\")\n            return ToolResult(\n                success=False,\n                error=str(e),\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista alla Docker containers", "output": "```python\n    async def _exec_docker_ps(self, params: dict) -> ToolResult:\n        \"\"\"Lista alla Docker containers\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"ps\", \"-a\", \"--format\",\n            \"table {{.Names}}\\t{{.Status}}\\t{{.Image}}\\t{{.Ports}}\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa container-loggar", "output": "```python\n    async def _exec_docker_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa container-loggar\"\"\"\n        container = params[\"container\"]\n        lines = params.get(\"lines\", 50)\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"logs\", \"--tail\", str(lines), container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        # Docker skriver ofta loggar till stderr\n        output = stdout.decode() + stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta om en container", "output": "```python\n    async def _exec_docker_restart(self, params: dict) -> ToolResult:\n        \"\"\"Starta om en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"restart\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har startats om\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Stoppa en container", "output": "```python\n    async def _exec_docker_stop(self, params: dict) -> ToolResult:\n        \"\"\"Stoppa en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"stop\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har stoppats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta en container", "output": "```python\n    async def _exec_docker_start(self, params: dict) -> ToolResult:\n        \"\"\"Starta en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"start\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har startats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa status för en systemd-tjänst", "output": "```python\n    async def _exec_service_status(self, params: dict) -> ToolResult:\n        \"\"\"Visa status för en systemd-tjänst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"systemctl\", \"status\", service, \"--no-pager\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        # systemctl status returnerar non-zero för inaktiva tjänster\n        output = stdout.decode() or stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa journalctl-loggar för en tjänst", "output": "```python\n    async def _exec_service_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa journalctl-loggar för en tjänst\"\"\"\n        service = params[\"service\"]\n        lines = params.get(\"lines\", 50)\n\n        proc = await asyncio.create_subprocess_exec(\n            \"journalctl\", \"-u\", service, \"-n\", str(lines), \"--no-pager\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode() or stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta om en systemd-tjänst", "output": "```python\n    async def _exec_service_restart(self, params: dict) -> ToolResult:\n        \"\"\"Starta om en systemd-tjänst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"restart\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tjänsten '{service}' har startats om\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Stoppa en systemd-tjänst", "output": "```python\n    async def _exec_service_stop(self, params: dict) -> ToolResult:\n        \"\"\"Stoppa en systemd-tjänst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"stop\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tjänsten '{service}' har stoppats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta en systemd-tjänst", "output": "```python\n    async def _exec_service_start(self, params: dict) -> ToolResult:\n        \"\"\"Starta en systemd-tjänst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"start\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tjänsten '{service}' har startats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kontrollera att sökväg är tillåten", "output": "```python\n    def _is_allowed_path(self, path: str) -> bool:\n        \"\"\"Kontrollera att sökväg är tillåten\"\"\"\n        try:\n            resolved = Path(path).resolve()\n            return any(str(resolved).startswith(p) for p in self.allowed_paths)\n        except Exception:\n            return False\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Läs innehållet i en fil", "output": "```python\n    async def _exec_file_read(self, params: dict) -> ToolResult:\n        \"\"\"Läs innehållet i en fil\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otillåten sökväg: {path}. Tillåtna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            file_path = Path(path)\n            if not file_path.exists():\n                return ToolResult(success=False, error=f\"Filen finns inte: {path}\")\n\n            if file_path.is_dir():\n                return ToolResult(success=False, error=f\"Sökvägen är en katalog, använd file_list istället\")\n\n            content = file_path.read_text(encoding='utf-8', errors='replace')\n            return ToolResult(success=True, output=content[:self.max_output_size])\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen läsbehörighet för: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Skriv innehåll till en fil", "output": "```python\n    async def _exec_file_write(self, params: dict) -> ToolResult:\n        \"\"\"Skriv innehåll till en fil\"\"\"\n        path = params[\"path\"]\n        content = params[\"content\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otillåten sökväg: {path}. Tillåtna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            file_path = Path(path)\n\n            # Skapa parent-kataloger om de inte finns\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n\n            file_path.write_text(content, encoding='utf-8')\n            return ToolResult(success=True, output=f\"Skrev {len(content)} tecken till {path}\")\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen skrivbehörighet för: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista filer i en katalog", "output": "```python\n    async def _exec_file_list(self, params: dict) -> ToolResult:\n        \"\"\"Lista filer i en katalog\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otillåten sökväg: {path}. Tillåtna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            dir_path = Path(path)\n            if not dir_path.exists():\n                return ToolResult(success=False, error=f\"Katalogen finns inte: {path}\")\n\n            if not dir_path.is_dir():\n                return ToolResult(success=False, error=f\"Sökvägen är inte en katalog: {path}\")\n\n            entries = []\n            for entry in sorted(dir_path.iterdir()):\n                entry_type = \"D\" if entry.is_dir() else \"F\"\n                size = entry.stat().st_size if entry.is_file() else 0\n                entries.append(f\"[{entry_type}] {entry.name} ({size} bytes)\")\n\n            output = f\"Innehåll i {path}:\\n\" + \"\\n\".join(entries[:100])  # Max 100 entries\n            return ToolResult(success=True, output=output)\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen läsbehörighet för: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kontrollera om en fil finns", "output": "```python\n    async def _exec_file_exists(self, params: dict) -> ToolResult:\n        \"\"\"Kontrollera om en fil finns\"\"\"\n        path = params[\"path\"]\n\n        try:\n            file_path = Path(path)\n            exists = file_path.exists()\n            file_type = \"katalog\" if file_path.is_dir() else \"fil\" if file_path.is_file() else \"okänd\"\n\n            if exists:\n                return ToolResult(success=True, output=f\"Ja, {path} finns ({file_type})\")\n            else:\n                return ToolResult(success=True, output=f\"Nej, {path} finns inte\")\n\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa system-statistik", "output": "```python\n    async def _exec_system_stats(self, params: dict) -> ToolResult:\n        \"\"\"Visa system-statistik\"\"\"\n        try:\n            import psutil\n\n            # CPU\n            cpu_percent = psutil.cpu_percent(interval=0.5)\n            cpu_count = psutil.cpu_count()\n\n            # RAM\n            mem = psutil.virtual_memory()\n            ram_used = mem.used / (1024**3)\n            ram_total = mem.total / (1024**3)\n            ram_percent = mem.percent\n\n            # Disk\n            disk = psutil.disk_usage('/')\n            disk_used = disk.used / (1024**3)\n            disk_total = disk.total / (1024**3)\n            disk_percent = disk.percent\n\n            output = f\"\"\"SYSTEM STATS\n═══════════════════════════════\nCPU:  {cpu_percent:.1f}% ({cpu_count} cores)\nRAM:  {ram_used:.1f} / {ram_total:.1f} GB ({ram_percent:.1f}%)\nDisk: {disk_used:.1f} / {disk_total:.1f} GB ({disk_percent:.1f}%)\n\"\"\"\n\n            # GPU (nvidia-smi)\n            gpu_proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,temperature.gpu,utilization.gpu\",\n                \"--format=csv,noheader,nounits\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            gpu_stdout, _ = await gpu_proc.communicate()\n\n            if gpu_proc.returncode == 0:\n                gpu_data = gpu_stdout.decode().strip().split(\", \")\n                if len(gpu_data) >= 5:\n                    output += f\"\"\"\nGPU:  {gpu_data[0]}\nVRAM: {gpu_data[1]} / {gpu_data[2]} MB\nTemp: {gpu_data[3]}°C\nUtil: {gpu_data[4]}%\n\"\"\"\n\n            return ToolResult(success=True, output=output)\n\n        except ImportError:\n            return ToolResult(success=False, error=\"psutil är inte installerat\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista installerade Ollama-modeller", "output": "```python\n    async def _exec_ollama_list(self, params: dict) -> ToolResult:\n        \"\"\"Lista installerade Ollama-modeller\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"ollama\", \"list\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa körande/laddade Ollama-modeller", "output": "```python\n    async def _exec_ollama_ps(self, params: dict) -> ToolResult:\n        \"\"\"Visa körande/laddade Ollama-modeller\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"ollama\", \"ps\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        output = stdout.decode()\n        if not output.strip() or \"NAME\" not in output:\n            output = \"Inga modeller är laddade just nu\"\n\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kör ett godtyckligt shell-kommando", "output": "```python\n    async def _exec_shell(self, params: dict) -> ToolResult:\n        \"\"\"Kör ett godtyckligt shell-kommando\"\"\"\n        command = params[\"command\"]\n\n        # Säkerhetskontroll - blockera extremt farliga kommandon\n        blocked_patterns = [\n            \"rm -rf /\",\n            \"rm -rf /*\",\n            \"> /dev/sd\",\n            \"mkfs\",\n            \":(){ :|:& };:\",  # Fork bomb\n            \"dd if=\",\n        ]\n\n        for pattern in blocked_patterns:\n            if pattern in command:\n                return ToolResult(\n                    success=False,\n                    error=f\"Kommandot är blockerat av säkerhetsskäl: {pattern}\"\n                )\n\n        proc = await asyncio.create_subprocess_shell(\n            command,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            cwd=\"/home/ai-server\"\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else f\"Exit code: {proc.returncode}\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa git status", "output": "```python\n    async def _exec_git_status(self, params: dict) -> ToolResult:\n        \"\"\"Visa git status\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"status\", \"--short\", \"--branch\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa git diff", "output": "```python\n    async def _exec_git_diff(self, params: dict) -> ToolResult:\n        \"\"\"Visa git diff\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"diff\", \"--stat\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        output = stdout.decode()\n        if not output.strip():\n            output = \"Inga ändringar (clean working tree)\"\n\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa senaste commits", "output": "```python\n    async def _exec_git_log(self, params: dict) -> ToolResult:\n        \"\"\"Visa senaste commits\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"log\", \"--oneline\", \"-n\", \"10\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista git branches", "output": "```python\n    async def _exec_git_branch(self, params: dict) -> ToolResult:\n        \"\"\"Lista git branches\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"branch\", \"-a\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kör pytest i en katalog", "output": "```python\n    async def _exec_pytest_run(self, params: dict) -> ToolResult:\n        \"\"\"Kör pytest i en katalog\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"python\", \"-m\", \"pytest\", \"-v\", \"--tb=short\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else \"Tester misslyckades\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kör pytest med coverage", "output": "```python\n    async def _exec_pytest_coverage(self, params: dict) -> ToolResult:\n        \"\"\"Kör pytest med coverage\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otillåten sökväg: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"python\", \"-m\", \"pytest\", \"--cov=.\", \"--cov-report=term-missing\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else \"Coverage-körning misslyckades\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Sök på webben via DuckDuckGo", "output": "```python\n    async def _exec_web_search(self, params: dict) -> ToolResult:\n        \"\"\"Sök på webben via DuckDuckGo\"\"\"\n        query = params[\"query\"]\n        max_results = params.get(\"max_results\", 5)\n\n        try:\n            from duckduckgo_search import DDGS\n\n            results = []\n            with DDGS() as ddgs:\n                for r in ddgs.text(query, max_results=max_results):\n                    results.append(f\"**{r['title']}**\\n{r['href']}\\n{r['body']}\\n\")\n\n            if not results:\n                return ToolResult(success=True, output=f\"Inga resultat för: {query}\")\n\n            output = f\"Sökresultat för '{query}':\\n\\n\" + \"\\n---\\n\".join(results)\n            return ToolResult(success=True, output=output[:self.max_output_size])\n\n        except ImportError:\n            return ToolResult(\n                success=False,\n                error=\"duckduckgo-search är inte installerat. Kör: pip install duckduckgo-search\"\n            )\n        except Exception as e:\n            return ToolResult(success=False, error=f\"Sökfel: {str(e)}\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Säkerhetsnivå för verktyg", "output": "```python\nclass SafetyLevel(Enum):\n    \"\"\"Säkerhetsnivå för verktyg\"\"\"\n    SAFE = \"safe\"           # Körs direkt\n    DANGEROUS = \"dangerous\" # Kräver bekräftelse\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Definition av ett verktyg", "output": "```python\nclass ToolDefinition:\n    \"\"\"Definition av ett verktyg\"\"\"\n    name: str\n    description: str\n    safety: SafetyLevel\n    params: list[str] = field(default_factory=list)\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Resultat från verktygsexekvering", "output": "```python\nclass ToolResult:\n    \"\"\"Resultat från verktygsexekvering\"\"\"\n    success: bool\n    output: str = \"\"\n    error: str = \"\"\n    tool_name: str = \"\"\n    duration_ms: int = 0\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"output\": self.output,\n            \"error\": self.error,\n            \"tool_name\": self.tool_name,\n            \"duration_ms\": self.duration_ms,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Executor för QWEN SysAdmin-verktyg", "output": "```python\nclass SystemTools:\n    \"\"\"Executor för QWEN SysAdmin-verktyg\"\"\"\n\n    def __init__(self):\n        # Tillåtna sökvägar för filoperationer\n        self.allowed_paths = [\n            \"/home/ai-server\",\n            \"/var/log\",\n            \"/etc/nginx\",\n            \"/tmp\",\n            \"/opt\"\n        ]\n        # Max output storlek (bytes)\n        self.max_output_size = 10240  # 10KB\n        # Timeout för kommandon (sekunder)\n        self.timeout = 60\n\n    async def execute(self, tool_name: str, params: dict) -> ToolResult:\n        \"\"\"Kör ett verktyg och returnera resultat\"\"\"\n        import time\n        start_time = time.time()\n\n        if tool_name not in TOOLS:\n            return ToolResult(\n                success=False,\n                error=f\"Okänt verktyg: {tool_name}\",\n                tool_name=tool_name\n            )\n\n        tool = TOOLS[tool_name]\n\n        # Validera obligatoriska parametrar\n        missing = [p for p in tool.params if p not in params and p != \"lines\"]\n        if missing:\n            return ToolResult(\n                success=False,\n                error=f\"Saknar obligatoriska parametrar: {', '.join(missing)}\",\n                tool_name=tool_name\n            )\n\n        # Dispatch till rätt metod\n        method = getattr(self, f\"_exec_{tool_name}\", None)\n        if not method:\n            return ToolResult(\n                success=False,\n                error=f\"Verktyget {tool_name} är inte implementerat\",\n                tool_name=tool_name\n            )\n\n        try:\n            result = await asyncio.wait_for(\n                method(params),\n                timeout=self.timeout\n            )\n            result.tool_name = tool_name\n            result.duration_ms = int((time.time() - start_time) * 1000)\n            logger.info(f\"Tool executed: {tool_name} (success={result.success}, {result.duration_ms}ms)\")\n            return result\n\n        except asyncio.TimeoutError:\n            logger.error(f\"Tool timeout: {tool_name}\")\n            return ToolResult(\n                success=False,\n                error=f\"Timeout efter {self.timeout} sekunder\",\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n        except Exception as e:\n            logger.error(f\"Tool error: {tool_name} - {e}\")\n            return ToolResult(\n                success=False,\n                error=str(e),\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n\n    # ═══════════════════════════════════════════════════════════════════════════\n    # DOCKER VERKTYG\n    # ═══════════════════════════════════════════════════════════════════════════\n\n    async def _exec_docker_ps(self, params: dict) -> ToolResult:\n        \"\"\"Lista alla Docker containers\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"ps\", \"-a\", \"--format\",\n            \"table {{.Names}}\\t{{.Status}}\\t{{.Image}}\\t{{.Ports}}\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n\n    async def _exec_docker_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa container-loggar\"\"\"\n        container = params[\"container\"]\n        lines = params.get(\"lines\", 50)\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Result from a deploy operation", "output": "```python\nclass DeployResult:\n    \"\"\"Result from a deploy operation\"\"\"\n    success: bool\n    operation: str\n    output: str\n    error: Optional[str] = None\n    duration_ms: int = 0\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"operation\": self.operation,\n            \"output\": self.output,\n            \"error\": self.error,\n            \"duration_ms\": self.duration_ms,\n            \"timestamp\": self.timestamp,\n        }\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-klass: Progress update during deployment", "output": "```python\nclass DeployProgress:\n    \"\"\"Progress update during deployment\"\"\"\n    step: str\n    status: str  # \"pending\", \"running\", \"complete\", \"failed\"\n    message: str\n    output: Optional[str] = None\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-klass: Manages deployment operations:\n- Git pull\n- Service restart\n- Full deploy (pull + restart)", "output": "```python\nclass DeployManager:\n    \"\"\"\n    Manages deployment operations:\n    - Git pull\n    - Service restart\n    - Full deploy (pull + restart)\n    \"\"\"\n\n    def __init__(self, project_root: Path = PROJECT_ROOT):\n        self.project_root = project_root\n\n    async def git_pull(self) -> DeployResult:\n        \"\"\"\n        Pull latest changes from git.\n\n        Uses --ff-only to prevent merge conflicts.\n        \"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n        start_time = asyncio.get_event_loop().time()\n\n        logger.info(f\"Running git pull in {self.project_root}\")\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"git\", \"-C\", str(self.project_root), \"pull\", \"--ff-only\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    proc.communicate(),\n                    timeout=GIT_TIMEOUT\n                )\n            except asyncio.TimeoutError:\n                proc.kill()\n                await proc.wait()\n                return DeployResult(\n                    success=False,\n                    operation=\"git_pull\",\n                    output=\"\",\n                    error=f\"Git pull timed out after {GIT_TIMEOUT}s\",\n                    duration_ms=int((asyncio.get_event_loop().time() - start_time) * 1000),\n                    timestamp=timestamp,\n                )\n\n            duration_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)\n\n            stdout_str = stdout.decode(\"utf-8\", errors=\"replace\").strip()\n            stderr_str = stderr.decode(\"utf-8\", errors=\"replace\").strip()\n\n            if proc.returncode == 0:\n                logger.info(f\"Git pull successful: {stdout_str}\")\n                return DeployResult(\n                    success=True,\n                    operation=\"git_pull\",\n                    output=stdout_str or \"Already up to date.\",\n                    duration_ms=duration_ms,\n                    timestamp=timestamp,\n                )\n            else:\n                logger.error(f\"Git pull failed: {stderr_str}\")\n                return DeployResult(\n                    success=False,\n                    operation=\"git_pull\",\n                    output=stdout_str,\n                    error=stderr_str or \"Git pull failed\",\n                    duration_ms=duration_ms,\n                    timestamp=timestamp,\n                )\n\n        except Exception as e:\n            logger.error(f\"Git pull error: {e}\")\n            return DeployResult(\n                success=False,\n                operation=\"git_pull\",\n                output=\"\",\n                error=str(e),\n                duration_ms=int((asyncio.get_event_loop().time() - start_time) * 1000),\n                timestamp=timestamp,\n            )\n\n    async def git_status(self) -> DeployResult:\n        \"\"\"Get current git status\"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n        start_time = asyncio.get_event_loop().time()\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"git\", \"-C\", str(self.project_root), \"status\", \"--short\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            stdout, stderr = await asyncio.wait_for(\n                proc.communicate(),\n                timeout=10\n            )\n\n            duration_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-funktion: Warmup the QWEN-SONNET model using the existing ollama_client", "output": "```python\nasync def warmup_model() -> Dict[str, Any]:\n    \"\"\"\n    Warmup the QWEN-SONNET model using the existing ollama_client.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Starting model warmup for qwen-sonnet\")\n\n        # Get the QWEN-SONNET profile\n        profile = get_profile(ProfileId.QWEN_SONNET)\n\n        # Use the ollama_client warmup function\n        success = await ollama_client.warmup_model(profile)\n\n        if success:\n            logger.info(\"Model warmup completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": f\"Model {profile.model} warmed up successfully\",\n                \"model\": profile.model\n            }\n        else:\n            logger.error(\"Model warmup failed\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to warmup model {profile.model}\",\n                \"model\": profile.model\n            }\n\n    except Exception as e:\n        logger.error(f\"Error during model warmup: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error during warmup: {str(e)}\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Restart the simons-ai-backend systemd service", "output": "```python\nasync def restart_backend() -> Dict[str, Any]:\n    \"\"\"\n    Restart the simons-ai-backend systemd service.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Attempting to restart simons-ai-backend service\")\n\n        # Run systemctl restart command\n        process = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"restart\", \"simons-ai-backend\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n\n        stdout, stderr = await process.communicate()\n\n        if process.returncode == 0:\n            logger.info(\"Backend service restarted successfully\")\n            return {\n                \"success\": True,\n                \"message\": \"Backend service restarted successfully\",\n                \"service\": \"simons-ai-backend\"\n            }\n        else:\n            error_msg = stderr.decode().strip() if stderr else \"Unknown error\"\n            logger.error(f\"Failed to restart backend service: {error_msg}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to restart backend: {error_msg}\",\n                \"service\": \"simons-ai-backend\",\n                \"error\": error_msg\n            }\n\n    except Exception as e:\n        logger.error(f\"Error restarting backend service: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error restarting backend: {str(e)}\",\n            \"service\": \"simons-ai-backend\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Cast dashboard to Nest Hub \"Sovis\" using catt", "output": "```python\nasync def recast_dashboard() -> Dict[str, Any]:\n    \"\"\"\n    Cast dashboard to Nest Hub \"Sovis\" using catt.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Casting dashboard to Nest Hub Sovis\")\n\n        process = await asyncio.create_subprocess_exec(\n            \"/home/ai-server/catt-venv/bin/catt\",\n            \"-d\", \"Sovis\",\n            \"cast_site\", \"http://192.168.86.32:5173/kiosk\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n\n        stdout, stderr = await process.communicate()\n\n        if process.returncode == 0:\n            logger.info(\"Dashboard cast to Sovis successfully\")\n            return {\n                \"success\": True,\n                \"message\": \"Dashboard cast to Sovis\",\n                \"device\": \"Sovis\"\n            }\n        else:\n            error_msg = stderr.decode().strip() if stderr else \"Unknown error\"\n            logger.error(f\"Failed to cast dashboard: {error_msg}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to cast: {error_msg}\",\n                \"device\": \"Sovis\",\n                \"error\": error_msg\n            }\n\n    except Exception as e:\n        logger.error(f\"Error casting dashboard: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error casting: {str(e)}\",\n            \"device\": \"Sovis\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Unload all running models from Ollama by setting keep_alive=0", "output": "```python\nasync def unload_all_models() -> Dict[str, Any]:\n    \"\"\"\n    Unload all running models from Ollama by setting keep_alive=0.\n\n    Returns:\n        Dict with success status and list of unloaded models\n    \"\"\"\n    try:\n        logger.info(\"Attempting to unload all models\")\n\n        # First, get list of running models\n        running_models = await ollama_client.list_running_models()\n\n        if not running_models:\n            logger.info(\"No models currently running\")\n            return {\n                \"success\": True,\n                \"message\": \"No models were running\",\n                \"unloaded_models\": []\n            }\n\n        model_names = [model.get(\"name\", \"unknown\") for model in running_models]\n        logger.info(f\"Found {len(model_names)} running models: {model_names}\")\n\n        # Use httpx to send unload request to each model\n        async with httpx.AsyncClient(timeout=30.0) as client:\n            for model_info in running_models:\n                model_name = model_info.get(\"name\", \"\")\n                if model_name:\n                    try:\n                        logger.info(f\"Unloading model: {model_name}\")\n                        response = await client.post(\n                            OLLAMA_GENERATE_ENDPOINT,\n                            json={\n                                \"model\": model_name,\n                                \"prompt\": \"\",\n                                \"keep_alive\": 0\n                            }\n                        )\n\n                        if response.status_code == 200:\n                            logger.info(f\"Successfully unloaded: {model_name}\")\n                        else:\n                            logger.warning(f\"Unexpected response unloading {model_name}: {response.status_code}\")\n\n                    except Exception as model_error:\n                        logger.error(f\"Error unloading {model_name}: {model_error}\")\n\n        logger.info(f\"Unloaded {len(model_names)} models\")\n        return {\n            \"success\": True,\n            \"message\": f\"Unloaded {len(model_names)} model(s)\",\n            \"unloaded_models\": model_names\n        }\n\n    except Exception as e:\n        logger.error(f\"Error unloading models: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error unloading models: {str(e)}\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Get today's usage statistics", "output": "```python\nasync def get_today_stats() -> Dict[str, Any]:\n    \"\"\"\n    Get today's usage statistics.\n    Currently returns mock data - will be wired up to real analytics later.\n\n    Returns:\n        Dict with today's stats:\n        - sessions_count: Number of chat sessions today\n        - total_tokens: Total tokens processed today\n        - avg_ttft_ms: Average time to first token in milliseconds\n        - agents_used: List of agent profiles used today\n    \"\"\"\n    logger.info(\"Fetching today's stats (mock data)\")\n\n    # TODO: Wire up to real analytics database\n    # For now, return mock data\n    mock_stats = {\n        \"sessions_count\": 42,\n        \"total_tokens\": 15847,\n        \"avg_ttft_ms\": 245,\n        \"agents_used\": [\"QWEN\"],\n        \"is_mock\": True  # Flag to indicate this is mock data\n    }\n\n    logger.info(f\"Returning mock stats: {mock_stats}\")\n    return mock_stats\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Kör full cascade pipeline: Planner → Coder → Reviewer\n\nArgs:\n    task: Uppgiftsbeskrivning från användaren\n    request_id: Unik request ID\n    skip_review: Hoppa över review-fasen (snabbare)\n\nYields:\n    CascadeResult för varje fas med streaming content", "output": "```python\n    async def execute_cascade(\n        self,\n        task: str,\n        request_id: str,\n        skip_review: bool = False,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        Kör full cascade pipeline: Planner → Coder → Reviewer\n\n        Args:\n            task: Uppgiftsbeskrivning från användaren\n            request_id: Unik request ID\n            skip_review: Hoppa över review-fasen (snabbare)\n\n        Yields:\n            CascadeResult för varje fas med streaming content\n        \"\"\"\n        logger.info(f\"[{request_id}] Starting cascade for task: {task[:100]}...\")\n\n        # === FAS 1: PLANNING ===\n        logger.info(f\"[{request_id}] Phase 1: PLANNING\")\n        plan_content = \"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.PLANNING,\n            input_content=task,\n            request_id=f\"{request_id}-plan\"\n        ):\n            if result.content:\n                plan_content += result.content\n            yield result\n\n        if not plan_content:\n            logger.error(f\"[{request_id}] Planning phase produced no output\")\n            return\n\n        # === FAS 2: CODING ===\n        logger.info(f\"[{request_id}] Phase 2: CODING\")\n        code_content = \"\"\n\n        # Bygg context för coder\n        coder_input = f\"\"\"UPPGIFT:\n{task}\n\nPLAN FRÅN PLANNER:\n{plan_content}\n\nImplementera nu koden enligt planen ovan.\"\"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.CODING,\n            input_content=coder_input,\n            request_id=f\"{request_id}-code\"\n        ):\n            if result.content:\n                code_content += result.content\n            yield result\n\n        if not code_content:\n            logger.error(f\"[{request_id}] Coding phase produced no output\")\n            return\n\n        # === FAS 3: REVIEWING (optional) ===\n        if not skip_review:\n            logger.info(f\"[{request_id}] Phase 3: REVIEWING\")\n\n            # Bygg context för reviewer\n            reviewer_input = f\"\"\"URSPRUNGLIG UPPGIFT:\n{task}\n\nPLAN:\n{plan_content}\n\nKOD ATT GRANSKA:\n{code_content}\n\nGranska koden ovan och ge feedback.\"\"\"\n\n            async for result in self._run_phase(\n                phase=CascadePhase.REVIEWING,\n                input_content=reviewer_input,\n                request_id=f\"{request_id}-review\"\n            ):\n                yield result\n\n        # === COMPLETE ===\n        yield CascadeResult(\n            phase=CascadePhase.COMPLETE,\n            agent_id=\"cascade\",\n            content=\"\",\n            stats=None\n        )\n        logger.info(f\"[{request_id}] Cascade complete\")\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Kör en enskild fas i cascade-pipelinen", "output": "```python\n    async def _run_phase(\n        self,\n        phase: CascadePhase,\n        input_content: str,\n        request_id: str,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        Kör en enskild fas i cascade-pipelinen.\n\n        Hanterar:\n        - Hot-swap av modell (unload andra)\n        - Streaming av output\n        - Stats collection\n        \"\"\"\n        config = self.AGENT_ROLES[phase]\n        profile = config[\"profile\"]\n\n        # Hot-swap: Unload andra modeller för att frigöra VRAM\n        logger.info(f\"[{request_id}] Hot-swap: Loading {profile}\")\n        try:\n            unloaded = await self._ollama.unload_other_models(profile)\n            if unloaded:\n                logger.info(f\"[{request_id}] Unloaded models: {unloaded}\")\n        except Exception as e:\n            logger.warning(f\"[{request_id}] Hot-swap warning: {e}\")\n\n        # Bygg messages med custom system prompt\n        messages = [\n            {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": input_content}\n        ]\n\n        # Streama från orchestrator\n        try:\n            async for token, stats, agent_id in self._orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n                temperature=config[\"temperature\"]\n            ):\n                if stats:\n                    # Final stats\n                    yield CascadeResult(\n                        phase=phase,\n                        agent_id=agent_id,\n                        content=\"\",\n                        stats=stats\n                    )\n                else:\n                    # Streaming token\n                    yield CascadeResult(\n                        phase=phase,\n                        agent_id=agent_id,\n                        content=token,\n                        stats=None\n                    )\n        except Exception as e:\n            logger.error(f\"[{request_id}] Phase {phase.value} error: {e}\")\n            yield CascadeResult(\n                phase=phase,\n                agent_id=profile,\n                content=f\"\\n[ERROR] {phase.value} failed: {str(e)}\",\n                stats=None\n            )\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Kör cascade och returnera sammanslaget resultat (ej streaming)", "output": "```python\n    async def quick_cascade(\n        self,\n        task: str,\n        request_id: str,\n    ) -> Dict[str, str]:\n        \"\"\"\n        Kör cascade och returnera sammanslaget resultat (ej streaming).\n\n        Returns:\n            {\"plan\": \"...\", \"code\": \"...\", \"review\": \"...\"}\n        \"\"\"\n        results = {\"plan\": \"\", \"code\": \"\", \"review\": \"\"}\n\n        async for result in self.execute_cascade(task, request_id):\n            if result.phase == CascadePhase.PLANNING:\n                results[\"plan\"] += result.content\n            elif result.phase == CascadePhase.CODING:\n                results[\"code\"] += result.content\n            elif result.phase == CascadePhase.REVIEWING:\n                results[\"review\"] += result.content\n\n        return results\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Faser i cascade-pipelinen", "output": "```python\nclass CascadePhase(str, Enum):\n    \"\"\"Faser i cascade-pipelinen\"\"\"\n    PLANNING = \"planning\"\n    CODING = \"coding\"\n    REVIEWING = \"reviewing\"\n    COMPLETE = \"complete\"\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Resultat från en cascade-fas", "output": "```python\nclass CascadeResult:\n    \"\"\"Resultat från en cascade-fas\"\"\"\n    phase: CascadePhase\n    agent_id: str\n    content: str\n    stats: Optional[UnifiedStats] = None\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Orkestrerar multi-agent cascade: Planner → Coder → Reviewer\n\nHanterar:\n- Sekventiell körning av agenter\n- Hot-swap av modeller (VRAM-effektivt)\n- Streaming av output från varje fas\n- Sammanslagning av resultat", "output": "```python\nclass CascadeOrchestrator:\n    \"\"\"\n    Orkestrerar multi-agent cascade: Planner → Coder → Reviewer\n\n    Hanterar:\n    - Sekventiell körning av agenter\n    - Hot-swap av modeller (VRAM-effektivt)\n    - Streaming av output från varje fas\n    - Sammanslagning av resultat\n    \"\"\"\n\n    # Agent-konfiguration för varje roll\n    AGENT_ROLES = {\n        CascadePhase.PLANNING: {\n            \"profile\": \"devstral-openhands\",\n            \"system_prompt\": PLANNER_SYSTEM_PROMPT,\n            \"temperature\": 0.3,  # Mer fokuserad för planering\n        },\n        CascadePhase.CODING: {\n            \"profile\": \"qwen-sonnet\",\n            \"system_prompt\": CODER_SYSTEM_PROMPT,\n            \"temperature\": 0.15,  # Mycket fokuserad för kod\n        },\n        CascadePhase.REVIEWING: {\n            \"profile\": \"qwen-openhands\",\n            \"system_prompt\": REVIEWER_SYSTEM_PROMPT,\n            \"temperature\": 0.2,  # Balanserad för granskning\n        },\n    }\n\n    def __init__(self):\n        self._ollama = ollama_client\n        self._orchestrator = orchestrator\n\n    async def execute_cascade(\n        self,\n        task: str,\n        request_id: str,\n        skip_review: bool = False,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        Kör full cascade pipeline: Planner → Coder → Reviewer\n\n        Args:\n            task: Uppgiftsbeskrivning från användaren\n            request_id: Unik request ID\n            skip_review: Hoppa över review-fasen (snabbare)\n\n        Yields:\n            CascadeResult för varje fas med streaming content\n        \"\"\"\n        logger.info(f\"[{request_id}] Starting cascade for task: {task[:100]}...\")\n\n        # === FAS 1: PLANNING ===\n        logger.info(f\"[{request_id}] Phase 1: PLANNING\")\n        plan_content = \"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.PLANNING,\n            input_content=task,\n            request_id=f\"{request_id}-plan\"\n        ):\n            if result.content:\n                plan_content += result.content\n            yield result\n\n        if not plan_content:\n            logger.error(f\"[{request_id}] Planning phase produced no output\")\n            return\n\n        # === FAS 2: CODING ===\n        logger.info(f\"[{request_id}] Phase 2: CODING\")\n        code_content = \"\"\n\n        # Bygg context för coder\n        coder_input = f\"\"\"UPPGIFT:\n{task}\n\nPLAN FRÅN PLANNER:\n{plan_content}\n\nImplementera nu koden enligt planen ovan.\"\"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.CODING,\n            input_content=coder_input,\n            request_id=f\"{request_id}-code\"\n        ):\n            if result.content:\n                code_content += result.content\n            yield result\n\n        if not code_content:\n            logger.error(f\"[{request_id}] Coding phase produced no output\")\n            return\n\n        # === FAS 3: REVIEWING (optional) ===\n        if not skip_review:\n            logger.info(f\"[{request_id}] Phase 3: REVIEWING\")\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Generate a filtered file tree for model context injection", "output": "```python\ndef get_project_structure(\n    root_path: str = \"/home/ai-server/local-llm-backend\",\n    max_depth: int = 4,\n    include_sizes: bool = False\n) -> ProjectContext:\n    \"\"\"\n    Generate a filtered file tree for model context injection.\n\n    Args:\n        root_path: Root directory to scan\n        max_depth: Maximum directory depth\n        include_sizes: Include file sizes in output\n\n    Returns:\n        ProjectContext with file tree and metadata\n    \"\"\"\n    root = Path(root_path)\n    if not root.exists():\n        logger.warning(f\"Project root not found: {root_path}\")\n        return ProjectContext(\n            root_path=root_path,\n            file_tree=\"[Project not found]\",\n            file_count=0,\n            directory_count=0\n        )\n\n    lines = []\n    file_count = 0\n    dir_count = 0\n    languages = set()\n    key_files = []\n\n    # Language detection by extension\n    lang_map = {\n        '.py': 'Python',\n        '.js': 'JavaScript',\n        '.ts': 'TypeScript',\n        '.tsx': 'TypeScript/React',\n        '.jsx': 'JavaScript/React',\n        '.html': 'HTML',\n        '.css': 'CSS',\n        '.scss': 'SCSS',\n        '.json': 'JSON',\n        '.yaml': 'YAML',\n        '.yml': 'YAML',\n        '.md': 'Markdown',\n        '.sh': 'Shell',\n        '.sql': 'SQL',\n        '.rs': 'Rust',\n        '.go': 'Go',\n    }\n\n    # Key file patterns\n    key_patterns = [\n        'main.py', 'app.py', 'index.ts', 'index.js',\n        'package.json', 'requirements.txt', 'pyproject.toml',\n        'Dockerfile', 'docker-compose.yml', '.env.example',\n        'README.md', 'Makefile', 'setup.py'\n    ]\n\n    def should_ignore(name: str, is_dir: bool) -> bool:\n        \"\"\"Check if path should be ignored\"\"\"\n        if is_dir:\n            return name in IGNORE_DIRS or name.startswith('.')\n        return any(\n            name == pat or (pat.startswith('*') and name.endswith(pat[1:]))\n            for pat in IGNORE_FILES\n        )\n\n    def build_tree(path: Path, prefix: str = \"\", depth: int = 0):\n        nonlocal file_count, dir_count\n\n        if depth > max_depth:\n            return\n\n        try:\n            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n        except PermissionError:\n            return\n\n        # Filter entries\n        entries = [e for e in entries if not should_ignore(e.name, e.is_dir())]\n\n        for i, entry in enumerate(entries):\n            is_last = i == len(entries) - 1\n            connector = \"└── \" if is_last else \"├── \"\n            new_prefix = prefix + (\"    \" if is_last else \"│   \")\n\n            if entry.is_dir():\n                dir_count += 1\n                lines.append(f\"{prefix}{connector}{entry.name}/\")\n                build_tree(entry, new_prefix, depth + 1)\n            else:\n                file_count += 1\n                suffix = entry.suffix.lower()\n\n                # Detect language\n                if suffix in lang_map:\n                    languages.add(lang_map[suffix])\n\n                # Check for key files\n                if entry.name in key_patterns:\n                    key_files.append(str(entry.relative_to(root)))\n\n                # Format line\n                size_str = \"\"\n                if include_sizes:\n                    try:\n                        size = entry.stat().st_size\n                        if size > 1024 * 1024:\n                            size_str = f\" ({size / 1024 / 1024:.1f}MB)\"\n                        elif size > 1024:\n                            size_str = f\" ({size / 1024:.1f}KB)\"\n                    except OSError:\n                        pass\n\n                lines.append(f\"{prefix}{connector}{entry.name}{size_str}\")\n\n    # Build the tree\n    lines.append(f\"{root.name}/\")\n    build_tree(root)\n\n    return ProjectContext(\n        root_path=root_path,\n        file_tree=\"\\n\".join(lines),\n        file_count=file_count,\n        directory_count=dir_count,\n        languages_detected=sorted(languages),\n        key_files=key_files\n    )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build a context-aware system prompt addition", "output": "```python\ndef build_context_prompt(project: ProjectContext) -> str:\n    \"\"\"\n    Build a context-aware system prompt addition.\n\n    Args:\n        project: ProjectContext from get_project_structure()\n\n    Returns:\n        String to append to system prompt\n    \"\"\"\n    return f\"\"\"\n## PROJECT CONTEXT\n\nDu arbetar med följande projektstruktur:\n\n```\n{project.file_tree}\n```\n\n**Projektöversikt:**\n- Filer: {project.file_count}\n- Mappar: {project.directory_count}\n- Språk: {', '.join(project.languages_detected) if project.languages_detected else 'Okänt'}\n- Viktiga filer: {', '.join(project.key_files) if project.key_files else 'Inga hittade'}\n\nNär du skriver kod, använd denna struktur för att:\n1. Referera till rätt filsökvägar\n2. Följa projektets konventioner\n3. Förstå var ny kod ska placeras\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion:     Extract code blocks from markdown-formatted text", "output": "```python\ndef extract_code_blocks(text: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    Extract code blocks from markdown-formatted text.\n\n    Args:\n        text: Text containing ```language\\ncode``` blocks\n\n    Returns:\n        List of (language, code) tuples\n    \"\"\"\n    blocks = []\n    for match in CODE_BLOCK_PATTERN.finditer(text):\n        language = match.group(1) or \"unknown\"\n        code = match.group(2).strip()\n        blocks.append((language.lower(), code))\n    return blocks\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Validate Python code syntax using AST parser", "output": "```python\ndef validate_python_syntax(code: str) -> SyntaxCheckResult:\n    \"\"\"\n    Validate Python code syntax using AST parser.\n\n    Args:\n        code: Python code string\n\n    Returns:\n        SyntaxCheckResult with validation details\n    \"\"\"\n    try:\n        ast.parse(code)\n        return SyntaxCheckResult(\n            is_valid=True,\n            language=\"python\",\n            code=code\n        )\n    except SyntaxError as e:\n        return SyntaxCheckResult(\n            is_valid=False,\n            language=\"python\",\n            code=code,\n            error_message=str(e.msg) if e.msg else str(e),\n            error_line=e.lineno\n        )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build a prompt for self-correction when syntax error is detected", "output": "```python\ndef build_correction_prompt(result: SyntaxCheckResult) -> str:\n    \"\"\"\n    Build a prompt for self-correction when syntax error is detected.\n\n    Args:\n        result: Failed SyntaxCheckResult\n\n    Returns:\n        Prompt for model to correct the code\n    \"\"\"\n    return f\"\"\"\n## SYNTAX ERROR DETECTED\n\nDin kod innehåller ett syntaxfel som måste korrigeras:\n\n**Fel:** {result.error_message}\n**Rad:** {result.error_line}\n\n**Felaktig kod:**\n```python\n{result.code}\n```\n\nKorrigera koden och returnera ENDAST den korrigerade versionen utan förklaringar.\nSvara med:\n```python\n[korrigerad kod här]\n```\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Validate all code blocks in a response", "output": "```python\ndef validate_code_in_response(text: str) -> Tuple[bool, List[SyntaxCheckResult], str]:\n    \"\"\"\n    Validate all code blocks in a response.\n\n    Args:\n        text: Full response text with potential code blocks\n\n    Returns:\n        Tuple of (all_valid, results, corrected_text)\n    \"\"\"\n    blocks = extract_code_blocks(text)\n    results = []\n    all_valid = True\n    corrections_needed = []\n\n    for language, code in blocks:\n        if language == \"python\":\n            result = validate_python_syntax(code)\n            results.append(result)\n            if not result.is_valid:\n                all_valid = False\n                corrections_needed.append((code, result))\n\n    # For now, return original text - correction happens in orchestrator\n    return all_valid, results, text\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build prompt addition to enforce JSON output", "output": "```python\ndef build_json_enforcement_prompt(schema_hint: Optional[Dict] = None) -> str:\n    \"\"\"\n    Build prompt addition to enforce JSON output.\n\n    Args:\n        schema_hint: Optional JSON schema for structure guidance\n\n    Returns:\n        Prompt addition for JSON enforcement\n    \"\"\"\n    schema_str = \"\"\n    if schema_hint:\n        schema_str = f\"\"\"\nAnvänd följande struktur:\n```json\n{json.dumps(schema_hint, indent=2, ensure_ascii=False)}\n```\n\"\"\"\n\n    return f\"\"\"\n## OUTPUT FORMAT: JSON\n\nVIKTIGT: Svara ENDAST med valid JSON. Ingen annan text.\n{schema_str}\nRegler:\n- Börja med {{ eller [\n- Sluta med }} eller ]\n- Använd dubbla citattecken för strängar\n- Inga trailing commas\n- Inga kommentarer\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Simple validation of JSON against required fields", "output": "```python\ndef validate_json_against_schema(\n    data: Any,\n    required_fields: List[str]\n) -> Tuple[bool, List[str]]:\n    \"\"\"\n    Simple validation of JSON against required fields.\n\n    Args:\n        data: Parsed JSON data\n        required_fields: List of required field names\n\n    Returns:\n        Tuple of (is_valid, missing_fields)\n    \"\"\"\n    if not isinstance(data, dict):\n        return False, required_fields\n\n    missing = [f for f in required_fields if f not in data]\n    return len(missing) == 0, missing\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Check if path should be ignored", "output": "```python\n    def should_ignore(name: str, is_dir: bool) -> bool:\n        \"\"\"Check if path should be ignored\"\"\"\n        if is_dir:\n            return name in IGNORE_DIRS or name.startswith('.')\n        return any(\n            name == pat or (pat.startswith('*') and name.endswith(pat[1:]))\n            for pat in IGNORE_FILES\n        )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Skapa en Python-funktion som build tree", "output": "```python\n    def build_tree(path: Path, prefix: str = \"\", depth: int = 0):\n        nonlocal file_count, dir_count\n\n        if depth > max_depth:\n            return\n\n        try:\n            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n        except PermissionError:\n            return\n\n        # Filter entries\n        entries = [e for e in entries if not should_ignore(e.name, e.is_dir())]\n\n        for i, entry in enumerate(entries):\n            is_last = i == len(entries) - 1\n            connector = \"└── \" if is_last else \"├── \"\n            new_prefix = prefix + (\"    \" if is_last else \"│   \")\n\n            if entry.is_dir():\n                dir_count += 1\n                lines.append(f\"{prefix}{connector}{entry.name}/\")\n                build_tree(entry, new_prefix, depth + 1)\n            else:\n                file_count += 1\n                suffix = entry.suffix.lower()\n\n                # Detect language\n                if suffix in lang_map:\n                    languages.add(lang_map[suffix])\n\n                # Check for key files\n                if entry.name in key_patterns:\n                    key_files.append(str(entry.relative_to(root)))\n\n                # Format line\n                size_str = \"\"\n                if include_sizes:\n                    try:\n                        size = entry.stat().st_size\n                        if size > 1024 * 1024:\n                            size_str = f\" ({size / 1024 / 1024:.1f}MB)\"\n                        elif size > 1024:\n                            size_str = f\" ({size / 1024:.1f}KB)\"\n                    except OSError:\n                        pass\n\n                lines.append(f\"{prefix}{connector}{entry.name}{size_str}\")\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Get cached or fresh project context", "output": "```python\n    def get_project_context(self, force_refresh: bool = False) -> ProjectContext:\n        \"\"\"Get cached or fresh project context\"\"\"\n        if self._project_context is None or force_refresh:\n            self._project_context = get_project_structure(self.project_root)\n            self._context_prompt = build_context_prompt(self._project_context)\n            logger.info(\n                f\"Project context loaded: {self._project_context.file_count} files, \"\n                f\"{self._project_context.directory_count} dirs\"\n            )\n        return self._project_context\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build enhanced system prompt with intelligence features", "output": "```python\n    def get_enhanced_system_prompt(\n        self,\n        base_prompt: str,\n        include_project: bool = True,\n        output_format: OutputFormat = OutputFormat.TEXT,\n        json_schema: Optional[Dict] = None\n    ) -> str:\n        \"\"\"\n        Build enhanced system prompt with intelligence features.\n\n        Args:\n            base_prompt: Original system prompt\n            include_project: Include project structure context\n            output_format: Desired output format\n            json_schema: Schema hint for JSON output\n\n        Returns:\n            Enhanced system prompt\n        \"\"\"\n        parts = [base_prompt]\n\n        if include_project:\n            self.get_project_context()\n            if self._context_prompt:\n                parts.append(self._context_prompt)\n\n        if output_format == OutputFormat.JSON:\n            parts.append(build_json_enforcement_prompt(json_schema))\n\n        return \"\\n\\n\".join(parts)\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Process model response with intelligence checks", "output": "```python\n    def process_response(\n        self,\n        response: str,\n        validate_python: bool = True,\n        expect_json: bool = False,\n        required_json_fields: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process model response with intelligence checks.\n\n        Args:\n            response: Raw model response\n            validate_python: Check Python syntax in code blocks\n            expect_json: Parse and validate JSON output\n            required_json_fields: Required fields for JSON validation\n\n        Returns:\n            Dict with processed response and metadata\n        \"\"\"\n        result = {\n            \"original\": response,\n            \"processed\": response,\n            \"syntax_valid\": True,\n            \"syntax_errors\": [],\n            \"json_valid\": None,\n            \"json_data\": None,\n            \"needs_correction\": False,\n            \"correction_prompt\": None\n        }\n\n        # Python syntax validation\n        if validate_python:\n            all_valid, checks, _ = validate_code_in_response(response)\n            result[\"syntax_valid\"] = all_valid\n\n            for check in checks:\n                if not check.is_valid:\n                    result[\"syntax_errors\"].append({\n                        \"error\": check.error_message,\n                        \"line\": check.error_line,\n                        \"code_snippet\": check.code[:100] + \"...\" if len(check.code) > 100 else check.code\n                    })\n                    result[\"needs_correction\"] = True\n                    result[\"correction_prompt\"] = build_correction_prompt(check)\n\n        # JSON validation\n        if expect_json:\n            success, data, raw = extract_json_from_response(response)\n            result[\"json_valid\"] = success\n\n            if success:\n                result[\"json_data\"] = data\n                result[\"processed\"] = raw\n\n                if required_json_fields:\n                    fields_valid, missing = validate_json_against_schema(data, required_json_fields)\n                    if not fields_valid:\n                        result[\"json_valid\"] = False\n                        result[\"json_data\"] = {\"error\": f\"Missing fields: {missing}\"}\n            else:\n                result[\"json_data\"] = {\"error\": data}\n\n        return result\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Supported output formats", "output": "```python\nclass OutputFormat(str, Enum):\n    \"\"\"Supported output formats\"\"\"\n    TEXT = \"text\"\n    JSON = \"json\"\n    CODE = \"code\"\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Result of syntax validation", "output": "```python\nclass SyntaxCheckResult:\n    \"\"\"Result of syntax validation\"\"\"\n    is_valid: bool\n    language: str\n    code: str\n    error_message: Optional[str] = None\n    error_line: Optional[int] = None\n    corrected_code: Optional[str] = None\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Project structure context for model injection", "output": "```python\nclass ProjectContext:\n    \"\"\"Project structure context for model injection\"\"\"\n    root_path: str\n    file_tree: str\n    file_count: int\n    directory_count: int\n    languages_detected: List[str] = field(default_factory=list)\n    key_files: List[str] = field(default_factory=list)\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Central manager for all intelligence features", "output": "```python\nclass IntelligenceManager:\n    \"\"\"\n    Central manager for all intelligence features.\n    Integrates with the orchestrator for enhanced responses.\n    \"\"\"\n\n    def __init__(self, project_root: str = \"/home/ai-server/local-llm-backend\"):\n        self.project_root = project_root\n        self._project_context: Optional[ProjectContext] = None\n        self._context_prompt: Optional[str] = None\n\n    def get_project_context(self, force_refresh: bool = False) -> ProjectContext:\n        \"\"\"Get cached or fresh project context\"\"\"\n        if self._project_context is None or force_refresh:\n            self._project_context = get_project_structure(self.project_root)\n            self._context_prompt = build_context_prompt(self._project_context)\n            logger.info(\n                f\"Project context loaded: {self._project_context.file_count} files, \"\n                f\"{self._project_context.directory_count} dirs\"\n            )\n        return self._project_context\n\n    def get_enhanced_system_prompt(\n        self,\n        base_prompt: str,\n        include_project: bool = True,\n        output_format: OutputFormat = OutputFormat.TEXT,\n        json_schema: Optional[Dict] = None\n    ) -> str:\n        \"\"\"\n        Build enhanced system prompt with intelligence features.\n\n        Args:\n            base_prompt: Original system prompt\n            include_project: Include project structure context\n            output_format: Desired output format\n            json_schema: Schema hint for JSON output\n\n        Returns:\n            Enhanced system prompt\n        \"\"\"\n        parts = [base_prompt]\n\n        if include_project:\n            self.get_project_context()\n            if self._context_prompt:\n                parts.append(self._context_prompt)\n\n        if output_format == OutputFormat.JSON:\n            parts.append(build_json_enforcement_prompt(json_schema))\n\n        return \"\\n\\n\".join(parts)\n\n    def process_response(\n        self,\n        response: str,\n        validate_python: bool = True,\n        expect_json: bool = False,\n        required_json_fields: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process model response with intelligence checks.\n\n        Args:\n            response: Raw model response\n            validate_python: Check Python syntax in code blocks\n            expect_json: Parse and validate JSON output\n            required_json_fields: Required fields for JSON validation\n\n        Returns:\n            Dict with processed response and metadata\n        \"\"\"\n        result = {\n            \"original\": response,\n            \"processed\": response,\n            \"syntax_valid\": True,\n            \"syntax_errors\": [],\n            \"json_valid\": None,\n            \"json_data\": None,\n            \"needs_correction\": False,\n            \"correction_prompt\": None\n        }\n\n        # Python syntax validation\n        if validate_python:\n            all_valid, checks, _ = validate_code_in_response(response)\n            result[\"syntax_valid\"] = all_valid\n\n            for check in checks:\n                if not check.is_valid:\n                    result[\"syntax_errors\"].append({\n                        \"error\": check.error_message,\n                        \"line\": check.error_line,\n                        \"code_snippet\": check.code[:100] + \"...\" if len(check.code) > 100 else check.code\n                    })\n                    result[\"needs_correction\"] = True\n                    result[\"correction_prompt\"] = build_correction_prompt(check)\n\n        # JSON validation\n        if expect_json:\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Run async coroutine in sync context", "output": "```python\ndef _run_async(coro):\n    \"\"\"Run async coroutine in sync context\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            # If we're in an async context, create a new thread\n            import concurrent.futures\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                future = executor.submit(asyncio.run, coro)\n                return future.result(timeout=60)\n        else:\n            return loop.run_until_complete(coro)\n    except RuntimeError:\n        # No event loop, create one\n        return asyncio.run(coro)\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file read operation", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file read operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileReadTool: Reading {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_read\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileReadTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute directory listing", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute directory listing\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileListTool: Listing {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_list\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileListTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file exists check", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file exists check\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileExistsTool: Checking {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_exists\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileExistsTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file write operation", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file write operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                return \"Error: Kunde inte tolka parametrar som JSON\"\n\n        path = params.get(\"path\", \"\")\n        content = params.get(\"content\", \"\")\n\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n        if not content:\n            return \"Error: Inget innehåll angivet\"\n\n        logger.info(f\"FileWriteTool: Writing to {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_write\", {\"path\": path, \"content\": content}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileWriteTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for reading file contents on the server", "output": "```python\nclass FileReadTool(BaseTool):\n    \"\"\"Tool for reading file contents on the server\"\"\"\n\n    description = \"Läs innehållet i en fil på servern (max 10KB). Användbart för att granska kod, konfigurationsfiler, loggar, etc.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut sökväg till filen som ska läsas, t.ex. /home/ai-server/app/main.py\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file read operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileReadTool: Reading {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_read\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileReadTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for listing directory contents", "output": "```python\nclass FileListTool(BaseTool):\n    \"\"\"Tool for listing directory contents\"\"\"\n\n    description = \"Lista filer och mappar i en katalog på servern. Visar filtyp, storlek och namn.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut sökväg till katalogen som ska listas, t.ex. /home/ai-server/\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute directory listing\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileListTool: Listing {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_list\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileListTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for checking if a file or directory exists", "output": "```python\nclass FileExistsTool(BaseTool):\n    \"\"\"Tool for checking if a file or directory exists\"\"\"\n\n    description = \"Kontrollera om en fil eller katalog finns på servern.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut sökväg att kontrollera\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file exists check\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n\n        logger.info(f\"FileExistsTool: Checking {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_exists\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileExistsTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for writing files to the server", "output": "```python\nclass FileWriteTool(BaseTool):\n    \"\"\"Tool for writing files to the server\"\"\"\n\n    description = \"Skriv innehåll till en fil på servern. Kan skapa nya filer eller skriva över befintliga. Tillåtna sökvägar: /home/ai-server, /tmp, /var/log, /etc/nginx, /opt\"\n    parameters = [\n        {\n            \"name\": \"path\",\n            \"type\": \"string\",\n            \"description\": \"Absolut sökväg till filen som ska skrivas, t.ex. /tmp/test.py\",\n            \"required\": True\n        },\n        {\n            \"name\": \"content\",\n            \"type\": \"string\",\n            \"description\": \"Innehållet som ska skrivas till filen\",\n            \"required\": True\n        }\n    ]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file write operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                return \"Error: Kunde inte tolka parametrar som JSON\"\n\n        path = params.get(\"path\", \"\")\n        content = params.get(\"content\", \"\")\n\n        if not path:\n            return \"Error: Ingen sökväg angiven\"\n        if not content:\n            return \"Error: Inget innehåll angivet\"\n\n        logger.info(f\"FileWriteTool: Writing to {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_write\", {\"path\": path, \"content\": content}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileWriteTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Skapa en Python-funktion som from xai", "output": "```python\n    def from_xai(cls, stats: XAIStats, agent_id: str = \"cloud\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_tokens,\n            provider=\"xai\",\n            model=\"grok-2\",\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Skapa en Python-funktion som from claude", "output": "```python\n    def from_claude(cls, stats: ClaudeStats, agent_id: str = \"claude\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_tokens,\n            provider=\"claude_ui\",\n            model=\"sonnet\",\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Get a fallback agent when primary fails", "output": "```python\n    def _get_fallback_agent(self, failed_agent: AgentConfig) -> Optional[AgentConfig]:\n        \"\"\"Get a fallback agent when primary fails\"\"\"\n        if failed_agent.provider == Provider.XAI:\n            # Cloud failed, fallback to local QWEN-SONNET\n            return AGENTS[AgentId.QWEN_SONNET]\n        elif failed_agent.provider == Provider.OLLAMA:\n            # Local failed, try cloud if available\n            if self._xai.is_configured:\n                return AGENTS[AgentId.CLOUD]\n        return None\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Stream from fallback agent", "output": "```python\n    async def _fallback_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        agent: AgentConfig,\n        temperature: float,\n        max_tokens: int\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str], None]:\n        \"\"\"Stream from fallback agent\"\"\"\n        try:\n            if agent.provider == Provider.XAI:\n                async for token, stats in self._xai.chat_stream(\n                    messages=messages,\n                    request_id=request_id,\n                    system_prompt=agent.system_prompt,\n                    temperature=temperature,\n                    max_tokens=max_tokens\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_xai(stats, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n            else:\n                msgs_with_system = [{\"role\": \"system\", \"content\": agent.system_prompt}]\n                msgs_with_system.extend([\n                    {\"role\": m.get(\"role\"), \"content\": m.get(\"content\")}\n                    for m in messages if m.get(\"role\") != \"system\"\n                ])\n\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 8192\n\n                profile_obj = SimpleProfile(agent.id.value, agent.model, temperature, max_tokens)\n\n                async for token, stats in self._ollama.chat_stream(\n                    profile=profile_obj,\n                    messages=msgs_with_system,\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_ollama(stats, agent.model, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n        except Exception as e:\n            logger.error(f\"[{request_id}] Fallback also failed: {e}\")\n            raise\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Chat with self-reflection/debate loop", "output": "```python\n    async def chat_with_reflection(\n        self,\n        messages: list[dict],\n        request_id: str,\n        profile: str = \"qwen-sonnet\",\n        temperature: Optional[float] = None,\n        max_iterations: int = 2,\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str, str], None]:\n        \"\"\"\n        Chat with self-reflection/debate loop.\n\n        Modellen kritiserar och förbättrar sitt eget svar iterativt.\n        Inspirerat av Claudes self-reflection och Groks debate-tips.\n\n        Yields:\n            Tuple of (token, stats, agent_id, phase)\n            - phase: \"initial\", \"critique\", \"improvement\", \"final\"\n        \"\"\"\n        # Phase 1: Initial response\n        logger.info(f\"[{request_id}] Debate loop: Phase 1 - Initial response\")\n        full_response = \"\"\n\n        async for token, stats, agent_id in self.chat_stream(\n            messages=messages,\n            request_id=f\"{request_id}-initial\",\n            profile=profile,\n            temperature=temperature\n        ):\n            if stats:\n                yield \"\", stats, agent_id, \"initial\"\n            else:\n                full_response += token\n                yield token, None, agent_id, \"initial\"\n\n        # Phase 2-3: Critique and improve (iterate)\n        for i in range(max_iterations):\n            # Critique prompt\n            critique_messages = [\n                {\"role\": \"user\", \"content\": f\"\"\"Kritiskt granska denna kod/text:\n\n{full_response}\n\nIdentifiera:\n1. Logiska fel eller buggar\n2. Säkerhetsproblem\n3. Förbättringsmöjligheter\n4. Missade edge cases\n\nOm allt ser bra ut, svara med \"APPROVED - ingen förbättring behövs\".\nAnnars lista konkreta förbättringar.\"\"\"}\n            ]\n\n            logger.info(f\"[{request_id}] Debate loop: Iteration {i+1} - Critique\")\n            critique_response = \"\"\n\n            async for token, stats, agent_id in self.chat_stream(\n                messages=critique_messages,\n                request_id=f\"{request_id}-critique-{i}\",\n                profile=profile,\n                temperature=0.3  # Lower temp for critique\n            ):\n                if stats:\n                    yield \"\", stats, agent_id, \"critique\"\n                else:\n                    critique_response += token\n                    yield token, None, agent_id, \"critique\"\n\n            # Check if approved\n            if \"APPROVED\" in critique_response.upper():\n                logger.info(f\"[{request_id}] Debate loop: Response approved after {i+1} iterations\")\n                break\n\n            # Improvement phase\n            improve_messages = [\n                {\"role\": \"user\", \"content\": messages[-1].get(\"content\", \"\")},\n                {\"role\": \"assistant\", \"content\": full_response},\n                {\"role\": \"user\", \"content\": f\"\"\"Förbättra ditt svar baserat på denna kritik:\n\n{critique_response}\n\nGe en förbättrad version av hela svaret.\"\"\"}\n            ]\n\n            logger.info(f\"[{request_id}] Debate loop: Iteration {i+1} - Improvement\")\n            improved_response = \"\"\n\n            async for token, stats, agent_id in self.chat_stream(\n                messages=improve_messages,\n                request_id=f\"{request_id}-improve-{i}\",\n                profile=profile,\n                temperature=temperature\n            ):\n                if stats:\n                    yield \"\", stats, agent_id, \"improvement\"\n                else:\n                    improved_response += token\n                    yield token, None, agent_id, \"improvement\"\n\n            full_response = improved_response\n\n        # Final marker\n        yield \"\", None, agent_id, \"final\"\n        logger.info(f\"[{request_id}] Debate loop: Complete\")\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Check if API key is configured", "output": "```python\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Gemini API is reachable and configured", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Gemini API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Gemini API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - list models\n            response = await client.get(\n                f\"{GEMINI_BASE_URL}/models\",\n                params={\"key\": self.api_key},\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Gemini connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Convert chat messages to Gemini format", "output": "```python\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> tuple[list[dict], Optional[dict]]:\n        \"\"\"Convert chat messages to Gemini format\"\"\"\n        gemini_contents = []\n        system_instruction = None\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Gemini uses system_instruction separately\n                system_instruction = {\"parts\": [{\"text\": content}]}\n            elif role == \"user\":\n                gemini_contents.append({\n                    \"role\": \"user\",\n                    \"parts\": [{\"text\": content}]\n                })\n            elif role == \"assistant\":\n                gemini_contents.append({\n                    \"role\": \"model\",\n                    \"parts\": [{\"text\": content}]\n                })\n\n        # Apply external system prompt if provided and none in messages\n        if system_prompt and not system_instruction:\n            system_instruction = {\"parts\": [{\"text\": system_prompt}]}\n\n        return gemini_contents, system_instruction\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Gemini API response", "output": "```python\nclass GeminiStats:\n    \"\"\"Statistics from Gemini API response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Gemini errors", "output": "```python\nclass GeminiError(Exception):\n    \"\"\"Base exception for Gemini errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Gemini failed", "output": "```python\nclass GeminiConnectionError(GeminiError):\n    \"\"\"Connection to Gemini failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Gemini API returned an error", "output": "```python\nclass GeminiAPIError(GeminiError):\n    \"\"\"Gemini API returned an error\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Google Gemini API with streaming support", "output": "```python\nclass GeminiClient:\n    \"\"\"\n    Async client for Google Gemini API with streaming support.\n\n    Used for FAST mode - immediate responses without GPU load.\n    \"\"\"\n\n    def __init__(self, api_key: str = GEMINI_API_KEY, model: str = GEMINI_MODEL):\n        self.api_key = api_key\n        self.model = model\n        self._client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Gemini API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Gemini API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - list models\n            response = await client.get(\n                f\"{GEMINI_BASE_URL}/models\",\n                params={\"key\": self.api_key},\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Gemini connection check failed: {e}\")\n            return False\n\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> tuple[list[dict], Optional[dict]]:\n        \"\"\"Convert chat messages to Gemini format\"\"\"\n        gemini_contents = []\n        system_instruction = None\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Gemini uses system_instruction separately\n                system_instruction = {\"parts\": [{\"text\": content}]}\n            elif role == \"user\":\n                gemini_contents.append({\n                    \"role\": \"user\",\n                    \"parts\": [{\"text\": content}]\n                })\n            elif role == \"assistant\":\n                gemini_contents.append({\n                    \"role\": \"model\",\n                    \"parts\": [{\"text\": content}]\n                })\n\n        # Apply external system prompt if provided and none in messages\n        if system_prompt and not system_instruction:\n            system_instruction = {\"parts\": [{\"text\": system_prompt}]}\n\n        return gemini_contents, system_instruction\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[GeminiStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Gemini.\n\n        Yields:\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if xAI API is reachable and configured", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if xAI API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"xAI API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - send a minimal request\n            response = await client.post(\n                self.base_url,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": self.model,\n                    \"messages\": [{\"role\": \"user\", \"content\": \"ping\"}],\n                    \"max_tokens\": 10,\n                    \"stream\": False\n                },\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"xAI connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from xAI API response", "output": "```python\nclass XAIStats:\n    \"\"\"Statistics from xAI API response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for xAI errors", "output": "```python\nclass XAIError(Exception):\n    \"\"\"Base exception for xAI errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to xAI failed", "output": "```python\nclass XAIConnectionError(XAIError):\n    \"\"\"Connection to xAI failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: xAI API returned an error", "output": "```python\nclass XAIAPIError(XAIError):\n    \"\"\"xAI API returned an error\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for xAI Grok API with streaming support", "output": "```python\nclass XAIClient:\n    \"\"\"\n    Async client for xAI Grok API with streaming support.\n\n    Used for FAST mode - immediate responses without GPU load.\n    Compatible with OpenAI API format.\n    \"\"\"\n\n    def __init__(self, api_key: str = XAI_API_KEY, model: str = XAI_MODEL, base_url: str = XAI_BASE_URL):\n        self.api_key = api_key\n        self.model = model\n        self.base_url = base_url\n        self._client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if xAI API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"xAI API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - send a minimal request\n            response = await client.post(\n                self.base_url,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": self.model,\n                    \"messages\": [{\"role\": \"user\", \"content\": \"ping\"}],\n                    \"max_tokens\": 10,\n                    \"stream\": False\n                },\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"xAI connection check failed: {e}\")\n            return False\n\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> list[dict]:\n        \"\"\"Convert chat messages to xAI/OpenAI format\"\"\"\n        xai_messages = []\n\n        # Add system prompt if provided\n        if system_prompt:\n            xai_messages.append({\n                \"role\": \"system\",\n                \"content\": system_prompt\n            })\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Skip system messages if we already added system_prompt\n                if not system_prompt:\n                    xai_messages.append({\n                        \"role\": \"system\",\n                        \"content\": content\n                    })\n            else:\n                # user and assistant roles are directly compatible\n                xai_messages.append({\n                    \"role\": role,\n                    \"content\": content\n                })\n\n        return xai_messages\n\n    async def chat_stream(\n        self,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Definiera ett TypeScript interface för \\1iosk \\1gent", "output": "```typescript\ninterface KioskAgent {\n  id: string\n  name: string\n  description: string\n  color: string\n  glowColor: string\n  iconName: 'brain' | 'code'\n  model: string\n  vramGb: number\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1og \\1tate", "output": "```typescript\ninterface LogState {\n  logs: LogEntry[];\n\n  addLog: (message: string, type?: LogEntry['type']) => void;\n  handleSystemLog: (data: { message: string; level?: string }\n```"}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1", "output": "```typescript\nconst BACKEND_URL = `http://${typeof window !== 'undefined' ? window.location.hostname : 'localhost'}:8000`;\n\n// Initial agent states for 3D - SVEN Dual-Expert System\nconst initialAgentStates: Record<string, AgentState> = {\n  'orchestrator': 'idle',\n  // Primary agents (new names)\n  'qwen-sonnet': 'idle',\n  'devstral-openhands': 'idle',\n  'qwen-openhands': 'idle',\n  // Legacy aliases (backwards compatibility)\n  'gpt-oss': 'idle',\n  'devstral': 'idle',\n}\n```\n\nKomponenten finns i `frontend/src/stores/agentStore.ts`."}
{"instruction": "Definiera ett TypeScript interface för \\1gent \\1tore \\1tate", "output": "```typescript\ninterface AgentStoreState {\n  // Profiles\n  profiles: Profile[];\n  activeAgentId: string | null;\n  loadingAgent: string | null;\n\n  // Agent network state (for 2D panel)\n  mutedAgents: Set<string>;\n  soloAgent: string | null;\n\n  // 3D Visualization state\n  agentStates: Record<string, AgentState>;\n  activeFlows: FlowConnection[];\n\n  // Actions\n  setActiveAgent: (id: string | null) => void;\n  setLoadingAgent: (id: string | null) => void;\n  toggleMute: (id: string) => void;\n  toggleSolo: (id: string) => void;\n\n  // 3D actions\n  setAgentState: (agentId: string, state: AgentState) => void;\n  startFlow: (from: string, to: string, taskId: string) => void;\n  endFlow: (taskId: string) => void;\n\n  // WebSocket handler\n  handleWSMessage: (data: any) => void;\n\n  // Profile actions\n  fetchProfileStatus: () => Promise<void>;\n  selectAgent: (profileId: string) => Promise<void>;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1essage", "output": "```typescript\ninterface Message {\n  id?: string;\n  role: 'system' | 'ai' | 'user';\n  text: string;\n  time: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1og \\1ntry", "output": "```typescript\ninterface LogEntry {\n  id: string;\n  time: string;\n  message: string;\n  type: 'info' | 'success' | 'warning' | 'error';\n  count: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1rofile", "output": "```typescript\ninterface Profile {\n  id: string;\n  name: string;\n  display_name: string;\n  description: string;\n  model: string;\n  estimated_vram_gb: number;\n  icon: string;\n  color: string;\n  is_active: boolean;\n  is_loading: boolean;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1 \\1 \\1 \\1tats", "output": "```typescript\ninterface GPUStats {\n  vram_used_gb: number;\n  vram_total_gb: number;\n  vram_percent: number;\n  temperature_c: number;\n  utilization_percent?: number;\n  power_draw_w?: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1 \\1 \\1essage", "output": "```typescript\ninterface WSMessage {\n  type: WSMessageType;\n  data?: any;\n  error?: string;\n  profile?: string;\n  timestamp?: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1obile \\1ctivity \\1tate", "output": "```typescript\ninterface MobileActivityState {\n  status: 'idle' | 'processing' | 'complete';\n  profile: string;\n  userPrompt: string;\n  streamingText: string;\n  tokensGenerated: number;\n  lastQuestion: string;\n  lastAnswer: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1low \\1onnection", "output": "```typescript\ninterface FlowConnection {\n  id: string;\n  from: string;\n  to: string;\n  active: boolean;\n  progress: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1ystem \\1tatus", "output": "```typescript\ninterface SystemStatus {\n  backend_online: boolean;\n  ollama_online: boolean;\n  gpu_available: boolean;\n  active_profile: string | null;\n  last_error: string | null;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1onversation \\1ontext", "output": "```typescript\ninterface ConversationContext {\n  session_id?: string;\n  messages: Message[];\n  active_profile: string;\n  started_at: string;\n  last_activity: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1treaming \\1tats", "output": "```typescript\ninterface StreamingStats {\n  tokens_generated: number;\n  tokens_per_second: number;\n  time_elapsed_ms: number;\n  is_streaming: boolean;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1ar \\1oom \\1vent", "output": "```typescript\ninterface WarRoomEvent {\n  id: string;\n  type: 'agent_join' | 'agent_leave' | 'agent_speak' | 'consensus' | 'vote';\n  agent_id: string;\n  timestamp: string;\n  data?: any;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1gent \\1etrics", "output": "```typescript\ninterface AgentMetrics {\n  profile_id: string;\n  total_requests: number;\n  total_tokens: number;\n  average_latency_ms: number;\n  success_rate: number;\n  last_used: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1 \\1 \\1tate", "output": "```typescript\ninterface WSState {\n  ws: WebSocket | null;\n  isConnected: boolean;\n  reconnectAttempts: number;\n\n  // Actions\n  connect: () => void;\n  disconnect: () => void;\n  send: (data: object) => void;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1hat \\1tate", "output": "```typescript\ninterface ChatState {\n  messages: Message[];\n  currentResponse: string;\n  isStreaming: boolean;\n  selectedProfile: string;\n  isModelLoading: boolean;\n\n  // Actions\n  addMessage: (message: Omit<Message, 'id'>) => void;\n  setSelectedProfile: (profile: string) => void;\n  setIsModelLoading: (loading: boolean) => void;\n  handleWSMessage: (data: any) => void;\n  sendMessage: (text: string) => void;\n  warmupModel: (profileId: string) => Promise<void>;\n  clearMessages: () => void;\n}\n```"}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nexport const STATUS_COLORS = {\n  active: '#22c55e',    // Green\n  loading: '#f59e0b',   // Amber\n  idle: 'rgba(255,255,255,0.4)'\n}\n```\n\nKomponenten finns i `frontend/src/config/KioskConfig.ts`."}
{"instruction": "Definiera ett TypeScript interface för \\1ode \\1osition", "output": "```typescript\ninterface NodePosition {\n  id: string\n  position: Vector3Tuple\n  label: string\n  color: string\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1onnection \\1onfig", "output": "```typescript\ninterface ConnectionConfig {\n  id: string\n  from: string\n  to: string\n  color: string\n  particleCount?: number\n  active: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1etwork \\1tate", "output": "```typescript\ninterface NetworkState {\n  nodes: AgentNode[]\n  connections: ConnectionConfig[]\n  activeAgent: string | null\n}\n```"}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst NODE_CONFIG = [\n    { id: 'orchestrator', name: 'ORCHESTRATOR', type: 'orchestrator', color: '#f472b6', subLabel: 'ROUTING' }\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent för \\1ontext \\1enu", "output": "```typescript\nconst ContextMenu = ({ x, y, onClose, onAction }\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent för \\1gent \\1ode", "output": "```typescript\nconst AgentNode = ({ \n    config, \n    isActive, \n    isThinking,\n    onSelect,\n    isSolo,\n    isMuted,\n    onToggleMute,\n    onToggleSolo\n}\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst THEME = {\n    black: '#020204',\n    cyan: '#00f0ff',        // QWEN-SONNET\n    purple: '#bd00ff',      // DEVSTRAL-OPENHANDS\n    steel: '#d0d8e0',       // QWEN-OPENHANDS (ljusgra)\n    steelEmissive: '#c0c8ff', // QWEN-OPENHANDS emissive\n    white: '#ffffff',\n    warning: '#ff0055'\n}\n```\n\nKomponenten finns i `frontend/src/Frontend_3D_Bakgrund.tsx`."}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst AGENT_COLORS = {\n    'qwen-sonnet': new THREE.Color('#00f0ff'),      // Cyan\n    'devstral-openhands': new THREE.Color('#bd00ff'), // Magenta\n    'qwen-openhands': new THREE.Color('#d0d8e0'),   // Gray\n    cloud: new THREE.Color('#38bdf8'),               // Sky blue\n    default: new THREE.Color('#00f0ff')              // Default to QWEN-SONNET\n}\n```\n\nKomponenten finns i `frontend/src/SynapticStream.tsx`."}
{"instruction": "Definiera ett TypeScript interface för \\1ynaptic \\1tream \\1rops", "output": "```typescript\ninterface SynapticStreamProps {\n    isTyping: boolean\n    activeAgent: string | null\n    startPos?: [number, number, number] // Orb Position\n    endPos?: [number, number, number]   // Chat Panel Position\n}\n```"}
{"instruction": "Skapa en React-komponent för \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst AI_MODULES = [\n  { id: 'qwen-sonnet', name: 'QWEN-SONNET', type: 'DAILY' }\n```\n\nKomponenten finns i `frontend/src/Frontend_Huvudsida.tsx`."}
{"instruction": "Skapa en React-komponent för \\1ystem \\1ontrol \\1ard", "output": "```typescript\nconst SystemControlCard = lazy(() => import('./components/SystemControlCard'))\n\n// Loading spinner for Suspense fallback\nconst LoadingSpinner = () => (\n  <div className=\"flex flex-col h-full justify-center items-center\">\n    <div className=\"w-16 h-16 border-4 border-cyan-500/30 border-t-cyan-400 rounded-full animate-spin\" />\n    <p className=\"text-cyan-400 text-lg font-mono mt-4\">Loading System Control...</p>\n  </div>\n)\n\n// Ripple Effect Component\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Skapa en React-komponent för \\1oading \\1pinner", "output": "```typescript\nconst LoadingSpinner = () => (\n  <div className=\"flex flex-col h-full justify-center items-center\">\n    <div className=\"w-16 h-16 border-4 border-cyan-500/30 border-t-cyan-400 rounded-full animate-spin\" />\n    <p className=\"text-cyan-400 text-lg font-mono mt-4\">Loading System Control...</p>\n  </div>\n)\n\n// Ripple Effect Component\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Skapa en React-komponent för \\1ipple \\1ffect", "output": "```typescript\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Definiera ett TypeScript interface för \\1eural \\1ode \\1lock \\1rops", "output": "```typescript\ninterface NeuralCodeBlockProps {\n  code: string\n  language?: string\n  isStreaming?: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1article", "output": "```typescript\ninterface Particle {\n  x: number\n  y: number\n  vx: number\n  vy: number\n  size: number\n  connections: number[]\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1onfetti", "output": "```typescript\ninterface Confetti {\n  id: number\n  x: number\n  y: number\n  color: string\n  rotation: number\n  scale: number\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1ystem \\1ontrol \\1ard \\1rops", "output": "```typescript\ninterface SystemControlCardProps {\n  addLog: (message: string, type: 'info' | 'success' | 'warning' | 'error') => void\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1laude \\1tats", "output": "```typescript\ninterface ClaudeStats {\n  success: boolean\n  stats?: {\n    tokens: number\n    cost: number\n    models: string[]\n    input_tokens: number\n    output_tokens: number\n    cache_read: number\n    cache_create: number\n  }\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1ode \\1lock \\1rops", "output": "```typescript\ninterface CodeBlockProps {\n  code: string\n  language?: string\n  isStreaming?: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface för \\1arsed \\1egment", "output": "```typescript\ninterface ParsedSegment {\n  type: 'text' | 'code'\n  content: string\n  language?: string\n}\n```"}
{"instruction": "Skapa en React-komponent för \\1peech \\1ecognition", "output": "```typescript\nconst SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition\n\n    if (!SpeechRecognition) {\n      setErrorMsg('Röst stöds ej')\n      onLog?.('Speech Recognition not available', 'error')\n      setState('error')\n      setTimeout(() => setState('idle'), 3000)\n      return\n    }\n```\n\nKomponenten finns i `frontend/src/components/VoiceButton.tsx`."}
{"instruction": "Definiera ett TypeScript interface för \\1oice \\1utton \\1rops", "output": "```typescript\ninterface VoiceButtonProps {\n  onLog?: (message: string, type: 'info' | 'success' | 'warning' | 'error') => void\n  backendUrl: string\n  selectedProfile?: string  // Agent profile (qwen-sonnet, devstral-openhands, cloud)\n}\n```"}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst POSITIONS = {\n  orchestrator: [0, 0, 0] as [number, number, number],\n  // QWEN-SONNET (cyan cube) - upper left\n  'qwen-sonnet': [-3, 1.5, -2] as [number, number, number],\n  // DEVSTRAL-OPENHANDS (magenta icosahedron) - upper right\n  'devstral-openhands': [3, 1, -2] as [number, number, number],\n  // QWEN-OPENHANDS (steel dodecahedron) - lower center\n  'qwen-openhands': [0, -2.5, -3] as [number, number, number],\n  // Legacy positions for backwards compatibility\n  'gpt-oss': [-3, 1.5, -2] as [number, number, number],\n  'devstral': [3, 1, -2] as [number, number, number],\n}\n```\n\nKomponenten finns i `frontend/src/components/3d/AgentGraph.tsx`."}
{"instruction": "Skapa en React-komponent för \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst COLORS = {\n  orchestrator: '#ffffff',\n  gptOss: '#00f0ff',\n  devstral: '#ff00ff'\n}\n```\n\nKomponenten finns i `frontend/src/components/3d/AgentNetworkScene.tsx`."}
{"instruction": "Definiera ett TypeScript interface för \\1ode \\1phere \\1rops", "output": "```typescript\ninterface NodeSphereProps {\n  position: [number, number, number]\n  color: string\n  active: boolean\n}\n```"}
