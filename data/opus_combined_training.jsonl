{"instruction": "Skapa ett n8n workflow som orkestrerar flera AI-agenter med Ollama RAG och specialiserade services", "output": "## AI Agent Orchestrator med Ollama RAG\n\n### Arkitektur\nWorkflowet använder en central orchestrator-agent som koordinerar specialiserade n8n workflow-services via HTTP webhooks.\n\n### Komponenter\n1. **Webhook Trigger** - Tar emot användarförfrågningar via POST\n2. **Workflow Configuration** - Centraliserad konfiguration för alla service-URLs\n3. **Orchestrator Agent** - LangChain agent med Ollama som koordinerar tools\n4. **RAG Vector Store** - In-memory vektor-databas för kontextuell kunskap\n5. **Specialized Service Tools** - HTTP Request Tools för Database, Sentiment, PDF services\n\n### Implementation\n```javascript\n// Orchestrator System Prompt\nconst systemMessage = `You are an orchestrator agent that coordinates specialized n8n workflow services.\n\nYour role is to:\n1. Analyze the user request and determine which specialized services are needed\n2. Call the appropriate service tools in the correct sequence\n3. Coordinate data flow between services\n4. Use the RAG knowledge base when you need contextual information\n5. Return the final result to the user`;\n\n// Service Tool Configuration\nconst databaseServiceTool = {\n  toolDescription: \"Calls the specialized database service workflow\",\n  method: \"POST\",\n  url: \"{{ $('Workflow Configuration').first().json.databaseServiceUrl }}\",\n  bodyParameters: {\n    query: \"{{ $fromAI('query', 'The database query to perform') }}\",\n    parameters: \"{{ $fromAI('parameters', 'Query parameters as JSON', 'json', {}) }}\"\n  }\n};\n```\n\n### Best Practices\n- Använd `retrieve-as-tool` mode för RAG vector store\n- Separera services i egna workflows för moduläritet\n- Inkludera `includeOtherFields: true` i Set-noder för att bevara data"}
{"instruction": "Bygg ett multi-source business intelligence system med AI-driven analytics i n8n", "output": "## AI-Driven Multi-Source Business Intelligence\n\n### Syfte\nSamla data från flera källor, analysera med AI och leverera insikter anpassade för olika målgrupper.\n\n### Dataflöde\n```\nMultiple Sources → Normalize → AI Analysis → Audience Adaptation → Delivery\n```\n\n### Komponenter\n1. **Data Sources** - Google Sheets, APIs, Databases\n2. **Data Normalization** - Set-noder som standardiserar format\n3. **AI Analysis Agent** - LangChain agent för intelligent analys\n4. **Audience Adaptation** - Switch-nod som routar till rätt format\n5. **Delivery Channels** - Slack, Email, Dashboard\n\n### Implementation\n```javascript\n// Normalize data from different sources\nconst normalizeData = {\n  assignments: [\n    { name: \"dataType\", value: \"={{ $json.type || 'unknown' }}\" },\n    { name: \"value\", value: \"={{ $json.amount || $json.value || 0 }}\" },\n    { name: \"timestamp\", value: \"={{ $json.date || $now.toISO() }}\" },\n    { name: \"source\", value: \"={{ $json.source || 'manual' }}\" }\n  ]\n};\n\n// AI Analysis System Prompt\nconst analysisPrompt = `Analyze this business data and provide:\n1. Key trends and patterns\n2. Anomalies that need attention\n3. Actionable recommendations\n4. Risk assessment\n\nBe data-driven and specific.`;\n```\n\n### Tips\n- Använd Merge-nod med `combineAll` för att samla alla datakällor\n- Implementera caching för frekventa API-anrop\n- Logga alla analyser för audit trail"}
{"instruction": "Skapa ett automatiskt dokumentationssystem som genererar docs från GitHub, Slack, Jira och Linear events", "output": "## Auto-Generated Internal Documentation System\n\n### Arkitektur\nWorkflowet lyssnar på events från utvecklingsverktyg och genererar automatiskt strukturerad dokumentation.\n\n### Event Sources\n1. **GitHub Webhook** - Push, PR, Issues\n2. **Slack Webhook** - Viktiga diskussioner\n3. **Jira Webhook** - Issue updates\n4. **Linear Webhook** - Task changes\n\n### AI Agents Pipeline\n```\nEvent → Normalize → Context Analyzer → Route by Type → Specialized Writer → Vector Store → Output\n```\n\n### Implementation\n```javascript\n// Normalize Event Data\nconst normalizeEvent = {\n  eventType: \"={{ $json.body.action || $json.body.event?.type || 'unknown' }}\",\n  eventSource: \"={{ $json.headers['x-github-event'] ? 'github' : $json.headers['x-slack-signature'] ? 'slack' : 'unknown' }}\",\n  eventTitle: \"={{ $json.body.pull_request?.title || $json.body.issue?.title || 'No title' }}\",\n  eventDescription: \"={{ $json.body.pull_request?.body || $json.body.issue?.body || '' }}\",\n  eventAuthor: \"={{ $json.body.sender?.login || $json.body.user?.name || 'Unknown' }}\"\n};\n\n// Context Analyzer Agent\nconst analyzerPrompt = `Analyze this development event and determine:\n1. What changed or happened?\n2. Why did it happen? (extract intent)\n3. What systems/components are affected?\n4. What type of documentation should be updated?\n\nDocumentation types:\n- architecture: System design changes\n- api: API endpoints, formats\n- changelog: User-facing changes\n- lessons: Problems and solutions found`;\n\n// Specialized Writers\nconst architectureWriter = {\n  systemMessage: `You are an architecture documentation writer.\n  - Describe system components and relationships\n  - Explain architectural decisions\n  - Use Mermaid diagrams when relevant\n  - Maintain continuity with existing docs`\n};\n```\n\n### Output Destinations\n- **Notion** - Structured database pages\n- **GitHub Docs** - Markdown files in repo\n- **Confluence** - Wiki pages\n- **Qdrant** - Vector storage for RAG"}
{"instruction": "Implementera ett intelligent RAG-system med Ollama som har multiple tools och cross-document intelligence", "output": "## Intelligent RAG Ollama Multi-Function System\n\n### Funktioner\n- RAG-baserad Q&A med lokal Ollama\n- Real-time API integration\n- Email/Slack actions\n- Cross-document relationship analysis\n\n### Komponenter\n```\nQuery → Config → RAG Agent → Tools → Actions → Response\n         ↓\n    Auto-Update → Fetch → Clean → Vector Store → Intelligence\n```\n\n### Implementation\n```javascript\n// Workflow Configuration\nconst config = {\n  ollamaModel: \"llama3.2\",\n  embeddingModel: \"nomic-embed-text\",\n  chunkSize: 1000,\n  chunkOverlap: 200\n};\n\n// RAG Orchestrator Agent\nconst ragPrompt = `Du är en intelligent RAG-assistent på svenska.\n\nTillgängliga verktyg:\n1. knowledge_base_search: Sök i kunskapsbasen\n2. fetch_live_data: Hämta realtidsdata\n3. Gmail/Slack: Skicka meddelanden\n4. schedule_reminder: Schemalägg uppgifter\n\nAnvänd rätt verktyg, kombinera data, förklara källor.`;\n\n// Cross-Document Intelligence (Code Node)\nfunction analyzeRelationships(documents) {\n  const extractKeyTerms = (text) => {\n    return text.toLowerCase()\n      .replace(/[^\\w\\s]/g, ' ')\n      .split(/\\s+/)\n      .filter(word => word.length > 4);\n  };\n  \n  const calculateSimilarity = (doc1, doc2) => {\n    const terms1 = new Set(extractKeyTerms(doc1.content));\n    const terms2 = new Set(extractKeyTerms(doc2.content));\n    const intersection = [...terms1].filter(x => terms2.has(x));\n    const union = new Set([...terms1, ...terms2]);\n    return intersection.length / union.size; // Jaccard similarity\n  };\n  \n  // Build knowledge graph\n  const knowledgeGraph = { nodes: [], edges: [] };\n  // ... implementation\n  return knowledgeGraph;\n}\n```\n\n### Swedish Document Handling\n```javascript\n// Normalize Swedish characters\ncontent = content\n  .replace(/Ã¥/g, 'å')\n  .replace(/Ã¤/g, 'ä')\n  .replace(/Ã¶/g, 'ö');\n```"}
{"instruction": "Bygg ett Second Brain cognitive engine med multi-source intelligence processing i n8n", "output": "## Second Brain Cognitive Engine\n\n### Koncept\nEtt personligt AI-system som samlar, analyserar och organiserar information från alla dina digitala källor.\n\n### Data Sources\n1. **Gmail** - Email trigger var 5:e minut\n2. **Google Drive** - Document changes\n3. **Google Calendar** - Event updates\n4. **Slack** - Important messages\n\n### AI Agent Architecture\n```\n┌─────────────────────────────────────────────┐\n│           ORCHESTRATOR AGENT                │\n├─────────────────────────────────────────────┤\n│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n│  │Classifier│ │Summarizer│ │Task     │       │\n│  │Agent    │ │Agent    │ │Extractor│       │\n│  └─────────┘ └─────────┘ └─────────┘       │\n│  ┌─────────┐ ┌─────────┐                   │\n│  │Priority │ │Context  │                   │\n│  │Planner  │ │Finder   │                   │\n│  └─────────┘ └─────────┘                   │\n└─────────────────────────────────────────────┘\n```\n\n### Implementation\n```javascript\n// Normalize all sources to common format\nconst normalizeEmail = {\n  sourceType: \"email\",\n  content: \"={{ $json.textPlain || $json.textHtml }}\",\n  title: \"={{ $json.subject }}\",\n  timestamp: \"={{ $json.date }}\",\n  sender: \"={{ $json.from.address }}\"\n};\n\n// Classifier Agent Tool\nconst classifierPrompt = `Categorize into ONE category:\n- economy (financial matters)\n- health (medical, fitness)\n- relationships (personal)\n- projects (work initiatives)\n- tasks (actionable items)\n- ideas (creative thoughts)\n- knowledge (learning)\nReturn ONLY the category name.`;\n\n// Task Extractor with Output Parser\nconst taskSchema = {\n  type: \"object\",\n  properties: {\n    tasks: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          description: { type: \"string\" },\n          deadline: { type: \"string\" },\n          priority: { type: \"string\" },\n          context: { type: \"string\" }\n        }\n      }\n    }\n  }\n};\n\n// Priority Planner - 3-day action plan\nconst plannerPrompt = `Create a 3-day action plan.\nConsider:\n- Deadlines and time sensitivity\n- Impact and importance\n- Dependencies between tasks\n- Realistic capacity (3-5 tasks/day)`;\n```\n\n### Output Channels\n- **Slack** - Daily summary with mrkdwn formatting\n- **Notion** - Structured archive database\n- **Vector Store** - Long-term memory for RAG"}
{"instruction": "Skapa ett automatiskt fakturapåminnelse-system med n8n", "output": "## Automated Invoice Payment Reminder System\n\n### Funktionalitet\nAutomatiska påminnelser vid T-3, förfallodatum och +7 dagar.\n\n### Workflow\n```\nDaily Check → Get Invoices → Calculate Stage → Filter → Prepare Email → Send → Log\n```\n\n### Implementation\n```javascript\n// Calculate Reminder Stage (Code Node)\nconst dueDate = new Date($input.item.json.dueDate);\nconst isPaid = $input.item.json.isPaid === 'TRUE';\nconst lastReminderSent = $input.item.json.lastReminderSent || '';\nconst today = new Date();\n\nconst daysUntilDue = Math.floor((dueDate - today) / (1000 * 60 * 60 * 24));\n\nlet reminderStage = null;\nlet shouldSendReminder = false;\n\nif (!isPaid) {\n  if (daysUntilDue === 3 && lastReminderSent !== 'T-3') {\n    reminderStage = 'T-3';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === 0 && lastReminderSent !== 'DueDate') {\n    reminderStage = 'DueDate';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === -7 && lastReminderSent !== 'Overdue+7') {\n    reminderStage = 'Overdue+7';\n    shouldSendReminder = true;\n  }\n}\n\nreturn { ...($input.item.json), daysUntilDue, reminderStage, shouldSendReminder };\n\n// Dynamic Email Subject\nconst emailSubject = reminderStage === 'T-3' \n  ? 'Påminnelse: Faktura förfaller om 3 dagar'\n  : reminderStage === 'DueDate'\n  ? 'Påminnelse: Faktura förfaller idag'\n  : 'Viktig påminnelse: Förfallen faktura (7 dagar)';\n```\n\n### Email Template (HTML)\n```html\n<div style=\"background-color: #f5f5f5; padding: 15px;\">\n  <strong>Fakturadetaljer:</strong><br>\n  Fakturanummer: {{ $json.invoiceNumber }}<br>\n  Belopp: {{ $json.amount }} SEK<br>\n  Förfallodatum: {{ $json.dueDate }}\n</div>\n<a href=\"{{ paymentLink }}\" style=\"background-color: #4CAF50; color: white; padding: 12px 24px;\">Betala nu</a>\n```\n\n### Google Sheets Integration\n- Läs fakturalista med `returnFirstMatch: false`\n- Uppdatera `lastReminderSent` och `lastReminderDate` efter skickat"}
{"instruction": "Hur strukturerar jag ett n8n workflow med AI-agenter och verktyg?", "output": "## n8n AI Agent Workflow Structure\n\n### Grundläggande Pattern\n```\nTrigger → Configuration → Agent → Tools → Output\n                           ↓\n                    Language Model\n                           ↓\n                      Memory/RAG\n```\n\n### Best Practices\n\n#### 1. Workflow Configuration Node\nAnvänd alltid en Set-nod i början för konfiguration:\n```javascript\n{\n  assignments: [\n    { name: \"apiUrl\", value: \"<PLACEHOLDER>\", type: \"string\" },\n    { name: \"modelName\", value: \"gpt-4.1-mini\", type: \"string\" },\n    { name: \"maxTokens\", value: 2048, type: \"number\" }\n  ],\n  includeOtherFields: true\n}\n```\n\n#### 2. Agent Node Setup\n```javascript\n// Använd promptType: \"define\" för full kontroll\n{\n  promptType: \"define\",\n  text: \"={{ $json.userInput }}\",\n  options: {\n    systemMessage: \"Du är en hjälpsam assistent...\"\n  }\n}\n```\n\n#### 3. Tool Connections\nKoppla verktyg via `ai_tool` connection type:\n- Vector Store som tool → `mode: \"retrieve-as-tool\"`\n- HTTP Request Tool → `toolDescription` är kritisk\n- Code Tool → Returnera alltid JSON.stringify()\n\n#### 4. Output Parsers\nFör strukturerad output:\n```javascript\n// Structured Output Parser\n{\n  schemaType: \"manual\",\n  inputSchema: JSON.stringify(yourSchema),\n  autoFix: true  // LLM fixar formatfel\n}\n```\n\n#### 5. Memory\n```javascript\n// Buffer Window Memory\n{\n  sessionKey: \"={{ $json.sessionId || 'default' }}\",\n  contextWindowLength: 10  // Antal meddelanden att minnas\n}\n```\n\n### Vanliga Misstag\n1. Glömmer `includeOtherFields: true` → tappar data\n2. Fel `typeVersion` → nod fungerar inte\n3. Saknar `toolDescription` → agent kan inte använda tool\n4. Hårdkodar värden istället för expressions"}
{"instruction": "Hur implementerar jag RAG med Vector Store i n8n?", "output": "## RAG Implementation i n8n\n\n### Två Modes\n1. **Insert Mode** - Lägg till dokument i vector store\n2. **Retrieve-as-Tool Mode** - Sök som agent tool\n\n### Insert Pipeline\n```\nData Source → Document Loader → Text Splitter → Embeddings → Vector Store (insert)\n```\n\n### Implementation\n```javascript\n// 1. Document Loader\n{\n  textSplittingMode: \"custom\",\n  jsonMode: \"expressionData\",\n  jsonData: \"={{ $json }}\"\n}\n\n// 2. Recursive Text Splitter\n{\n  chunkSize: 1000,\n  chunkOverlap: 200  // 20% overlap är standard\n}\n\n// 3. Embeddings (Ollama eller OpenAI)\n// Ollama:\n{ model: \"nomic-embed-text\" }\n// OpenAI:\n{ model: \"text-embedding-3-small\" }\n\n// 4. Vector Store - Insert\n{\n  mode: \"insert\",\n  memoryKey: \"my_knowledge_base\"\n}\n```\n\n### Retrieve som Tool\n```javascript\n// Vector Store - Retrieve as Tool\n{\n  mode: \"retrieve-as-tool\",\n  toolDescription: \"Sök i kunskapsbasen för att hitta relevant information\",\n  memoryKey: \"my_knowledge_base\",\n  topK: 5  // Antal resultat att returnera\n}\n```\n\n### Qdrant Integration (Persistent)\n```javascript\n// Qdrant Vector Store\n{\n  qdrantCollection: {\n    __rl: true,\n    mode: \"id\",\n    value: \"={{ $('Config').first().json.collectionName }}\"\n  }\n}\n```\n\n### Tips\n- Använd `In-Memory` för utveckling, `Qdrant` för produktion\n- `topK: 5` är bra default, öka vid behov\n- `toolDescription` måste vara tydlig för att agent ska använda rätt"}
{"instruction": "Skapa ett multipart memory ingestion system som hanterar audio, bild, PDF, URL och text med AI-processning och vector storage", "output": "## Multipart Memory Ingestion System\n\n### Arkitektur\nWebhook-baserat system som tar emot olika innehållstyper, extraherar text och lagrar i Qdrant vector store.\n\n### Content Type Routing\n```\nWebhook → Parse Multipart → Switch by Type → Process → Summarize → Keywords → Qdrant\n                                │\n                    ┌───────────┼───────────┐\n                    ▼           ▼           ▼\n                  Audio       Image        PDF\n                (Whisper)    (OCR)      (Extract)\n```\n\n### Implementation\n```javascript\n// Parse Multipart Data (Code Node)\nconst parsedData = {\n  type: body.type || null,\n  title: body.title || null,\n  text: body.text || null,\n  source: body.source || null,\n  timestamp: body.timestamp || new Date().toISOString(),\n  metadata: body.metadata || {},\n  user_id: body.user_id || null\n};\n\n// Handle binary file if present\nif (item.binary && Object.keys(item.binary).length > 0) {\n  binaryData = item.binary;\n  parsedData.file = Object.keys(item.binary)[0];\n}\n\n// Extract Keywords (Code Node)\nfunction extractKeywords(text) {\n  const stopWords = new Set(['the', 'a', 'an', 'and', 'or', ...]);\n  const words = text.toLowerCase()\n    .replace(/[^a-z0-9\\s]/g, ' ')\n    .split(/\\s+/)\n    .filter(word => word.length > 3 && !stopWords.has(word));\n  \n  const wordCount = {};\n  words.forEach(word => wordCount[word] = (wordCount[word] || 0) + 1);\n  \n  return Object.entries(wordCount)\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, 10)\n    .map(entry => entry[0]);\n}\n```\n\n### Content Processors\n- **Audio**: OpenAI Whisper transcription\n- **Image**: OCR via extractFromFile\n- **URL**: HTTP Request med responseFormat: text\n- **PDF**: extractFromFile med operation: pdf\n- **Text**: Direkt genomströmning\n\n### Vector Storage\n```javascript\n// Qdrant Insert\n{\n  mode: \"insert\",\n  qdrantCollection: \"{{ $('Config').first().json.qdrantCollection }}\"\n}\n\n// Search endpoint\n{\n  mode: \"load\",\n  prompt: \"{{ $json.query }}\",\n  topK: \"{{ $json.topK }}\"\n}\n```"}
{"instruction": "Implementera bidirectional data sync mellan Linear, Gmail, Calendar, Notion och Slack med conflict resolution", "output": "## Multi-Platform Bidirectional Data Sync\n\n### Funktionalitet\n- Synkar data mellan 5 plattformar var 15:e minut\n- Normaliserar data till gemensamt format\n- Hanterar konflikter med configuerbar strategi\n- Sparar sync state för incremental updates\n\n### Sync Flow\n```\nSchedule → Server Check → Load State → Fetch All → Normalize → Detect Changes → Resolve Conflicts → Route → Update → Save State\n```\n\n### Data Normalization (Code Node)\n```javascript\n// Detect platform from data structure\nlet platform, id, title, description, timestamp, status, metadata;\n\n// Linear Issues\nif (json.identifier || json.team) {\n  platform = 'linear';\n  id = json.id || json.identifier;\n  title = json.title || '';\n  status = json.state?.name || '';\n  metadata = { assignee: json.assignee?.name, priority: json.priority };\n}\n// Gmail Messages  \nelse if (json.threadId || json.labelIds) {\n  platform = 'gmail';\n  id = json.id || json.threadId;\n  title = json.subject || '';\n  status = json.labelIds?.includes('UNREAD') ? 'unread' : 'read';\n}\n// Google Calendar\nelse if (json.start?.dateTime) {\n  platform = 'calendar';\n  title = json.summary || '';\n  metadata = { start: json.start, end: json.end, attendees: json.attendees };\n}\n// Notion\nelse if (json.properties || json.object === 'page') {\n  platform = 'notion';\n  title = json.properties?.Name?.title?.[0]?.plain_text || '';\n}\n// Slack\nelse if (json.ts || json.channel) {\n  platform = 'slack';\n  title = json.text?.substring(0, 100) || '';\n}\n```\n\n### Conflict Resolution Strategies\n```javascript\nconst strategy = config.conflictResolutionStrategy; // 'latest_wins' | 'merge' | 'manual_review'\n\nif (strategy === 'latest_wins') {\n  resolved.data = sourceTimestamp > targetTimestamp ? sourceData : targetData;\n} else if (strategy === 'merge') {\n  resolved.data = { ...targetData };\n  for (const [key, value] of Object.entries(sourceData)) {\n    if (!targetData[key]) resolved.data[key] = value;\n    else if (JSON.stringify(targetData[key]) !== JSON.stringify(value)) {\n      resolved.needsManualReview = true;\n      resolved.conflictDetails.push({ field: key, sourceValue: value, targetValue: targetData[key] });\n    }\n  }\n}\n```\n\n### State Management\n```javascript\n// Save Sync State\n{\n  timestamp: $now.toISO(),\n  platforms: { linear, gmail, calendar, notion, slack },\n  lastSync: $now.toISO(),\n  syncStatus: 'completed'\n}\n```"}
{"instruction": "Bygg ett AI-driven newsletter system med Google Drive trigger, AI-generering, approval workflow och automated reminders", "output": "## AI Newsletter Generation System\n\n### Workflow Overview\n1. **Trigger**: Nya filer i Google Drive-mapp\n2. **Process**: Extrahera text, generera med AI\n3. **Store**: Spara draft i PostgreSQL\n4. **Approve**: Webhook för godkännande\n5. **Send**: Skicka via Gmail\n6. **Remind**: Veckovis påminnelse om tomt innehåll\n\n### Implementation\n```javascript\n// Workflow Configuration\n{\n  distributionList: \"team@company.com,all-hands@company.com\",\n  reminderRecipients: \"content-team@company.com\",\n  databaseTable: \"newsletters\"\n}\n\n// Check If Already Processed\n\"SELECT COUNT(*) as count FROM processed_files WHERE file_id = $1\"\n\n// Generate Newsletter with AI\n{\n  model: \"gpt-4o\",\n  messages: [\n    {\n      role: \"system\",\n      content: \"You are a professional newsletter writer. Create well-formatted HTML...\"\n    },\n    { role: \"user\", content: \"={{ $json.text }}\" }\n  ],\n  temperature: 0.7\n}\n\n// Save Newsletter Draft\nINSERT INTO newsletters (id, html_content, status, file_id, created_at)\nVALUES ($1, $2, 'pending', $3, NOW())\n```\n\n### Approval Flow\n```\nPOST /webhook/newsletter-approval\n  → Get Newsletter WHERE status='pending'\n  → Send via Gmail\n  → UPDATE status='sent', sent_at=NOW()\n  → Respond { success: true }\n```\n\n### Weekly Reminder\n```javascript\n// Check Content Last Week\n\"SELECT COUNT(*) FROM processed_files WHERE processed_at >= NOW() - INTERVAL '7 days'\"\n\n// If count = 0, send reminder\nif ($json.count === 0) {\n  sendEmail({\n    to: config.reminderRecipients,\n    subject: \"Reminder: No Newsletter Content This Week\"\n  });\n}\n```\n\n### Database Schema\n```sql\nCREATE TABLE newsletters (\n  id TEXT PRIMARY KEY,\n  html_content TEXT,\n  status TEXT DEFAULT 'pending',\n  file_id TEXT,\n  created_at TIMESTAMP,\n  sent_at TIMESTAMP\n);\n\nCREATE TABLE processed_files (\n  file_id TEXT PRIMARY KEY,\n  file_name TEXT,\n  processed_at TIMESTAMP DEFAULT NOW()\n);\n```"}
{"instruction": "Hur hanterar jag errors och edge cases i n8n workflows?", "output": "## Error Handling i n8n Workflows\n\n### 1. Server Health Check Pattern\n```javascript\n// Kolla server status innan main flow\n{\n  url: \"{{ $('Config').json.statusCheckUrl }}\",\n  options: { timeout: 5000 }\n}\n\n// If-nod: statusCode === 200\n// True → Continue\n// False → Send Error Alert\n```\n\n### 2. Idempotency Pattern\n```sql\n-- Kolla om redan processad\nSELECT COUNT(*) as count FROM processed_items WHERE id = $1\n\n-- If count > 0 → Skip\n-- Else → Process och INSERT\n```\n\n### 3. HTTP Request Error Handling\n```javascript\n{\n  options: {\n    response: {\n      response: {\n        neverError: true  // Returnera error som data istället för att faila\n      }\n    }\n  }\n}\n```\n\n### 4. Fallback Values\n```javascript\n// Säkra expressions med fallbacks\n$json.value || $json.defaultValue || 'fallback'\n$json.nested?.deeply?.value ?? 'default'\n```\n\n### 5. Error Webhook Alerts\n```javascript\n// Send Error Alert nod\n{\n  method: \"POST\",\n  url: \"{{ config.alertWebhookUrl }}\",\n  body: {\n    timestamp: \"{{ $now.toISO() }}\",\n    error: \"{{ $json.error }}\",\n    workflow: \"Workflow Name\",\n    node: \"{{ $node.name }}\"\n  }\n}\n```\n\n### 6. Retry Logic\n```javascript\n// Använd n8n's inbyggda retry\n{\n  retryOnFail: true,\n  maxTries: 3,\n  waitBetweenTries: 1000\n}\n```\n\n### 7. Data Validation\n```javascript\n// Code Node validation\nconst required = ['email', 'name', 'amount'];\nconst missing = required.filter(field => !$json[field]);\n\nif (missing.length > 0) {\n  throw new Error(`Missing fields: ${missing.join(', ')}`);\n}\n```\n\n### 8. Graceful Degradation\n```javascript\n// Switch med fallback output\n{\n  rules: [...],\n  options: {\n    fallbackOutput: 0  // Default route om ingen regel matchar\n  }\n}\n```"}
{"instruction": "Hur optimerar jag n8n workflows för prestanda?", "output": "## n8n Performance Optimization\n\n### 1. Batch Processing\n```javascript\n// Använd SplitInBatches för stora datasets\n{\n  batchSize: 100,\n  options: { reset: false }\n}\n\n// Eller Loop Over Items\nfor (const item of $input.all()) {\n  // Process\n}\n```\n\n### 2. Parallel Execution\n```javascript\n// Merge med combineByPosition för parallella branches\n{\n  mode: \"combine\",\n  combineBy: \"combineByPosition\",\n  numberInputs: 5\n}\n```\n\n### 3. Caching\n```javascript\n// Spara resultat i workflow static data\nconst cache = $getWorkflowStaticData('global');\nconst cacheKey = `api_${$json.id}`;\n\nif (cache[cacheKey] && Date.now() - cache[cacheKey].timestamp < 3600000) {\n  return cache[cacheKey].data;\n}\n\n// Fetch and cache\nconst result = await fetch(...);\ncache[cacheKey] = { data: result, timestamp: Date.now() };\n```\n\n### 4. Incremental Sync\n```javascript\n// Spara last sync timestamp\nconst lastSync = $getWorkflowStaticData('global').lastSync || '1970-01-01';\n\n// Query endast nya items\n{\n  filter: `modified_at > '${lastSync}'`\n}\n\n// Uppdatera efter sync\n$getWorkflowStaticData('global').lastSync = $now.toISO();\n```\n\n### 5. Minimize API Calls\n```javascript\n// Batch API requests\nconst ids = items.map(i => i.json.id);\nconst response = await fetch(`/api/items?ids=${ids.join(',')}`);\n\n// Istället för\nfor (const item of items) {\n  await fetch(`/api/items/${item.json.id}`); // Långsamt!\n}\n```\n\n### 6. Response Size\n```javascript\n// Begränsa returnerade fält\n{\n  options: {\n    fields: \"id,name,status\"  // Bara nödvändiga fält\n  }\n}\n```\n\n### 7. Timeout Configuration\n```javascript\n// Sätt rimliga timeouts\n{\n  options: {\n    timeout: 30000  // 30 sekunder\n  }\n}\n```\n\n### 8. Memory Management\n```javascript\n// Rensa stora objekt efter användning\ndelete $json._original;\ndelete $json.largePayload;\n\n// Använd streaming för stora filer\n{ responseFormat: \"file\" }\n```"}
{"instruction": "Skapa webhooks med authentication och response handling i n8n", "output": "## n8n Webhook Patterns\n\n### 1. Basic Authenticated Webhook\n```javascript\n{\n  httpMethod: \"POST\",\n  path: \"my-webhook\",\n  authentication: \"headerAuth\",  // Kräver credentials\n  responseMode: \"responseNode\",  // Manuell response\n  options: {}\n}\n```\n\n### 2. Response Modes\n```javascript\n// responseMode options:\n// - \"onReceived\": Svara direkt med 200\n// - \"lastNode\": Svara efter sista noden\n// - \"responseNode\": Använd Respond to Webhook nod\n\n// Respond to Webhook\n{\n  respondWith: \"json\",\n  responseBody: \"={{ $json }}\",\n  options: {\n    responseCode: 200,\n    responseHeaders: {\n      \"X-Custom-Header\": \"value\"\n    }\n  }\n}\n```\n\n### 3. Request Parsing\n```javascript\n// Webhook data locations\n$json.body       // POST body\n$json.query      // Query parameters\n$json.headers    // Request headers\n$json.params     // URL parameters\n\n// Multipart form data\n$json.body.field_name\n$binary.file     // Uploaded file\n```\n\n### 4. Validation Pattern\n```javascript\n// Code Node efter webhook\nconst required = ['action', 'data'];\nconst body = $json.body;\n\nfor (const field of required) {\n  if (!body[field]) {\n    return {\n      json: {\n        error: `Missing required field: ${field}`,\n        statusCode: 400\n      }\n    };\n  }\n}\n\nreturn { json: { ...body, validated: true } };\n```\n\n### 5. Webhook URL Format\n```\nProduction: https://your-n8n.com/webhook/path-name\nTest: https://your-n8n.com/webhook-test/path-name\n```\n\n### 6. Error Response\n```javascript\n// Respond to Webhook för errors\n{\n  respondWith: \"json\",\n  responseBody: {\n    success: false,\n    error: \"{{ $json.errorMessage }}\",\n    code: \"{{ $json.errorCode }}\"\n  },\n  options: {\n    responseCode: 400\n  }\n}\n```\n\n### 7. Async Processing\n```javascript\n// Svara direkt, processa i bakgrunden\n// 1. Webhook med responseMode: \"onReceived\"\n// 2. Async processing continues\n// 3. Callback webhook när klart\n```"}
{"instruction": "Create a rate limiter middleware for FastAPI that limits requests per IP to 100 per minute", "output": "Here's a production-ready rate limiter middleware:\n\n```python\nfrom fastapi import FastAPI, Request, HTTPException\nfrom collections import defaultdict\nimport time\nimport asyncio\n\nclass RateLimiter:\n    def __init__(self, requests_per_minute: int = 100):\n        self.requests_per_minute = requests_per_minute\n        self.requests = defaultdict(list)\n        self._cleanup_task = None\n    \n    async def start_cleanup(self):\n        while True:\n            await asyncio.sleep(60)\n            cutoff = time.time() - 60\n            for ip in list(self.requests.keys()):\n                self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n                if not self.requests[ip]:\n                    del self.requests[ip]\n    \n    def is_allowed(self, ip: str) -> bool:\n        now = time.time()\n        cutoff = now - 60\n        self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n        \n        if len(self.requests[ip]) >= self.requests_per_minute:\n            return False\n        \n        self.requests[ip].append(now)\n        return True\n\nrate_limiter = RateLimiter(requests_per_minute=100)\n\n@app.middleware('http')\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n    \n    if not rate_limiter.is_allowed(client_ip):\n        raise HTTPException(\n            status_code=429,\n            detail='Rate limit exceeded. Max 100 requests per minute.'\n        )\n    \n    return await call_next(request)\n```\n\nKey features:\n1. In-memory storage with automatic cleanup\n2. Sliding window algorithm (more accurate than fixed windows)\n3. Per-IP tracking\n4. Async-compatible cleanup task\n5. Returns 429 with clear error message"}
{"instruction": "Write a Python decorator that retries failed async functions with exponential backoff", "output": "Here's a robust retry decorator with exponential backoff:\n\n```python\nimport asyncio\nimport functools\nimport logging\nfrom typing import Type, Tuple, Callable, Any\n\nlogger = logging.getLogger(__name__)\n\ndef async_retry(\n    max_attempts: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n) -> Callable:\n    \"\"\"Retry decorator with exponential backoff for async functions.\n    \n    Args:\n        max_attempts: Maximum number of retry attempts\n        base_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay cap\n        exponential_base: Base for exponential calculation\n        exceptions: Tuple of exceptions to catch and retry\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> Any:\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_attempts - 1:\n                        logger.error(f'{func.__name__} failed after {max_attempts} attempts: {e}')\n                        raise\n                    \n                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n                    logger.warning(f'{func.__name__} attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s')\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage example:\n@async_retry(max_attempts=5, base_delay=0.5, exceptions=(ConnectionError, TimeoutError))\nasync def fetch_data(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, timeout=10)\n        return response.json()\n```\n\nFeatures:\n1. Configurable retry count and delays\n2. Exponential backoff with max cap\n3. Selective exception handling\n4. Logging for debugging\n5. Preserves function metadata with functools.wraps"}
{"instruction": "Implement a simple pub/sub system in Python using asyncio", "output": "Here's a lightweight async pub/sub implementation:\n\n```python\nimport asyncio\nfrom collections import defaultdict\nfrom typing import Callable, Any, Dict, Set\nfrom dataclasses import dataclass, field\nimport uuid\n\n@dataclass\nclass Subscription:\n    id: str\n    topic: str\n    callback: Callable\n\nclass PubSub:\n    def __init__(self):\n        self._subscribers: Dict[str, Set[Subscription]] = defaultdict(set)\n        self._lock = asyncio.Lock()\n    \n    async def subscribe(self, topic: str, callback: Callable) -> str:\n        \"\"\"Subscribe to a topic. Returns subscription ID for unsubscribing.\"\"\"\n        sub_id = str(uuid.uuid4())\n        subscription = Subscription(id=sub_id, topic=topic, callback=callback)\n        \n        async with self._lock:\n            self._subscribers[topic].add(subscription)\n        \n        return sub_id\n    \n    async def unsubscribe(self, sub_id: str) -> bool:\n        \"\"\"Unsubscribe using subscription ID.\"\"\"\n        async with self._lock:\n            for topic, subs in self._subscribers.items():\n                for sub in subs:\n                    if sub.id == sub_id:\n                        subs.remove(sub)\n                        return True\n        return False\n    \n    async def publish(self, topic: str, message: Any) -> int:\n        \"\"\"Publish message to topic. Returns number of subscribers notified.\"\"\"\n        async with self._lock:\n            subscribers = list(self._subscribers.get(topic, []))\n        \n        if not subscribers:\n            return 0\n        \n        tasks = []\n        for sub in subscribers:\n            if asyncio.iscoroutinefunction(sub.callback):\n                tasks.append(sub.callback(message))\n            else:\n                tasks.append(asyncio.to_thread(sub.callback, message))\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n        return len(subscribers)\n    \n    async def publish_pattern(self, pattern: str, message: Any) -> int:\n        \"\"\"Publish to all topics matching pattern (simple * wildcard).\"\"\"\n        import fnmatch\n        count = 0\n        \n        async with self._lock:\n            matching_topics = [t for t in self._subscribers.keys() if fnmatch.fnmatch(t, pattern)]\n        \n        for topic in matching_topics:\n            count += await self.publish(topic, message)\n        \n        return count\n\n# Usage:\npubsub = PubSub()\n\nasync def handler(msg):\n    print(f'Received: {msg}')\n\nasync def main():\n    sub_id = await pubsub.subscribe('events.user.*', handler)\n    await pubsub.publish('events.user.login', {'user': 'simon'})\n    await pubsub.unsubscribe(sub_id)\n```\n\nFeatures:\n1. Async-safe with locks\n2. Supports both sync and async callbacks\n3. Subscription IDs for clean unsubscribe\n4. Wildcard pattern matching\n5. Thread-safe for sync callbacks"}
{"instruction": "Create a WebSocket connection manager for FastAPI that handles multiple rooms/channels", "output": "Here's a full-featured WebSocket manager with room support:\n\n```python\nfrom fastapi import WebSocket\nfrom typing import Dict, Set, Optional, Any\nimport json\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass Connection:\n    websocket: WebSocket\n    user_id: str\n    rooms: Set[str] = field(default_factory=set)\n    connected_at: datetime = field(default_factory=datetime.utcnow)\n\nclass ConnectionManager:\n    def __init__(self):\n        self._connections: Dict[str, Connection] = {}  # user_id -> Connection\n        self._rooms: Dict[str, Set[str]] = {}  # room_name -> set of user_ids\n        self._lock = asyncio.Lock()\n    \n    async def connect(self, websocket: WebSocket, user_id: str) -> Connection:\n        await websocket.accept()\n        conn = Connection(websocket=websocket, user_id=user_id)\n        \n        async with self._lock:\n            # Disconnect existing connection for same user\n            if user_id in self._connections:\n                await self._disconnect_internal(user_id)\n            self._connections[user_id] = conn\n        \n        return conn\n    \n    async def disconnect(self, user_id: str):\n        async with self._lock:\n            await self._disconnect_internal(user_id)\n    \n    async def _disconnect_internal(self, user_id: str):\n        if user_id not in self._connections:\n            return\n        \n        conn = self._connections[user_id]\n        \n        # Remove from all rooms\n        for room in conn.rooms.copy():\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n        \n        del self._connections[user_id]\n        \n        try:\n            await conn.websocket.close()\n        except:\n            pass\n    \n    async def join_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id not in self._connections:\n                return\n            \n            self._connections[user_id].rooms.add(room)\n            if room not in self._rooms:\n                self._rooms[room] = set()\n            self._rooms[room].add(user_id)\n    \n    async def leave_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id in self._connections:\n                self._connections[user_id].rooms.discard(room)\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n    \n    async def send_personal(self, user_id: str, message: Any):\n        if user_id not in self._connections:\n            return False\n        \n        try:\n            data = json.dumps(message) if not isinstance(message, str) else message\n            await self._connections[user_id].websocket.send_text(data)\n            return True\n        except:\n            await self.disconnect(user_id)\n            return False\n    \n    async def broadcast_room(self, room: str, message: Any, exclude: Optional[str] = None):\n        if room not in self._rooms:\n            return 0\n        \n        tasks = []\n        for user_id in self._rooms[room]:\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    async def broadcast_all(self, message: Any, exclude: Optional[str] = None):\n        tasks = []\n        for user_id in list(self._connections.keys()):\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    def get_room_users(self, room: str) -> Set[str]:\n        return self._rooms.get(room, set()).copy()\n    \n    def get_user_rooms(self, user_id: str) -> Set[str]:\n        if user_id in self._connections:\n            return self._connections[user_id].rooms.copy()\n        return set()\n\n# Usage in FastAPI:\nmanager = ConnectionManager()\n\n@app.websocket('/ws/{user_id}')\nasync def websocket_endpoint(websocket: WebSocket, user_id: str):\n    conn = await manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_json()\n            \n            if data.get('action') == 'join':\n                await manager.join_room(user_id, data['room'])\n            elif data.get('action') == 'leave':\n                await manager.leave_room(user_id, data['room'])\n            elif data.get('action') == 'message':\n                await manager.broadcast_room(data['room'], {\n                    'from': user_id,\n                    'content': data['content']\n                })\n    except:\n        await manager.disconnect(user_id)\n```\n\nFeatures:\n1. Multi-room support with join/leave\n2. Personal and broadcast messaging\n3. Automatic cleanup on disconnect\n4. Thread-safe with async locks\n5. Handles reconnection (kicks old connection)"}
{"instruction": "Write a Python script that monitors a directory for new files and processes them", "output": "Here's a robust file watcher with processing queue:\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler, FileCreatedEvent\nimport queue\nimport threading\nimport logging\nfrom typing import Callable, Optional\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass FileProcessor:\n    def __init__(\n        self,\n        watch_dir: str,\n        process_func: Callable[[Path], None],\n        patterns: Optional[list] = None,\n        debounce_seconds: float = 1.0\n    ):\n        self.watch_dir = Path(watch_dir)\n        self.process_func = process_func\n        self.patterns = patterns or ['*']\n        self.debounce_seconds = debounce_seconds\n        \n        self._queue = queue.Queue()\n        self._seen = {}  # path -> last_seen_time\n        self._running = False\n        self._observer = None\n        self._worker_thread = None\n    \n    def _matches_pattern(self, path: Path) -> bool:\n        import fnmatch\n        return any(fnmatch.fnmatch(path.name, p) for p in self.patterns)\n    \n    def _on_created(self, event):\n        if event.is_directory:\n            return\n        \n        path = Path(event.src_path)\n        if not self._matches_pattern(path):\n            return\n        \n        # Debounce: skip if we've seen this file recently\n        now = datetime.now().timestamp()\n        if path in self._seen:\n            if now - self._seen[path] < self.debounce_seconds:\n                return\n        \n        self._seen[path] = now\n        self._queue.put(path)\n        logger.info(f'Queued: {path.name}')\n    \n    def _worker(self):\n        while self._running:\n            try:\n                path = self._queue.get(timeout=1.0)\n            except queue.Empty:\n                continue\n            \n            # Wait for file to be fully written\n            asyncio.run(self._wait_for_stable(path))\n            \n            if not path.exists():\n                logger.warning(f'File disappeared: {path}')\n                continue\n            \n            try:\n                logger.info(f'Processing: {path.name}')\n                self.process_func(path)\n                logger.info(f'Completed: {path.name}')\n            except Exception as e:\n                logger.error(f'Error processing {path.name}: {e}')\n            finally:\n                self._queue.task_done()\n    \n    async def _wait_for_stable(self, path: Path, checks: int = 3, interval: float = 0.5):\n        \"\"\"Wait until file size stops changing.\"\"\"\n        prev_size = -1\n        stable_count = 0\n        \n        while stable_count < checks:\n            if not path.exists():\n                return\n            \n            size = path.stat().st_size\n            if size == prev_size:\n                stable_count += 1\n            else:\n                stable_count = 0\n                prev_size = size\n            \n            await asyncio.sleep(interval)\n    \n    def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        \n        # Start worker thread\n        self._worker_thread = threading.Thread(target=self._worker, daemon=True)\n        self._worker_thread.start()\n        \n        # Setup watchdog\n        handler = FileSystemEventHandler()\n        handler.on_created = self._on_created\n        \n        self._observer = Observer()\n        self._observer.schedule(handler, str(self.watch_dir), recursive=False)\n        self._observer.start()\n        \n        logger.info(f'Watching: {self.watch_dir}')\n    \n    def stop(self):\n        self._running = False\n        \n        if self._observer:\n            self._observer.stop()\n            self._observer.join()\n        \n        if self._worker_thread:\n            self._worker_thread.join(timeout=5.0)\n        \n        logger.info('Stopped watching')\n    \n    def process_existing(self):\n        \"\"\"Process files that already exist in the directory.\"\"\"\n        for path in self.watch_dir.iterdir():\n            if path.is_file() and self._matches_pattern(path):\n                self._queue.put(path)\n\n# Example usage:\ndef process_file(path: Path):\n    \"\"\"Your processing logic here.\"\"\"\n    content = path.read_text()\n    print(f'File {path.name} has {len(content)} characters')\n    # Move to processed folder, upload somewhere, etc.\n\nif __name__ == '__main__':\n    processor = FileProcessor(\n        watch_dir='/path/to/watch',\n        process_func=process_file,\n        patterns=['*.json', '*.csv'],\n        debounce_seconds=2.0\n    )\n    \n    processor.start()\n    processor.process_existing()  # Handle files already there\n    \n    try:\n        while True:\n            asyncio.run(asyncio.sleep(1))\n    except KeyboardInterrupt:\n        processor.stop()\n```\n\nFeatures:\n1. Pattern-based file filtering\n2. Debouncing to avoid duplicate processing\n3. Waits for files to be fully written before processing\n4. Queue-based processing with worker thread\n5. Can process existing files on startup\n6. Graceful shutdown"}
{"instruction": "Create a caching decorator that supports both sync and async functions with TTL", "output": "Here's a universal caching decorator with TTL support:\n\n```python\nimport asyncio\nimport functools\nimport time\nimport hashlib\nimport json\nfrom typing import Any, Callable, Optional, Union\nfrom collections import OrderedDict\nimport threading\n\nclass Cache:\n    def __init__(self, maxsize: int = 1000):\n        self._cache: OrderedDict = OrderedDict()\n        self._maxsize = maxsize\n        self._lock = threading.Lock()\n    \n    def get(self, key: str) -> tuple[bool, Any]:\n        with self._lock:\n            if key not in self._cache:\n                return False, None\n            \n            value, expiry = self._cache[key]\n            if expiry and time.time() > expiry:\n                del self._cache[key]\n                return False, None\n            \n            # Move to end (LRU)\n            self._cache.move_to_end(key)\n            return True, value\n    \n    def set(self, key: str, value: Any, ttl: Optional[float] = None):\n        with self._lock:\n            expiry = time.time() + ttl if ttl else None\n            \n            if key in self._cache:\n                del self._cache[key]\n            \n            self._cache[key] = (value, expiry)\n            \n            # Evict oldest if over maxsize\n            while len(self._cache) > self._maxsize:\n                self._cache.popitem(last=False)\n    \n    def delete(self, key: str):\n        with self._lock:\n            self._cache.pop(key, None)\n    \n    def clear(self):\n        with self._lock:\n            self._cache.clear()\n\n# Global cache instance\n_default_cache = Cache()\n\ndef _make_key(func: Callable, args: tuple, kwargs: dict) -> str:\n    \"\"\"Create a hashable cache key from function and arguments.\"\"\"\n    key_parts = [\n        func.__module__,\n        func.__qualname__,\n        str(args),\n        str(sorted(kwargs.items()))\n    ]\n    key_string = '|'.join(key_parts)\n    return hashlib.md5(key_string.encode()).hexdigest()\n\ndef cached(\n    ttl: Optional[float] = None,\n    maxsize: int = 1000,\n    key_func: Optional[Callable] = None,\n    cache: Optional[Cache] = None\n) -> Callable:\n    \"\"\"Universal caching decorator for sync and async functions.\n    \n    Args:\n        ttl: Time to live in seconds. None = no expiry.\n        maxsize: Maximum cache entries.\n        key_func: Custom function to generate cache key.\n        cache: Custom Cache instance. Uses global cache if None.\n    \"\"\"\n    _cache = cache or _default_cache\n    \n    def decorator(func: Callable) -> Callable:\n        is_async = asyncio.iscoroutinefunction(func)\n        \n        if is_async:\n            @functools.wraps(func)\n            async def async_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = await func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            async_wrapper.cache_clear = _cache.clear\n            async_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return async_wrapper\n        else:\n            @functools.wraps(func)\n            def sync_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            sync_wrapper.cache_clear = _cache.clear\n            sync_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return sync_wrapper\n    \n    return decorator\n\n# Usage examples:\n\n@cached(ttl=60)  # Cache for 60 seconds\ndef get_user(user_id: int) -> dict:\n    # Expensive database query\n    return {'id': user_id, 'name': 'Simon'}\n\n@cached(ttl=300, key_func=lambda url, **kw: url)  # Custom key\nasync def fetch_url(url: str, headers: dict = None) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        return response.text\n\n# Manual cache control\nget_user.cache_clear()  # Clear all\nget_user.cache_delete(42)  # Delete specific key\n```\n\nFeatures:\n1. Works with both sync and async functions\n2. TTL (time-to-live) support\n3. LRU eviction when maxsize reached\n4. Custom key functions\n5. Thread-safe\n6. Manual cache control methods\n7. Shared or isolated cache instances"}
{"instruction": "Build a simple task queue with priority support using asyncio", "output": "Here's a priority task queue implementation:\n\n```python\nimport asyncio\nimport heapq\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Coroutine, Optional\nfrom datetime import datetime\nimport uuid\nimport logging\nfrom enum import IntEnum\n\nlogger = logging.getLogger(__name__)\n\nclass Priority(IntEnum):\n    LOW = 3\n    NORMAL = 2\n    HIGH = 1\n    CRITICAL = 0\n\n@dataclass(order=True)\nclass Task:\n    priority: int\n    created_at: float = field(compare=True)\n    id: str = field(default_factory=lambda: str(uuid.uuid4()), compare=False)\n    func: Callable = field(compare=False)\n    args: tuple = field(default=(), compare=False)\n    kwargs: dict = field(default_factory=dict, compare=False)\n    result: Any = field(default=None, compare=False)\n    error: Optional[Exception] = field(default=None, compare=False)\n    completed: bool = field(default=False, compare=False)\n\nclass TaskQueue:\n    def __init__(self, num_workers: int = 3):\n        self._heap: list[Task] = []\n        self._num_workers = num_workers\n        self._workers: list[asyncio.Task] = []\n        self._running = False\n        self._task_added = asyncio.Event()\n        self._results: dict[str, Task] = {}\n        self._lock = asyncio.Lock()\n    \n    async def add(\n        self,\n        func: Callable[..., Coroutine],\n        *args,\n        priority: Priority = Priority.NORMAL,\n        **kwargs\n    ) -> str:\n        \"\"\"Add a task to the queue. Returns task ID.\"\"\"\n        task = Task(\n            priority=priority,\n            created_at=datetime.now().timestamp(),\n            func=func,\n            args=args,\n            kwargs=kwargs\n        )\n        \n        async with self._lock:\n            heapq.heappush(self._heap, task)\n            self._results[task.id] = task\n        \n        self._task_added.set()\n        logger.debug(f'Added task {task.id} with priority {priority.name}')\n        return task.id\n    \n    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Wait for task completion and return result.\"\"\"\n        start = asyncio.get_event_loop().time()\n        \n        while True:\n            if task_id in self._results:\n                task = self._results[task_id]\n                if task.completed:\n                    if task.error:\n                        raise task.error\n                    return task.result\n            \n            if timeout:\n                elapsed = asyncio.get_event_loop().time() - start\n                if elapsed >= timeout:\n                    raise TimeoutError(f'Task {task_id} did not complete in {timeout}s')\n            \n            await asyncio.sleep(0.1)\n    \n    async def _worker(self, worker_id: int):\n        logger.info(f'Worker {worker_id} started')\n        \n        while self._running:\n            # Wait for tasks\n            task = None\n            async with self._lock:\n                if self._heap:\n                    task = heapq.heappop(self._heap)\n            \n            if not task:\n                self._task_added.clear()\n                try:\n                    await asyncio.wait_for(self._task_added.wait(), timeout=1.0)\n                except asyncio.TimeoutError:\n                    pass\n                continue\n            \n            # Execute task\n            logger.debug(f'Worker {worker_id} executing task {task.id}')\n            try:\n                if asyncio.iscoroutinefunction(task.func):\n                    task.result = await task.func(*task.args, **task.kwargs)\n                else:\n                    task.result = await asyncio.to_thread(task.func, *task.args, **task.kwargs)\n            except Exception as e:\n                task.error = e\n                logger.error(f'Task {task.id} failed: {e}')\n            finally:\n                task.completed = True\n        \n        logger.info(f'Worker {worker_id} stopped')\n    \n    async def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        self._workers = [\n            asyncio.create_task(self._worker(i))\n            for i in range(self._num_workers)\n        ]\n        logger.info(f'TaskQueue started with {self._num_workers} workers')\n    \n    async def stop(self, wait_for_completion: bool = True):\n        if wait_for_completion:\n            # Wait for queue to drain\n            while self._heap:\n                await asyncio.sleep(0.1)\n        \n        self._running = False\n        self._task_added.set()  # Wake up workers\n        \n        await asyncio.gather(*self._workers, return_exceptions=True)\n        logger.info('TaskQueue stopped')\n    \n    @property\n    def pending_count(self) -> int:\n        return len(self._heap)\n    \n    def get_status(self, task_id: str) -> dict:\n        if task_id not in self._results:\n            return {'status': 'not_found'}\n        \n        task = self._results[task_id]\n        return {\n            'id': task.id,\n            'status': 'completed' if task.completed else 'pending',\n            'priority': Priority(task.priority).name,\n            'error': str(task.error) if task.error else None\n        }\n\n# Example usage:\nasync def slow_task(name: str, delay: float) -> str:\n    await asyncio.sleep(delay)\n    return f'Task {name} completed after {delay}s'\n\nasync def main():\n    queue = TaskQueue(num_workers=2)\n    await queue.start()\n    \n    # Add tasks with different priorities\n    id1 = await queue.add(slow_task, 'low', 2.0, priority=Priority.LOW)\n    id2 = await queue.add(slow_task, 'critical', 1.0, priority=Priority.CRITICAL)\n    id3 = await queue.add(slow_task, 'normal', 1.5, priority=Priority.NORMAL)\n    \n    # Critical will run first despite being added second\n    result = await queue.get_result(id2)\n    print(result)  # 'Task critical completed after 1.0s'\n    \n    await queue.stop()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nFeatures:\n1. Priority levels (CRITICAL, HIGH, NORMAL, LOW)\n2. Multiple worker support\n3. Async and sync function support\n4. Result retrieval with timeout\n5. Graceful shutdown\n6. Task status tracking\n7. Thread-safe operations"}
{"instruction": "Implement a circuit breaker pattern for external API calls", "output": "Here's a robust circuit breaker implementation:\n\n```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional, Any, Type\nimport functools\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'      # Normal operation\n    OPEN = 'open'          # Failing, reject calls\n    HALF_OPEN = 'half_open'  # Testing recovery\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    last_success_time: Optional[float] = None\n    consecutive_failures: int = 0\n    consecutive_successes: int = 0\n\nclass CircuitBreakerOpen(Exception):\n    \"\"\"Raised when circuit breaker is open.\"\"\"\n    def __init__(self, name: str, retry_after: float):\n        self.name = name\n        self.retry_after = retry_after\n        super().__init__(f'Circuit {name} is open. Retry after {retry_after:.1f}s')\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        name: str,\n        failure_threshold: int = 5,\n        success_threshold: int = 3,\n        timeout: float = 30.0,\n        exceptions: tuple[Type[Exception], ...] = (Exception,),\n        exclude: tuple[Type[Exception], ...] = ()\n    ):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.success_threshold = success_threshold\n        self.timeout = timeout\n        self.exceptions = exceptions\n        self.exclude = exclude\n        \n        self._state = CircuitState.CLOSED\n        self._stats = CircuitStats()\n        self._opened_at: Optional[float] = None\n        self._lock = asyncio.Lock()\n    \n    @property\n    def state(self) -> CircuitState:\n        if self._state == CircuitState.OPEN:\n            if time.time() - self._opened_at >= self.timeout:\n                return CircuitState.HALF_OPEN\n        return self._state\n    \n    async def _on_success(self):\n        async with self._lock:\n            self._stats.successes += 1\n            self._stats.last_success_time = time.time()\n            self._stats.consecutive_successes += 1\n            self._stats.consecutive_failures = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                if self._stats.consecutive_successes >= self.success_threshold:\n                    logger.info(f'Circuit {self.name}: HALF_OPEN -> CLOSED')\n                    self._state = CircuitState.CLOSED\n                    self._stats.consecutive_successes = 0\n    \n    async def _on_failure(self, error: Exception):\n        async with self._lock:\n            self._stats.failures += 1\n            self._stats.last_failure_time = time.time()\n            self._stats.consecutive_failures += 1\n            self._stats.consecutive_successes = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                logger.warning(f'Circuit {self.name}: HALF_OPEN -> OPEN (test failed)')\n                self._state = CircuitState.OPEN\n                self._opened_at = time.time()\n            \n            elif self._state == CircuitState.CLOSED:\n                if self._stats.consecutive_failures >= self.failure_threshold:\n                    logger.warning(f'Circuit {self.name}: CLOSED -> OPEN (threshold reached)')\n                    self._state = CircuitState.OPEN\n                    self._opened_at = time.time()\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        state = self.state  # Check with timeout logic\n        \n        if state == CircuitState.OPEN:\n            retry_after = self.timeout - (time.time() - self._opened_at)\n            raise CircuitBreakerOpen(self.name, retry_after)\n        \n        try:\n            if asyncio.iscoroutinefunction(func):\n                result = await func(*args, **kwargs)\n            else:\n                result = await asyncio.to_thread(func, *args, **kwargs)\n            \n            await self._on_success()\n            return result\n        \n        except self.exclude:\n            # These exceptions don't count as failures\n            raise\n        except self.exceptions as e:\n            await self._on_failure(e)\n            raise\n    \n    def get_stats(self) -> dict:\n        return {\n            'name': self.name,\n            'state': self.state.value,\n            'failures': self._stats.failures,\n            'successes': self._stats.successes,\n            'consecutive_failures': self._stats.consecutive_failures,\n            'consecutive_successes': self._stats.consecutive_successes\n        }\n    \n    async def reset(self):\n        async with self._lock:\n            self._state = CircuitState.CLOSED\n            self._stats = CircuitStats()\n            self._opened_at = None\n\n# Decorator version\ndef circuit_breaker(\n    name: str,\n    failure_threshold: int = 5,\n    success_threshold: int = 3,\n    timeout: float = 30.0,\n    exceptions: tuple = (Exception,)\n) -> Callable:\n    breaker = CircuitBreaker(\n        name=name,\n        failure_threshold=failure_threshold,\n        success_threshold=success_threshold,\n        timeout=timeout,\n        exceptions=exceptions\n    )\n    \n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await breaker.call(func, *args, **kwargs)\n        \n        wrapper.circuit_breaker = breaker\n        return wrapper\n    \n    return decorator\n\n# Usage examples:\n\n# As decorator\n@circuit_breaker('external_api', failure_threshold=3, timeout=60)\nasync def call_external_api(endpoint: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f'https://api.example.com/{endpoint}')\n        response.raise_for_status()\n        return response.json()\n\n# Manual usage\napi_breaker = CircuitBreaker('payment_gateway', failure_threshold=2, timeout=120)\n\nasync def process_payment(amount: float):\n    try:\n        result = await api_breaker.call(payment_api.charge, amount)\n        return result\n    except CircuitBreakerOpen as e:\n        # Handle gracefully - maybe queue for later\n        logger.warning(f'Payment service unavailable: {e}')\n        return {'status': 'queued', 'retry_after': e.retry_after}\n\n# Check stats\nprint(call_external_api.circuit_breaker.get_stats())\n```\n\nFeatures:\n1. Three states: CLOSED (normal), OPEN (failing), HALF_OPEN (testing)\n2. Configurable failure/success thresholds\n3. Automatic timeout and recovery testing\n4. Exception filtering (track specific errors)\n5. Both decorator and manual usage patterns\n6. Statistics tracking\n7. Async-safe with locks\n8. Clear error messages with retry timing"}
