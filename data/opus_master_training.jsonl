{"instruction": "Skapa ett n8n workflow som orkestrerar flera AI-agenter med Ollama RAG och specialiserade services", "output": "## AI Agent Orchestrator med Ollama RAG\n\n### Arkitektur\nWorkflowet använder en central orchestrator-agent som koordinerar specialiserade n8n workflow-services via HTTP webhooks.\n\n### Komponenter\n1. **Webhook Trigger** - Tar emot användarförfrågningar via POST\n2. **Workflow Configuration** - Centraliserad konfiguration för alla service-URLs\n3. **Orchestrator Agent** - LangChain agent med Ollama som koordinerar tools\n4. **RAG Vector Store** - In-memory vektor-databas för kontextuell kunskap\n5. **Specialized Service Tools** - HTTP Request Tools för Database, Sentiment, PDF services\n\n### Implementation\n```javascript\n// Orchestrator System Prompt\nconst systemMessage = `You are an orchestrator agent that coordinates specialized n8n workflow services.\n\nYour role is to:\n1. Analyze the user request and determine which specialized services are needed\n2. Call the appropriate service tools in the correct sequence\n3. Coordinate data flow between services\n4. Use the RAG knowledge base when you need contextual information\n5. Return the final result to the user`;\n\n// Service Tool Configuration\nconst databaseServiceTool = {\n  toolDescription: \"Calls the specialized database service workflow\",\n  method: \"POST\",\n  url: \"{{ $('Workflow Configuration').first().json.databaseServiceUrl }}\",\n  bodyParameters: {\n    query: \"{{ $fromAI('query', 'The database query to perform') }}\",\n    parameters: \"{{ $fromAI('parameters', 'Query parameters as JSON', 'json', {}) }}\"\n  }\n};\n```\n\n### Best Practices\n- Använd `retrieve-as-tool` mode för RAG vector store\n- Separera services i egna workflows för moduläritet\n- Inkludera `includeOtherFields: true` i Set-noder för att bevara data"}
{"instruction": "Bygg ett multi-source business intelligence system med AI-driven analytics i n8n", "output": "## AI-Driven Multi-Source Business Intelligence\n\n### Syfte\nSamla data från flera källor, analysera med AI och leverera insikter anpassade för olika målgrupper.\n\n### Dataflöde\n```\nMultiple Sources → Normalize → AI Analysis → Audience Adaptation → Delivery\n```\n\n### Komponenter\n1. **Data Sources** - Google Sheets, APIs, Databases\n2. **Data Normalization** - Set-noder som standardiserar format\n3. **AI Analysis Agent** - LangChain agent för intelligent analys\n4. **Audience Adaptation** - Switch-nod som routar till rätt format\n5. **Delivery Channels** - Slack, Email, Dashboard\n\n### Implementation\n```javascript\n// Normalize data from different sources\nconst normalizeData = {\n  assignments: [\n    { name: \"dataType\", value: \"={{ $json.type || 'unknown' }}\" },\n    { name: \"value\", value: \"={{ $json.amount || $json.value || 0 }}\" },\n    { name: \"timestamp\", value: \"={{ $json.date || $now.toISO() }}\" },\n    { name: \"source\", value: \"={{ $json.source || 'manual' }}\" }\n  ]\n};\n\n// AI Analysis System Prompt\nconst analysisPrompt = `Analyze this business data and provide:\n1. Key trends and patterns\n2. Anomalies that need attention\n3. Actionable recommendations\n4. Risk assessment\n\nBe data-driven and specific.`;\n```\n\n### Tips\n- Använd Merge-nod med `combineAll` för att samla alla datakällor\n- Implementera caching för frekventa API-anrop\n- Logga alla analyser för audit trail"}
{"instruction": "Skapa ett automatiskt dokumentationssystem som genererar docs från GitHub, Slack, Jira och Linear events", "output": "## Auto-Generated Internal Documentation System\n\n### Arkitektur\nWorkflowet lyssnar på events från utvecklingsverktyg och genererar automatiskt strukturerad dokumentation.\n\n### Event Sources\n1. **GitHub Webhook** - Push, PR, Issues\n2. **Slack Webhook** - Viktiga diskussioner\n3. **Jira Webhook** - Issue updates\n4. **Linear Webhook** - Task changes\n\n### AI Agents Pipeline\n```\nEvent → Normalize → Context Analyzer → Route by Type → Specialized Writer → Vector Store → Output\n```\n\n### Implementation\n```javascript\n// Normalize Event Data\nconst normalizeEvent = {\n  eventType: \"={{ $json.body.action || $json.body.event?.type || 'unknown' }}\",\n  eventSource: \"={{ $json.headers['x-github-event'] ? 'github' : $json.headers['x-slack-signature'] ? 'slack' : 'unknown' }}\",\n  eventTitle: \"={{ $json.body.pull_request?.title || $json.body.issue?.title || 'No title' }}\",\n  eventDescription: \"={{ $json.body.pull_request?.body || $json.body.issue?.body || '' }}\",\n  eventAuthor: \"={{ $json.body.sender?.login || $json.body.user?.name || 'Unknown' }}\"\n};\n\n// Context Analyzer Agent\nconst analyzerPrompt = `Analyze this development event and determine:\n1. What changed or happened?\n2. Why did it happen? (extract intent)\n3. What systems/components are affected?\n4. What type of documentation should be updated?\n\nDocumentation types:\n- architecture: System design changes\n- api: API endpoints, formats\n- changelog: User-facing changes\n- lessons: Problems and solutions found`;\n\n// Specialized Writers\nconst architectureWriter = {\n  systemMessage: `You are an architecture documentation writer.\n  - Describe system components and relationships\n  - Explain architectural decisions\n  - Use Mermaid diagrams when relevant\n  - Maintain continuity with existing docs`\n};\n```\n\n### Output Destinations\n- **Notion** - Structured database pages\n- **GitHub Docs** - Markdown files in repo\n- **Confluence** - Wiki pages\n- **Qdrant** - Vector storage for RAG"}
{"instruction": "Implementera ett intelligent RAG-system med Ollama som har multiple tools och cross-document intelligence", "output": "## Intelligent RAG Ollama Multi-Function System\n\n### Funktioner\n- RAG-baserad Q&A med lokal Ollama\n- Real-time API integration\n- Email/Slack actions\n- Cross-document relationship analysis\n\n### Komponenter\n```\nQuery → Config → RAG Agent → Tools → Actions → Response\n         ↓\n    Auto-Update → Fetch → Clean → Vector Store → Intelligence\n```\n\n### Implementation\n```javascript\n// Workflow Configuration\nconst config = {\n  ollamaModel: \"llama3.2\",\n  embeddingModel: \"nomic-embed-text\",\n  chunkSize: 1000,\n  chunkOverlap: 200\n};\n\n// RAG Orchestrator Agent\nconst ragPrompt = `Du är en intelligent RAG-assistent på svenska.\n\nTillgängliga verktyg:\n1. knowledge_base_search: Sök i kunskapsbasen\n2. fetch_live_data: Hämta realtidsdata\n3. Gmail/Slack: Skicka meddelanden\n4. schedule_reminder: Schemalägg uppgifter\n\nAnvänd rätt verktyg, kombinera data, förklara källor.`;\n\n// Cross-Document Intelligence (Code Node)\nfunction analyzeRelationships(documents) {\n  const extractKeyTerms = (text) => {\n    return text.toLowerCase()\n      .replace(/[^\\w\\s]/g, ' ')\n      .split(/\\s+/)\n      .filter(word => word.length > 4);\n  };\n  \n  const calculateSimilarity = (doc1, doc2) => {\n    const terms1 = new Set(extractKeyTerms(doc1.content));\n    const terms2 = new Set(extractKeyTerms(doc2.content));\n    const intersection = [...terms1].filter(x => terms2.has(x));\n    const union = new Set([...terms1, ...terms2]);\n    return intersection.length / union.size; // Jaccard similarity\n  };\n  \n  // Build knowledge graph\n  const knowledgeGraph = { nodes: [], edges: [] };\n  // ... implementation\n  return knowledgeGraph;\n}\n```\n\n### Swedish Document Handling\n```javascript\n// Normalize Swedish characters\ncontent = content\n  .replace(/Ã¥/g, 'å')\n  .replace(/Ã¤/g, 'ä')\n  .replace(/Ã¶/g, 'ö');\n```"}
{"instruction": "Bygg ett Second Brain cognitive engine med multi-source intelligence processing i n8n", "output": "## Second Brain Cognitive Engine\n\n### Koncept\nEtt personligt AI-system som samlar, analyserar och organiserar information från alla dina digitala källor.\n\n### Data Sources\n1. **Gmail** - Email trigger var 5:e minut\n2. **Google Drive** - Document changes\n3. **Google Calendar** - Event updates\n4. **Slack** - Important messages\n\n### AI Agent Architecture\n```\n┌─────────────────────────────────────────────┐\n│           ORCHESTRATOR AGENT                │\n├─────────────────────────────────────────────┤\n│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n│  │Classifier│ │Summarizer│ │Task     │       │\n│  │Agent    │ │Agent    │ │Extractor│       │\n│  └─────────┘ └─────────┘ └─────────┘       │\n│  ┌─────────┐ ┌─────────┐                   │\n│  │Priority │ │Context  │                   │\n│  │Planner  │ │Finder   │                   │\n│  └─────────┘ └─────────┘                   │\n└─────────────────────────────────────────────┘\n```\n\n### Implementation\n```javascript\n// Normalize all sources to common format\nconst normalizeEmail = {\n  sourceType: \"email\",\n  content: \"={{ $json.textPlain || $json.textHtml }}\",\n  title: \"={{ $json.subject }}\",\n  timestamp: \"={{ $json.date }}\",\n  sender: \"={{ $json.from.address }}\"\n};\n\n// Classifier Agent Tool\nconst classifierPrompt = `Categorize into ONE category:\n- economy (financial matters)\n- health (medical, fitness)\n- relationships (personal)\n- projects (work initiatives)\n- tasks (actionable items)\n- ideas (creative thoughts)\n- knowledge (learning)\nReturn ONLY the category name.`;\n\n// Task Extractor with Output Parser\nconst taskSchema = {\n  type: \"object\",\n  properties: {\n    tasks: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          description: { type: \"string\" },\n          deadline: { type: \"string\" },\n          priority: { type: \"string\" },\n          context: { type: \"string\" }\n        }\n      }\n    }\n  }\n};\n\n// Priority Planner - 3-day action plan\nconst plannerPrompt = `Create a 3-day action plan.\nConsider:\n- Deadlines and time sensitivity\n- Impact and importance\n- Dependencies between tasks\n- Realistic capacity (3-5 tasks/day)`;\n```\n\n### Output Channels\n- **Slack** - Daily summary with mrkdwn formatting\n- **Notion** - Structured archive database\n- **Vector Store** - Long-term memory for RAG"}
{"instruction": "Skapa ett automatiskt fakturapåminnelse-system med n8n", "output": "## Automated Invoice Payment Reminder System\n\n### Funktionalitet\nAutomatiska påminnelser vid T-3, förfallodatum och +7 dagar.\n\n### Workflow\n```\nDaily Check → Get Invoices → Calculate Stage → Filter → Prepare Email → Send → Log\n```\n\n### Implementation\n```javascript\n// Calculate Reminder Stage (Code Node)\nconst dueDate = new Date($input.item.json.dueDate);\nconst isPaid = $input.item.json.isPaid === 'TRUE';\nconst lastReminderSent = $input.item.json.lastReminderSent || '';\nconst today = new Date();\n\nconst daysUntilDue = Math.floor((dueDate - today) / (1000 * 60 * 60 * 24));\n\nlet reminderStage = null;\nlet shouldSendReminder = false;\n\nif (!isPaid) {\n  if (daysUntilDue === 3 && lastReminderSent !== 'T-3') {\n    reminderStage = 'T-3';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === 0 && lastReminderSent !== 'DueDate') {\n    reminderStage = 'DueDate';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === -7 && lastReminderSent !== 'Overdue+7') {\n    reminderStage = 'Overdue+7';\n    shouldSendReminder = true;\n  }\n}\n\nreturn { ...($input.item.json), daysUntilDue, reminderStage, shouldSendReminder };\n\n// Dynamic Email Subject\nconst emailSubject = reminderStage === 'T-3' \n  ? 'Påminnelse: Faktura förfaller om 3 dagar'\n  : reminderStage === 'DueDate'\n  ? 'Påminnelse: Faktura förfaller idag'\n  : 'Viktig påminnelse: Förfallen faktura (7 dagar)';\n```\n\n### Email Template (HTML)\n```html\n<div style=\"background-color: #f5f5f5; padding: 15px;\">\n  <strong>Fakturadetaljer:</strong><br>\n  Fakturanummer: {{ $json.invoiceNumber }}<br>\n  Belopp: {{ $json.amount }} SEK<br>\n  Förfallodatum: {{ $json.dueDate }}\n</div>\n<a href=\"{{ paymentLink }}\" style=\"background-color: #4CAF50; color: white; padding: 12px 24px;\">Betala nu</a>\n```\n\n### Google Sheets Integration\n- Läs fakturalista med `returnFirstMatch: false`\n- Uppdatera `lastReminderSent` och `lastReminderDate` efter skickat"}
{"instruction": "Hur strukturerar jag ett n8n workflow med AI-agenter och verktyg?", "output": "## n8n AI Agent Workflow Structure\n\n### Grundläggande Pattern\n```\nTrigger → Configuration → Agent → Tools → Output\n                           ↓\n                    Language Model\n                           ↓\n                      Memory/RAG\n```\n\n### Best Practices\n\n#### 1. Workflow Configuration Node\nAnvänd alltid en Set-nod i början för konfiguration:\n```javascript\n{\n  assignments: [\n    { name: \"apiUrl\", value: \"<PLACEHOLDER>\", type: \"string\" },\n    { name: \"modelName\", value: \"gpt-4.1-mini\", type: \"string\" },\n    { name: \"maxTokens\", value: 2048, type: \"number\" }\n  ],\n  includeOtherFields: true\n}\n```\n\n#### 2. Agent Node Setup\n```javascript\n// Använd promptType: \"define\" för full kontroll\n{\n  promptType: \"define\",\n  text: \"={{ $json.userInput }}\",\n  options: {\n    systemMessage: \"Du är en hjälpsam assistent...\"\n  }\n}\n```\n\n#### 3. Tool Connections\nKoppla verktyg via `ai_tool` connection type:\n- Vector Store som tool → `mode: \"retrieve-as-tool\"`\n- HTTP Request Tool → `toolDescription` är kritisk\n- Code Tool → Returnera alltid JSON.stringify()\n\n#### 4. Output Parsers\nFör strukturerad output:\n```javascript\n// Structured Output Parser\n{\n  schemaType: \"manual\",\n  inputSchema: JSON.stringify(yourSchema),\n  autoFix: true  // LLM fixar formatfel\n}\n```\n\n#### 5. Memory\n```javascript\n// Buffer Window Memory\n{\n  sessionKey: \"={{ $json.sessionId || 'default' }}\",\n  contextWindowLength: 10  // Antal meddelanden att minnas\n}\n```\n\n### Vanliga Misstag\n1. Glömmer `includeOtherFields: true` → tappar data\n2. Fel `typeVersion` → nod fungerar inte\n3. Saknar `toolDescription` → agent kan inte använda tool\n4. Hårdkodar värden istället för expressions"}
{"instruction": "Hur implementerar jag RAG med Vector Store i n8n?", "output": "## RAG Implementation i n8n\n\n### Två Modes\n1. **Insert Mode** - Lägg till dokument i vector store\n2. **Retrieve-as-Tool Mode** - Sök som agent tool\n\n### Insert Pipeline\n```\nData Source → Document Loader → Text Splitter → Embeddings → Vector Store (insert)\n```\n\n### Implementation\n```javascript\n// 1. Document Loader\n{\n  textSplittingMode: \"custom\",\n  jsonMode: \"expressionData\",\n  jsonData: \"={{ $json }}\"\n}\n\n// 2. Recursive Text Splitter\n{\n  chunkSize: 1000,\n  chunkOverlap: 200  // 20% overlap är standard\n}\n\n// 3. Embeddings (Ollama eller OpenAI)\n// Ollama:\n{ model: \"nomic-embed-text\" }\n// OpenAI:\n{ model: \"text-embedding-3-small\" }\n\n// 4. Vector Store - Insert\n{\n  mode: \"insert\",\n  memoryKey: \"my_knowledge_base\"\n}\n```\n\n### Retrieve som Tool\n```javascript\n// Vector Store - Retrieve as Tool\n{\n  mode: \"retrieve-as-tool\",\n  toolDescription: \"Sök i kunskapsbasen för att hitta relevant information\",\n  memoryKey: \"my_knowledge_base\",\n  topK: 5  // Antal resultat att returnera\n}\n```\n\n### Qdrant Integration (Persistent)\n```javascript\n// Qdrant Vector Store\n{\n  qdrantCollection: {\n    __rl: true,\n    mode: \"id\",\n    value: \"={{ $('Config').first().json.collectionName }}\"\n  }\n}\n```\n\n### Tips\n- Använd `In-Memory` för utveckling, `Qdrant` för produktion\n- `topK: 5` är bra default, öka vid behov\n- `toolDescription` måste vara tydlig för att agent ska använda rätt"}
{"instruction": "Skapa ett multipart memory ingestion system som hanterar audio, bild, PDF, URL och text med AI-processning och vector storage", "output": "## Multipart Memory Ingestion System\n\n### Arkitektur\nWebhook-baserat system som tar emot olika innehållstyper, extraherar text och lagrar i Qdrant vector store.\n\n### Content Type Routing\n```\nWebhook → Parse Multipart → Switch by Type → Process → Summarize → Keywords → Qdrant\n                                │\n                    ┌───────────┼───────────┐\n                    ▼           ▼           ▼\n                  Audio       Image        PDF\n                (Whisper)    (OCR)      (Extract)\n```\n\n### Implementation\n```javascript\n// Parse Multipart Data (Code Node)\nconst parsedData = {\n  type: body.type || null,\n  title: body.title || null,\n  text: body.text || null,\n  source: body.source || null,\n  timestamp: body.timestamp || new Date().toISOString(),\n  metadata: body.metadata || {},\n  user_id: body.user_id || null\n};\n\n// Handle binary file if present\nif (item.binary && Object.keys(item.binary).length > 0) {\n  binaryData = item.binary;\n  parsedData.file = Object.keys(item.binary)[0];\n}\n\n// Extract Keywords (Code Node)\nfunction extractKeywords(text) {\n  const stopWords = new Set(['the', 'a', 'an', 'and', 'or', ...]);\n  const words = text.toLowerCase()\n    .replace(/[^a-z0-9\\s]/g, ' ')\n    .split(/\\s+/)\n    .filter(word => word.length > 3 && !stopWords.has(word));\n  \n  const wordCount = {};\n  words.forEach(word => wordCount[word] = (wordCount[word] || 0) + 1);\n  \n  return Object.entries(wordCount)\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, 10)\n    .map(entry => entry[0]);\n}\n```\n\n### Content Processors\n- **Audio**: OpenAI Whisper transcription\n- **Image**: OCR via extractFromFile\n- **URL**: HTTP Request med responseFormat: text\n- **PDF**: extractFromFile med operation: pdf\n- **Text**: Direkt genomströmning\n\n### Vector Storage\n```javascript\n// Qdrant Insert\n{\n  mode: \"insert\",\n  qdrantCollection: \"{{ $('Config').first().json.qdrantCollection }}\"\n}\n\n// Search endpoint\n{\n  mode: \"load\",\n  prompt: \"{{ $json.query }}\",\n  topK: \"{{ $json.topK }}\"\n}\n```"}
{"instruction": "Implementera bidirectional data sync mellan Linear, Gmail, Calendar, Notion och Slack med conflict resolution", "output": "## Multi-Platform Bidirectional Data Sync\n\n### Funktionalitet\n- Synkar data mellan 5 plattformar var 15:e minut\n- Normaliserar data till gemensamt format\n- Hanterar konflikter med configuerbar strategi\n- Sparar sync state för incremental updates\n\n### Sync Flow\n```\nSchedule → Server Check → Load State → Fetch All → Normalize → Detect Changes → Resolve Conflicts → Route → Update → Save State\n```\n\n### Data Normalization (Code Node)\n```javascript\n// Detect platform from data structure\nlet platform, id, title, description, timestamp, status, metadata;\n\n// Linear Issues\nif (json.identifier || json.team) {\n  platform = 'linear';\n  id = json.id || json.identifier;\n  title = json.title || '';\n  status = json.state?.name || '';\n  metadata = { assignee: json.assignee?.name, priority: json.priority };\n}\n// Gmail Messages  \nelse if (json.threadId || json.labelIds) {\n  platform = 'gmail';\n  id = json.id || json.threadId;\n  title = json.subject || '';\n  status = json.labelIds?.includes('UNREAD') ? 'unread' : 'read';\n}\n// Google Calendar\nelse if (json.start?.dateTime) {\n  platform = 'calendar';\n  title = json.summary || '';\n  metadata = { start: json.start, end: json.end, attendees: json.attendees };\n}\n// Notion\nelse if (json.properties || json.object === 'page') {\n  platform = 'notion';\n  title = json.properties?.Name?.title?.[0]?.plain_text || '';\n}\n// Slack\nelse if (json.ts || json.channel) {\n  platform = 'slack';\n  title = json.text?.substring(0, 100) || '';\n}\n```\n\n### Conflict Resolution Strategies\n```javascript\nconst strategy = config.conflictResolutionStrategy; // 'latest_wins' | 'merge' | 'manual_review'\n\nif (strategy === 'latest_wins') {\n  resolved.data = sourceTimestamp > targetTimestamp ? sourceData : targetData;\n} else if (strategy === 'merge') {\n  resolved.data = { ...targetData };\n  for (const [key, value] of Object.entries(sourceData)) {\n    if (!targetData[key]) resolved.data[key] = value;\n    else if (JSON.stringify(targetData[key]) !== JSON.stringify(value)) {\n      resolved.needsManualReview = true;\n      resolved.conflictDetails.push({ field: key, sourceValue: value, targetValue: targetData[key] });\n    }\n  }\n}\n```\n\n### State Management\n```javascript\n// Save Sync State\n{\n  timestamp: $now.toISO(),\n  platforms: { linear, gmail, calendar, notion, slack },\n  lastSync: $now.toISO(),\n  syncStatus: 'completed'\n}\n```"}
{"instruction": "Bygg ett AI-driven newsletter system med Google Drive trigger, AI-generering, approval workflow och automated reminders", "output": "## AI Newsletter Generation System\n\n### Workflow Overview\n1. **Trigger**: Nya filer i Google Drive-mapp\n2. **Process**: Extrahera text, generera med AI\n3. **Store**: Spara draft i PostgreSQL\n4. **Approve**: Webhook för godkännande\n5. **Send**: Skicka via Gmail\n6. **Remind**: Veckovis påminnelse om tomt innehåll\n\n### Implementation\n```javascript\n// Workflow Configuration\n{\n  distributionList: \"team@company.com,all-hands@company.com\",\n  reminderRecipients: \"content-team@company.com\",\n  databaseTable: \"newsletters\"\n}\n\n// Check If Already Processed\n\"SELECT COUNT(*) as count FROM processed_files WHERE file_id = $1\"\n\n// Generate Newsletter with AI\n{\n  model: \"gpt-4o\",\n  messages: [\n    {\n      role: \"system\",\n      content: \"You are a professional newsletter writer. Create well-formatted HTML...\"\n    },\n    { role: \"user\", content: \"={{ $json.text }}\" }\n  ],\n  temperature: 0.7\n}\n\n// Save Newsletter Draft\nINSERT INTO newsletters (id, html_content, status, file_id, created_at)\nVALUES ($1, $2, 'pending', $3, NOW())\n```\n\n### Approval Flow\n```\nPOST /webhook/newsletter-approval\n  → Get Newsletter WHERE status='pending'\n  → Send via Gmail\n  → UPDATE status='sent', sent_at=NOW()\n  → Respond { success: true }\n```\n\n### Weekly Reminder\n```javascript\n// Check Content Last Week\n\"SELECT COUNT(*) FROM processed_files WHERE processed_at >= NOW() - INTERVAL '7 days'\"\n\n// If count = 0, send reminder\nif ($json.count === 0) {\n  sendEmail({\n    to: config.reminderRecipients,\n    subject: \"Reminder: No Newsletter Content This Week\"\n  });\n}\n```\n\n### Database Schema\n```sql\nCREATE TABLE newsletters (\n  id TEXT PRIMARY KEY,\n  html_content TEXT,\n  status TEXT DEFAULT 'pending',\n  file_id TEXT,\n  created_at TIMESTAMP,\n  sent_at TIMESTAMP\n);\n\nCREATE TABLE processed_files (\n  file_id TEXT PRIMARY KEY,\n  file_name TEXT,\n  processed_at TIMESTAMP DEFAULT NOW()\n);\n```"}
{"instruction": "Hur hanterar jag errors och edge cases i n8n workflows?", "output": "## Error Handling i n8n Workflows\n\n### 1. Server Health Check Pattern\n```javascript\n// Kolla server status innan main flow\n{\n  url: \"{{ $('Config').json.statusCheckUrl }}\",\n  options: { timeout: 5000 }\n}\n\n// If-nod: statusCode === 200\n// True → Continue\n// False → Send Error Alert\n```\n\n### 2. Idempotency Pattern\n```sql\n-- Kolla om redan processad\nSELECT COUNT(*) as count FROM processed_items WHERE id = $1\n\n-- If count > 0 → Skip\n-- Else → Process och INSERT\n```\n\n### 3. HTTP Request Error Handling\n```javascript\n{\n  options: {\n    response: {\n      response: {\n        neverError: true  // Returnera error som data istället för att faila\n      }\n    }\n  }\n}\n```\n\n### 4. Fallback Values\n```javascript\n// Säkra expressions med fallbacks\n$json.value || $json.defaultValue || 'fallback'\n$json.nested?.deeply?.value ?? 'default'\n```\n\n### 5. Error Webhook Alerts\n```javascript\n// Send Error Alert nod\n{\n  method: \"POST\",\n  url: \"{{ config.alertWebhookUrl }}\",\n  body: {\n    timestamp: \"{{ $now.toISO() }}\",\n    error: \"{{ $json.error }}\",\n    workflow: \"Workflow Name\",\n    node: \"{{ $node.name }}\"\n  }\n}\n```\n\n### 6. Retry Logic\n```javascript\n// Använd n8n's inbyggda retry\n{\n  retryOnFail: true,\n  maxTries: 3,\n  waitBetweenTries: 1000\n}\n```\n\n### 7. Data Validation\n```javascript\n// Code Node validation\nconst required = ['email', 'name', 'amount'];\nconst missing = required.filter(field => !$json[field]);\n\nif (missing.length > 0) {\n  throw new Error(`Missing fields: ${missing.join(', ')}`);\n}\n```\n\n### 8. Graceful Degradation\n```javascript\n// Switch med fallback output\n{\n  rules: [...],\n  options: {\n    fallbackOutput: 0  // Default route om ingen regel matchar\n  }\n}\n```"}
{"instruction": "Hur optimerar jag n8n workflows för prestanda?", "output": "## n8n Performance Optimization\n\n### 1. Batch Processing\n```javascript\n// Använd SplitInBatches för stora datasets\n{\n  batchSize: 100,\n  options: { reset: false }\n}\n\n// Eller Loop Over Items\nfor (const item of $input.all()) {\n  // Process\n}\n```\n\n### 2. Parallel Execution\n```javascript\n// Merge med combineByPosition för parallella branches\n{\n  mode: \"combine\",\n  combineBy: \"combineByPosition\",\n  numberInputs: 5\n}\n```\n\n### 3. Caching\n```javascript\n// Spara resultat i workflow static data\nconst cache = $getWorkflowStaticData('global');\nconst cacheKey = `api_${$json.id}`;\n\nif (cache[cacheKey] && Date.now() - cache[cacheKey].timestamp < 3600000) {\n  return cache[cacheKey].data;\n}\n\n// Fetch and cache\nconst result = await fetch(...);\ncache[cacheKey] = { data: result, timestamp: Date.now() };\n```\n\n### 4. Incremental Sync\n```javascript\n// Spara last sync timestamp\nconst lastSync = $getWorkflowStaticData('global').lastSync || '1970-01-01';\n\n// Query endast nya items\n{\n  filter: `modified_at > '${lastSync}'`\n}\n\n// Uppdatera efter sync\n$getWorkflowStaticData('global').lastSync = $now.toISO();\n```\n\n### 5. Minimize API Calls\n```javascript\n// Batch API requests\nconst ids = items.map(i => i.json.id);\nconst response = await fetch(`/api/items?ids=${ids.join(',')}`);\n\n// Istället för\nfor (const item of items) {\n  await fetch(`/api/items/${item.json.id}`); // Långsamt!\n}\n```\n\n### 6. Response Size\n```javascript\n// Begränsa returnerade fält\n{\n  options: {\n    fields: \"id,name,status\"  // Bara nödvändiga fält\n  }\n}\n```\n\n### 7. Timeout Configuration\n```javascript\n// Sätt rimliga timeouts\n{\n  options: {\n    timeout: 30000  // 30 sekunder\n  }\n}\n```\n\n### 8. Memory Management\n```javascript\n// Rensa stora objekt efter användning\ndelete $json._original;\ndelete $json.largePayload;\n\n// Använd streaming för stora filer\n{ responseFormat: \"file\" }\n```"}
{"instruction": "Skapa webhooks med authentication och response handling i n8n", "output": "## n8n Webhook Patterns\n\n### 1. Basic Authenticated Webhook\n```javascript\n{\n  httpMethod: \"POST\",\n  path: \"my-webhook\",\n  authentication: \"headerAuth\",  // Kräver credentials\n  responseMode: \"responseNode\",  // Manuell response\n  options: {}\n}\n```\n\n### 2. Response Modes\n```javascript\n// responseMode options:\n// - \"onReceived\": Svara direkt med 200\n// - \"lastNode\": Svara efter sista noden\n// - \"responseNode\": Använd Respond to Webhook nod\n\n// Respond to Webhook\n{\n  respondWith: \"json\",\n  responseBody: \"={{ $json }}\",\n  options: {\n    responseCode: 200,\n    responseHeaders: {\n      \"X-Custom-Header\": \"value\"\n    }\n  }\n}\n```\n\n### 3. Request Parsing\n```javascript\n// Webhook data locations\n$json.body       // POST body\n$json.query      // Query parameters\n$json.headers    // Request headers\n$json.params     // URL parameters\n\n// Multipart form data\n$json.body.field_name\n$binary.file     // Uploaded file\n```\n\n### 4. Validation Pattern\n```javascript\n// Code Node efter webhook\nconst required = ['action', 'data'];\nconst body = $json.body;\n\nfor (const field of required) {\n  if (!body[field]) {\n    return {\n      json: {\n        error: `Missing required field: ${field}`,\n        statusCode: 400\n      }\n    };\n  }\n}\n\nreturn { json: { ...body, validated: true } };\n```\n\n### 5. Webhook URL Format\n```\nProduction: https://your-n8n.com/webhook/path-name\nTest: https://your-n8n.com/webhook-test/path-name\n```\n\n### 6. Error Response\n```javascript\n// Respond to Webhook för errors\n{\n  respondWith: \"json\",\n  responseBody: {\n    success: false,\n    error: \"{{ $json.errorMessage }}\",\n    code: \"{{ $json.errorCode }}\"\n  },\n  options: {\n    responseCode: 400\n  }\n}\n```\n\n### 7. Async Processing\n```javascript\n// Svara direkt, processa i bakgrunden\n// 1. Webhook med responseMode: \"onReceived\"\n// 2. Async processing continues\n// 3. Callback webhook när klart\n```"}
{"instruction": "Create a rate limiter middleware for FastAPI that limits requests per IP to 100 per minute", "output": "Here's a production-ready rate limiter middleware:\n\n```python\nfrom fastapi import FastAPI, Request, HTTPException\nfrom collections import defaultdict\nimport time\nimport asyncio\n\nclass RateLimiter:\n    def __init__(self, requests_per_minute: int = 100):\n        self.requests_per_minute = requests_per_minute\n        self.requests = defaultdict(list)\n        self._cleanup_task = None\n    \n    async def start_cleanup(self):\n        while True:\n            await asyncio.sleep(60)\n            cutoff = time.time() - 60\n            for ip in list(self.requests.keys()):\n                self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n                if not self.requests[ip]:\n                    del self.requests[ip]\n    \n    def is_allowed(self, ip: str) -> bool:\n        now = time.time()\n        cutoff = now - 60\n        self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n        \n        if len(self.requests[ip]) >= self.requests_per_minute:\n            return False\n        \n        self.requests[ip].append(now)\n        return True\n\nrate_limiter = RateLimiter(requests_per_minute=100)\n\n@app.middleware('http')\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n    \n    if not rate_limiter.is_allowed(client_ip):\n        raise HTTPException(\n            status_code=429,\n            detail='Rate limit exceeded. Max 100 requests per minute.'\n        )\n    \n    return await call_next(request)\n```\n\nKey features:\n1. In-memory storage with automatic cleanup\n2. Sliding window algorithm (more accurate than fixed windows)\n3. Per-IP tracking\n4. Async-compatible cleanup task\n5. Returns 429 with clear error message"}
{"instruction": "Write a Python decorator that retries failed async functions with exponential backoff", "output": "Here's a robust retry decorator with exponential backoff:\n\n```python\nimport asyncio\nimport functools\nimport logging\nfrom typing import Type, Tuple, Callable, Any\n\nlogger = logging.getLogger(__name__)\n\ndef async_retry(\n    max_attempts: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n) -> Callable:\n    \"\"\"Retry decorator with exponential backoff for async functions.\n    \n    Args:\n        max_attempts: Maximum number of retry attempts\n        base_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay cap\n        exponential_base: Base for exponential calculation\n        exceptions: Tuple of exceptions to catch and retry\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> Any:\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_attempts - 1:\n                        logger.error(f'{func.__name__} failed after {max_attempts} attempts: {e}')\n                        raise\n                    \n                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n                    logger.warning(f'{func.__name__} attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s')\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage example:\n@async_retry(max_attempts=5, base_delay=0.5, exceptions=(ConnectionError, TimeoutError))\nasync def fetch_data(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, timeout=10)\n        return response.json()\n```\n\nFeatures:\n1. Configurable retry count and delays\n2. Exponential backoff with max cap\n3. Selective exception handling\n4. Logging for debugging\n5. Preserves function metadata with functools.wraps"}
{"instruction": "Implement a simple pub/sub system in Python using asyncio", "output": "Here's a lightweight async pub/sub implementation:\n\n```python\nimport asyncio\nfrom collections import defaultdict\nfrom typing import Callable, Any, Dict, Set\nfrom dataclasses import dataclass, field\nimport uuid\n\n@dataclass\nclass Subscription:\n    id: str\n    topic: str\n    callback: Callable\n\nclass PubSub:\n    def __init__(self):\n        self._subscribers: Dict[str, Set[Subscription]] = defaultdict(set)\n        self._lock = asyncio.Lock()\n    \n    async def subscribe(self, topic: str, callback: Callable) -> str:\n        \"\"\"Subscribe to a topic. Returns subscription ID for unsubscribing.\"\"\"\n        sub_id = str(uuid.uuid4())\n        subscription = Subscription(id=sub_id, topic=topic, callback=callback)\n        \n        async with self._lock:\n            self._subscribers[topic].add(subscription)\n        \n        return sub_id\n    \n    async def unsubscribe(self, sub_id: str) -> bool:\n        \"\"\"Unsubscribe using subscription ID.\"\"\"\n        async with self._lock:\n            for topic, subs in self._subscribers.items():\n                for sub in subs:\n                    if sub.id == sub_id:\n                        subs.remove(sub)\n                        return True\n        return False\n    \n    async def publish(self, topic: str, message: Any) -> int:\n        \"\"\"Publish message to topic. Returns number of subscribers notified.\"\"\"\n        async with self._lock:\n            subscribers = list(self._subscribers.get(topic, []))\n        \n        if not subscribers:\n            return 0\n        \n        tasks = []\n        for sub in subscribers:\n            if asyncio.iscoroutinefunction(sub.callback):\n                tasks.append(sub.callback(message))\n            else:\n                tasks.append(asyncio.to_thread(sub.callback, message))\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n        return len(subscribers)\n    \n    async def publish_pattern(self, pattern: str, message: Any) -> int:\n        \"\"\"Publish to all topics matching pattern (simple * wildcard).\"\"\"\n        import fnmatch\n        count = 0\n        \n        async with self._lock:\n            matching_topics = [t for t in self._subscribers.keys() if fnmatch.fnmatch(t, pattern)]\n        \n        for topic in matching_topics:\n            count += await self.publish(topic, message)\n        \n        return count\n\n# Usage:\npubsub = PubSub()\n\nasync def handler(msg):\n    print(f'Received: {msg}')\n\nasync def main():\n    sub_id = await pubsub.subscribe('events.user.*', handler)\n    await pubsub.publish('events.user.login', {'user': 'simon'})\n    await pubsub.unsubscribe(sub_id)\n```\n\nFeatures:\n1. Async-safe with locks\n2. Supports both sync and async callbacks\n3. Subscription IDs for clean unsubscribe\n4. Wildcard pattern matching\n5. Thread-safe for sync callbacks"}
{"instruction": "Create a WebSocket connection manager for FastAPI that handles multiple rooms/channels", "output": "Here's a full-featured WebSocket manager with room support:\n\n```python\nfrom fastapi import WebSocket\nfrom typing import Dict, Set, Optional, Any\nimport json\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass Connection:\n    websocket: WebSocket\n    user_id: str\n    rooms: Set[str] = field(default_factory=set)\n    connected_at: datetime = field(default_factory=datetime.utcnow)\n\nclass ConnectionManager:\n    def __init__(self):\n        self._connections: Dict[str, Connection] = {}  # user_id -> Connection\n        self._rooms: Dict[str, Set[str]] = {}  # room_name -> set of user_ids\n        self._lock = asyncio.Lock()\n    \n    async def connect(self, websocket: WebSocket, user_id: str) -> Connection:\n        await websocket.accept()\n        conn = Connection(websocket=websocket, user_id=user_id)\n        \n        async with self._lock:\n            # Disconnect existing connection for same user\n            if user_id in self._connections:\n                await self._disconnect_internal(user_id)\n            self._connections[user_id] = conn\n        \n        return conn\n    \n    async def disconnect(self, user_id: str):\n        async with self._lock:\n            await self._disconnect_internal(user_id)\n    \n    async def _disconnect_internal(self, user_id: str):\n        if user_id not in self._connections:\n            return\n        \n        conn = self._connections[user_id]\n        \n        # Remove from all rooms\n        for room in conn.rooms.copy():\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n        \n        del self._connections[user_id]\n        \n        try:\n            await conn.websocket.close()\n        except:\n            pass\n    \n    async def join_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id not in self._connections:\n                return\n            \n            self._connections[user_id].rooms.add(room)\n            if room not in self._rooms:\n                self._rooms[room] = set()\n            self._rooms[room].add(user_id)\n    \n    async def leave_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id in self._connections:\n                self._connections[user_id].rooms.discard(room)\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n    \n    async def send_personal(self, user_id: str, message: Any):\n        if user_id not in self._connections:\n            return False\n        \n        try:\n            data = json.dumps(message) if not isinstance(message, str) else message\n            await self._connections[user_id].websocket.send_text(data)\n            return True\n        except:\n            await self.disconnect(user_id)\n            return False\n    \n    async def broadcast_room(self, room: str, message: Any, exclude: Optional[str] = None):\n        if room not in self._rooms:\n            return 0\n        \n        tasks = []\n        for user_id in self._rooms[room]:\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    async def broadcast_all(self, message: Any, exclude: Optional[str] = None):\n        tasks = []\n        for user_id in list(self._connections.keys()):\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    def get_room_users(self, room: str) -> Set[str]:\n        return self._rooms.get(room, set()).copy()\n    \n    def get_user_rooms(self, user_id: str) -> Set[str]:\n        if user_id in self._connections:\n            return self._connections[user_id].rooms.copy()\n        return set()\n\n# Usage in FastAPI:\nmanager = ConnectionManager()\n\n@app.websocket('/ws/{user_id}')\nasync def websocket_endpoint(websocket: WebSocket, user_id: str):\n    conn = await manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_json()\n            \n            if data.get('action') == 'join':\n                await manager.join_room(user_id, data['room'])\n            elif data.get('action') == 'leave':\n                await manager.leave_room(user_id, data['room'])\n            elif data.get('action') == 'message':\n                await manager.broadcast_room(data['room'], {\n                    'from': user_id,\n                    'content': data['content']\n                })\n    except:\n        await manager.disconnect(user_id)\n```\n\nFeatures:\n1. Multi-room support with join/leave\n2. Personal and broadcast messaging\n3. Automatic cleanup on disconnect\n4. Thread-safe with async locks\n5. Handles reconnection (kicks old connection)"}
{"instruction": "Write a Python script that monitors a directory for new files and processes them", "output": "Here's a robust file watcher with processing queue:\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler, FileCreatedEvent\nimport queue\nimport threading\nimport logging\nfrom typing import Callable, Optional\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass FileProcessor:\n    def __init__(\n        self,\n        watch_dir: str,\n        process_func: Callable[[Path], None],\n        patterns: Optional[list] = None,\n        debounce_seconds: float = 1.0\n    ):\n        self.watch_dir = Path(watch_dir)\n        self.process_func = process_func\n        self.patterns = patterns or ['*']\n        self.debounce_seconds = debounce_seconds\n        \n        self._queue = queue.Queue()\n        self._seen = {}  # path -> last_seen_time\n        self._running = False\n        self._observer = None\n        self._worker_thread = None\n    \n    def _matches_pattern(self, path: Path) -> bool:\n        import fnmatch\n        return any(fnmatch.fnmatch(path.name, p) for p in self.patterns)\n    \n    def _on_created(self, event):\n        if event.is_directory:\n            return\n        \n        path = Path(event.src_path)\n        if not self._matches_pattern(path):\n            return\n        \n        # Debounce: skip if we've seen this file recently\n        now = datetime.now().timestamp()\n        if path in self._seen:\n            if now - self._seen[path] < self.debounce_seconds:\n                return\n        \n        self._seen[path] = now\n        self._queue.put(path)\n        logger.info(f'Queued: {path.name}')\n    \n    def _worker(self):\n        while self._running:\n            try:\n                path = self._queue.get(timeout=1.0)\n            except queue.Empty:\n                continue\n            \n            # Wait for file to be fully written\n            asyncio.run(self._wait_for_stable(path))\n            \n            if not path.exists():\n                logger.warning(f'File disappeared: {path}')\n                continue\n            \n            try:\n                logger.info(f'Processing: {path.name}')\n                self.process_func(path)\n                logger.info(f'Completed: {path.name}')\n            except Exception as e:\n                logger.error(f'Error processing {path.name}: {e}')\n            finally:\n                self._queue.task_done()\n    \n    async def _wait_for_stable(self, path: Path, checks: int = 3, interval: float = 0.5):\n        \"\"\"Wait until file size stops changing.\"\"\"\n        prev_size = -1\n        stable_count = 0\n        \n        while stable_count < checks:\n            if not path.exists():\n                return\n            \n            size = path.stat().st_size\n            if size == prev_size:\n                stable_count += 1\n            else:\n                stable_count = 0\n                prev_size = size\n            \n            await asyncio.sleep(interval)\n    \n    def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        \n        # Start worker thread\n        self._worker_thread = threading.Thread(target=self._worker, daemon=True)\n        self._worker_thread.start()\n        \n        # Setup watchdog\n        handler = FileSystemEventHandler()\n        handler.on_created = self._on_created\n        \n        self._observer = Observer()\n        self._observer.schedule(handler, str(self.watch_dir), recursive=False)\n        self._observer.start()\n        \n        logger.info(f'Watching: {self.watch_dir}')\n    \n    def stop(self):\n        self._running = False\n        \n        if self._observer:\n            self._observer.stop()\n            self._observer.join()\n        \n        if self._worker_thread:\n            self._worker_thread.join(timeout=5.0)\n        \n        logger.info('Stopped watching')\n    \n    def process_existing(self):\n        \"\"\"Process files that already exist in the directory.\"\"\"\n        for path in self.watch_dir.iterdir():\n            if path.is_file() and self._matches_pattern(path):\n                self._queue.put(path)\n\n# Example usage:\ndef process_file(path: Path):\n    \"\"\"Your processing logic here.\"\"\"\n    content = path.read_text()\n    print(f'File {path.name} has {len(content)} characters')\n    # Move to processed folder, upload somewhere, etc.\n\nif __name__ == '__main__':\n    processor = FileProcessor(\n        watch_dir='/path/to/watch',\n        process_func=process_file,\n        patterns=['*.json', '*.csv'],\n        debounce_seconds=2.0\n    )\n    \n    processor.start()\n    processor.process_existing()  # Handle files already there\n    \n    try:\n        while True:\n            asyncio.run(asyncio.sleep(1))\n    except KeyboardInterrupt:\n        processor.stop()\n```\n\nFeatures:\n1. Pattern-based file filtering\n2. Debouncing to avoid duplicate processing\n3. Waits for files to be fully written before processing\n4. Queue-based processing with worker thread\n5. Can process existing files on startup\n6. Graceful shutdown"}
{"instruction": "Create a caching decorator that supports both sync and async functions with TTL", "output": "Here's a universal caching decorator with TTL support:\n\n```python\nimport asyncio\nimport functools\nimport time\nimport hashlib\nimport json\nfrom typing import Any, Callable, Optional, Union\nfrom collections import OrderedDict\nimport threading\n\nclass Cache:\n    def __init__(self, maxsize: int = 1000):\n        self._cache: OrderedDict = OrderedDict()\n        self._maxsize = maxsize\n        self._lock = threading.Lock()\n    \n    def get(self, key: str) -> tuple[bool, Any]:\n        with self._lock:\n            if key not in self._cache:\n                return False, None\n            \n            value, expiry = self._cache[key]\n            if expiry and time.time() > expiry:\n                del self._cache[key]\n                return False, None\n            \n            # Move to end (LRU)\n            self._cache.move_to_end(key)\n            return True, value\n    \n    def set(self, key: str, value: Any, ttl: Optional[float] = None):\n        with self._lock:\n            expiry = time.time() + ttl if ttl else None\n            \n            if key in self._cache:\n                del self._cache[key]\n            \n            self._cache[key] = (value, expiry)\n            \n            # Evict oldest if over maxsize\n            while len(self._cache) > self._maxsize:\n                self._cache.popitem(last=False)\n    \n    def delete(self, key: str):\n        with self._lock:\n            self._cache.pop(key, None)\n    \n    def clear(self):\n        with self._lock:\n            self._cache.clear()\n\n# Global cache instance\n_default_cache = Cache()\n\ndef _make_key(func: Callable, args: tuple, kwargs: dict) -> str:\n    \"\"\"Create a hashable cache key from function and arguments.\"\"\"\n    key_parts = [\n        func.__module__,\n        func.__qualname__,\n        str(args),\n        str(sorted(kwargs.items()))\n    ]\n    key_string = '|'.join(key_parts)\n    return hashlib.md5(key_string.encode()).hexdigest()\n\ndef cached(\n    ttl: Optional[float] = None,\n    maxsize: int = 1000,\n    key_func: Optional[Callable] = None,\n    cache: Optional[Cache] = None\n) -> Callable:\n    \"\"\"Universal caching decorator for sync and async functions.\n    \n    Args:\n        ttl: Time to live in seconds. None = no expiry.\n        maxsize: Maximum cache entries.\n        key_func: Custom function to generate cache key.\n        cache: Custom Cache instance. Uses global cache if None.\n    \"\"\"\n    _cache = cache or _default_cache\n    \n    def decorator(func: Callable) -> Callable:\n        is_async = asyncio.iscoroutinefunction(func)\n        \n        if is_async:\n            @functools.wraps(func)\n            async def async_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = await func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            async_wrapper.cache_clear = _cache.clear\n            async_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return async_wrapper\n        else:\n            @functools.wraps(func)\n            def sync_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            sync_wrapper.cache_clear = _cache.clear\n            sync_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return sync_wrapper\n    \n    return decorator\n\n# Usage examples:\n\n@cached(ttl=60)  # Cache for 60 seconds\ndef get_user(user_id: int) -> dict:\n    # Expensive database query\n    return {'id': user_id, 'name': 'Simon'}\n\n@cached(ttl=300, key_func=lambda url, **kw: url)  # Custom key\nasync def fetch_url(url: str, headers: dict = None) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        return response.text\n\n# Manual cache control\nget_user.cache_clear()  # Clear all\nget_user.cache_delete(42)  # Delete specific key\n```\n\nFeatures:\n1. Works with both sync and async functions\n2. TTL (time-to-live) support\n3. LRU eviction when maxsize reached\n4. Custom key functions\n5. Thread-safe\n6. Manual cache control methods\n7. Shared or isolated cache instances"}
{"instruction": "Build a simple task queue with priority support using asyncio", "output": "Here's a priority task queue implementation:\n\n```python\nimport asyncio\nimport heapq\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Coroutine, Optional\nfrom datetime import datetime\nimport uuid\nimport logging\nfrom enum import IntEnum\n\nlogger = logging.getLogger(__name__)\n\nclass Priority(IntEnum):\n    LOW = 3\n    NORMAL = 2\n    HIGH = 1\n    CRITICAL = 0\n\n@dataclass(order=True)\nclass Task:\n    priority: int\n    created_at: float = field(compare=True)\n    id: str = field(default_factory=lambda: str(uuid.uuid4()), compare=False)\n    func: Callable = field(compare=False)\n    args: tuple = field(default=(), compare=False)\n    kwargs: dict = field(default_factory=dict, compare=False)\n    result: Any = field(default=None, compare=False)\n    error: Optional[Exception] = field(default=None, compare=False)\n    completed: bool = field(default=False, compare=False)\n\nclass TaskQueue:\n    def __init__(self, num_workers: int = 3):\n        self._heap: list[Task] = []\n        self._num_workers = num_workers\n        self._workers: list[asyncio.Task] = []\n        self._running = False\n        self._task_added = asyncio.Event()\n        self._results: dict[str, Task] = {}\n        self._lock = asyncio.Lock()\n    \n    async def add(\n        self,\n        func: Callable[..., Coroutine],\n        *args,\n        priority: Priority = Priority.NORMAL,\n        **kwargs\n    ) -> str:\n        \"\"\"Add a task to the queue. Returns task ID.\"\"\"\n        task = Task(\n            priority=priority,\n            created_at=datetime.now().timestamp(),\n            func=func,\n            args=args,\n            kwargs=kwargs\n        )\n        \n        async with self._lock:\n            heapq.heappush(self._heap, task)\n            self._results[task.id] = task\n        \n        self._task_added.set()\n        logger.debug(f'Added task {task.id} with priority {priority.name}')\n        return task.id\n    \n    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Wait for task completion and return result.\"\"\"\n        start = asyncio.get_event_loop().time()\n        \n        while True:\n            if task_id in self._results:\n                task = self._results[task_id]\n                if task.completed:\n                    if task.error:\n                        raise task.error\n                    return task.result\n            \n            if timeout:\n                elapsed = asyncio.get_event_loop().time() - start\n                if elapsed >= timeout:\n                    raise TimeoutError(f'Task {task_id} did not complete in {timeout}s')\n            \n            await asyncio.sleep(0.1)\n    \n    async def _worker(self, worker_id: int):\n        logger.info(f'Worker {worker_id} started')\n        \n        while self._running:\n            # Wait for tasks\n            task = None\n            async with self._lock:\n                if self._heap:\n                    task = heapq.heappop(self._heap)\n            \n            if not task:\n                self._task_added.clear()\n                try:\n                    await asyncio.wait_for(self._task_added.wait(), timeout=1.0)\n                except asyncio.TimeoutError:\n                    pass\n                continue\n            \n            # Execute task\n            logger.debug(f'Worker {worker_id} executing task {task.id}')\n            try:\n                if asyncio.iscoroutinefunction(task.func):\n                    task.result = await task.func(*task.args, **task.kwargs)\n                else:\n                    task.result = await asyncio.to_thread(task.func, *task.args, **task.kwargs)\n            except Exception as e:\n                task.error = e\n                logger.error(f'Task {task.id} failed: {e}')\n            finally:\n                task.completed = True\n        \n        logger.info(f'Worker {worker_id} stopped')\n    \n    async def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        self._workers = [\n            asyncio.create_task(self._worker(i))\n            for i in range(self._num_workers)\n        ]\n        logger.info(f'TaskQueue started with {self._num_workers} workers')\n    \n    async def stop(self, wait_for_completion: bool = True):\n        if wait_for_completion:\n            # Wait for queue to drain\n            while self._heap:\n                await asyncio.sleep(0.1)\n        \n        self._running = False\n        self._task_added.set()  # Wake up workers\n        \n        await asyncio.gather(*self._workers, return_exceptions=True)\n        logger.info('TaskQueue stopped')\n    \n    @property\n    def pending_count(self) -> int:\n        return len(self._heap)\n    \n    def get_status(self, task_id: str) -> dict:\n        if task_id not in self._results:\n            return {'status': 'not_found'}\n        \n        task = self._results[task_id]\n        return {\n            'id': task.id,\n            'status': 'completed' if task.completed else 'pending',\n            'priority': Priority(task.priority).name,\n            'error': str(task.error) if task.error else None\n        }\n\n# Example usage:\nasync def slow_task(name: str, delay: float) -> str:\n    await asyncio.sleep(delay)\n    return f'Task {name} completed after {delay}s'\n\nasync def main():\n    queue = TaskQueue(num_workers=2)\n    await queue.start()\n    \n    # Add tasks with different priorities\n    id1 = await queue.add(slow_task, 'low', 2.0, priority=Priority.LOW)\n    id2 = await queue.add(slow_task, 'critical', 1.0, priority=Priority.CRITICAL)\n    id3 = await queue.add(slow_task, 'normal', 1.5, priority=Priority.NORMAL)\n    \n    # Critical will run first despite being added second\n    result = await queue.get_result(id2)\n    print(result)  # 'Task critical completed after 1.0s'\n    \n    await queue.stop()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nFeatures:\n1. Priority levels (CRITICAL, HIGH, NORMAL, LOW)\n2. Multiple worker support\n3. Async and sync function support\n4. Result retrieval with timeout\n5. Graceful shutdown\n6. Task status tracking\n7. Thread-safe operations"}
{"instruction": "Implement a circuit breaker pattern for external API calls", "output": "Here's a robust circuit breaker implementation:\n\n```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional, Any, Type\nimport functools\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'      # Normal operation\n    OPEN = 'open'          # Failing, reject calls\n    HALF_OPEN = 'half_open'  # Testing recovery\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    last_success_time: Optional[float] = None\n    consecutive_failures: int = 0\n    consecutive_successes: int = 0\n\nclass CircuitBreakerOpen(Exception):\n    \"\"\"Raised when circuit breaker is open.\"\"\"\n    def __init__(self, name: str, retry_after: float):\n        self.name = name\n        self.retry_after = retry_after\n        super().__init__(f'Circuit {name} is open. Retry after {retry_after:.1f}s')\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        name: str,\n        failure_threshold: int = 5,\n        success_threshold: int = 3,\n        timeout: float = 30.0,\n        exceptions: tuple[Type[Exception], ...] = (Exception,),\n        exclude: tuple[Type[Exception], ...] = ()\n    ):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.success_threshold = success_threshold\n        self.timeout = timeout\n        self.exceptions = exceptions\n        self.exclude = exclude\n        \n        self._state = CircuitState.CLOSED\n        self._stats = CircuitStats()\n        self._opened_at: Optional[float] = None\n        self._lock = asyncio.Lock()\n    \n    @property\n    def state(self) -> CircuitState:\n        if self._state == CircuitState.OPEN:\n            if time.time() - self._opened_at >= self.timeout:\n                return CircuitState.HALF_OPEN\n        return self._state\n    \n    async def _on_success(self):\n        async with self._lock:\n            self._stats.successes += 1\n            self._stats.last_success_time = time.time()\n            self._stats.consecutive_successes += 1\n            self._stats.consecutive_failures = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                if self._stats.consecutive_successes >= self.success_threshold:\n                    logger.info(f'Circuit {self.name}: HALF_OPEN -> CLOSED')\n                    self._state = CircuitState.CLOSED\n                    self._stats.consecutive_successes = 0\n    \n    async def _on_failure(self, error: Exception):\n        async with self._lock:\n            self._stats.failures += 1\n            self._stats.last_failure_time = time.time()\n            self._stats.consecutive_failures += 1\n            self._stats.consecutive_successes = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                logger.warning(f'Circuit {self.name}: HALF_OPEN -> OPEN (test failed)')\n                self._state = CircuitState.OPEN\n                self._opened_at = time.time()\n            \n            elif self._state == CircuitState.CLOSED:\n                if self._stats.consecutive_failures >= self.failure_threshold:\n                    logger.warning(f'Circuit {self.name}: CLOSED -> OPEN (threshold reached)')\n                    self._state = CircuitState.OPEN\n                    self._opened_at = time.time()\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        state = self.state  # Check with timeout logic\n        \n        if state == CircuitState.OPEN:\n            retry_after = self.timeout - (time.time() - self._opened_at)\n            raise CircuitBreakerOpen(self.name, retry_after)\n        \n        try:\n            if asyncio.iscoroutinefunction(func):\n                result = await func(*args, **kwargs)\n            else:\n                result = await asyncio.to_thread(func, *args, **kwargs)\n            \n            await self._on_success()\n            return result\n        \n        except self.exclude:\n            # These exceptions don't count as failures\n            raise\n        except self.exceptions as e:\n            await self._on_failure(e)\n            raise\n    \n    def get_stats(self) -> dict:\n        return {\n            'name': self.name,\n            'state': self.state.value,\n            'failures': self._stats.failures,\n            'successes': self._stats.successes,\n            'consecutive_failures': self._stats.consecutive_failures,\n            'consecutive_successes': self._stats.consecutive_successes\n        }\n    \n    async def reset(self):\n        async with self._lock:\n            self._state = CircuitState.CLOSED\n            self._stats = CircuitStats()\n            self._opened_at = None\n\n# Decorator version\ndef circuit_breaker(\n    name: str,\n    failure_threshold: int = 5,\n    success_threshold: int = 3,\n    timeout: float = 30.0,\n    exceptions: tuple = (Exception,)\n) -> Callable:\n    breaker = CircuitBreaker(\n        name=name,\n        failure_threshold=failure_threshold,\n        success_threshold=success_threshold,\n        timeout=timeout,\n        exceptions=exceptions\n    )\n    \n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await breaker.call(func, *args, **kwargs)\n        \n        wrapper.circuit_breaker = breaker\n        return wrapper\n    \n    return decorator\n\n# Usage examples:\n\n# As decorator\n@circuit_breaker('external_api', failure_threshold=3, timeout=60)\nasync def call_external_api(endpoint: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f'https://api.example.com/{endpoint}')\n        response.raise_for_status()\n        return response.json()\n\n# Manual usage\napi_breaker = CircuitBreaker('payment_gateway', failure_threshold=2, timeout=120)\n\nasync def process_payment(amount: float):\n    try:\n        result = await api_breaker.call(payment_api.charge, amount)\n        return result\n    except CircuitBreakerOpen as e:\n        # Handle gracefully - maybe queue for later\n        logger.warning(f'Payment service unavailable: {e}')\n        return {'status': 'queued', 'retry_after': e.retry_after}\n\n# Check stats\nprint(call_external_api.circuit_breaker.get_stats())\n```\n\nFeatures:\n1. Three states: CLOSED (normal), OPEN (failing), HALF_OPEN (testing)\n2. Configurable failure/success thresholds\n3. Automatic timeout and recovery testing\n4. Exception filtering (track specific errors)\n5. Both decorator and manual usage patterns\n6. Statistics tracking\n7. Async-safe with locks\n8. Clear error messages with retry timing"}
{"instruction": "Write a function that fetches user data from an API", "output": "Here's a properly structured implementation:\n\n```python\nfrom typing import Optional\nfrom dataclasses import dataclass\nimport httpx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass User:\n    \"\"\"User data model.\"\"\"\n    id: int\n    name: str\n    email: str\n    is_active: bool = True\n\nclass UserAPIError(Exception):\n    \"\"\"Raised when user API request fails.\"\"\"\n    pass\n\nclass UserNotFoundError(UserAPIError):\n    \"\"\"Raised when user doesn't exist.\"\"\"\n    pass\n\nasync def fetch_user(user_id: int, base_url: str = 'https://api.example.com') -> User:\n    \"\"\"\n    Fetch user data from API.\n    \n    Args:\n        user_id: The user's unique identifier\n        base_url: API base URL\n    \n    Returns:\n        User object with fetched data\n    \n    Raises:\n        UserNotFoundError: If user doesn't exist\n        UserAPIError: If API request fails\n    \"\"\"\n    if user_id <= 0:\n        raise ValueError('user_id must be positive')\n    \n    url = f'{base_url}/users/{user_id}'\n    \n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(url)\n            \n            if response.status_code == 404:\n                raise UserNotFoundError(f'User {user_id} not found')\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            return User(\n                id=data['id'],\n                name=data['name'],\n                email=data['email'],\n                is_active=data.get('is_active', True)\n            )\n    \n    except httpx.TimeoutException:\n        logger.error(f'Timeout fetching user {user_id}')\n        raise UserAPIError('API request timed out')\n    \n    except httpx.HTTPStatusError as e:\n        logger.error(f'HTTP error fetching user {user_id}: {e}')\n        raise UserAPIError(f'API returned status {e.response.status_code}')\n```\n\nKey quality aspects:\n1. **Type hints** on all parameters and return values\n2. **Dataclass** for structured data instead of raw dict\n3. **Custom exceptions** for different error types\n4. **Docstring** with Args, Returns, Raises\n5. **Input validation** before making request\n6. **Specific error handling** for different failure modes\n7. **Logging** for debugging\n8. **Timeout** to prevent hanging"}
{"instruction": "Create a configuration loader", "output": "Here's a production-ready configuration system:\n\n```python\nfrom typing import Any, Optional, TypeVar, Type\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport os\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\nclass ConfigError(Exception):\n    \"\"\"Configuration loading or validation error.\"\"\"\n    pass\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection settings.\"\"\"\n    host: str = 'localhost'\n    port: int = 5432\n    name: str = 'app_db'\n    user: str = 'postgres'\n    password: str = field(default='', repr=False)  # Hide password in logs\n    \n    @property\n    def url(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.name}'\n\n@dataclass\nclass APIConfig:\n    \"\"\"API server settings.\"\"\"\n    host: str = '0.0.0.0'\n    port: int = 8000\n    debug: bool = False\n    cors_origins: list[str] = field(default_factory=lambda: ['http://localhost:3000'])\n\n@dataclass\nclass AppConfig:\n    \"\"\"Main application configuration.\"\"\"\n    app_name: str = 'MyApp'\n    environment: str = 'development'\n    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n    api: APIConfig = field(default_factory=APIConfig)\n    \n    @property\n    def is_production(self) -> bool:\n        return self.environment == 'production'\n\nclass ConfigLoader:\n    \"\"\"\n    Load configuration from multiple sources with priority:\n    1. Environment variables (highest)\n    2. Config file\n    3. Defaults (lowest)\n    \"\"\"\n    \n    def __init__(self, config_path: Optional[Path] = None):\n        self._config_path = config_path\n        self._file_config: dict = {}\n        \n        if config_path and config_path.exists():\n            self._load_file(config_path)\n    \n    def _load_file(self, path: Path) -> None:\n        try:\n            with open(path) as f:\n                self._file_config = json.load(f)\n            logger.info(f'Loaded config from {path}')\n        except json.JSONDecodeError as e:\n            raise ConfigError(f'Invalid JSON in {path}: {e}')\n        except IOError as e:\n            raise ConfigError(f'Cannot read {path}: {e}')\n    \n    def get(self, key: str, default: T = None, required: bool = False) -> T:\n        \"\"\"\n        Get config value with environment override.\n        \n        Environment variables use uppercase with underscores:\n        'database.host' -> DATABASE_HOST\n        \"\"\"\n        # Try environment variable first\n        env_key = key.upper().replace('.', '_')\n        env_value = os.environ.get(env_key)\n        \n        if env_value is not None:\n            return self._convert_type(env_value, type(default) if default else str)\n        \n        # Try config file\n        value = self._get_nested(self._file_config, key)\n        if value is not None:\n            return value\n        \n        # Use default or raise\n        if required and default is None:\n            raise ConfigError(f'Required config missing: {key}')\n        \n        return default\n    \n    def _get_nested(self, data: dict, key: str) -> Any:\n        \"\"\"Get nested value using dot notation.\"\"\"\n        keys = key.split('.')\n        value = data\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return None\n        \n        return value\n    \n    def _convert_type(self, value: str, target_type: Type[T]) -> T:\n        \"\"\"Convert string to target type.\"\"\"\n        if target_type == bool:\n            return value.lower() in ('true', '1', 'yes')\n        if target_type == int:\n            return int(value)\n        if target_type == float:\n            return float(value)\n        if target_type == list:\n            return value.split(',')\n        return value\n    \n    def load_app_config(self) -> AppConfig:\n        \"\"\"Load complete application configuration.\"\"\"\n        return AppConfig(\n            app_name=self.get('app_name', 'MyApp'),\n            environment=self.get('environment', 'development'),\n            database=DatabaseConfig(\n                host=self.get('database.host', 'localhost'),\n                port=self.get('database.port', 5432),\n                name=self.get('database.name', 'app_db'),\n                user=self.get('database.user', 'postgres'),\n                password=self.get('database.password', '', required=False),\n            ),\n            api=APIConfig(\n                host=self.get('api.host', '0.0.0.0'),\n                port=self.get('api.port', 8000),\n                debug=self.get('api.debug', False),\n            )\n        )\n\n# Usage:\nconfig_loader = ConfigLoader(Path('config.json'))\nconfig = config_loader.load_app_config()\n\nprint(config.database.url)  # Uses env var if DATABASE_PASSWORD is set\nprint(config.is_production)  # Property for common checks\n```\n\nQuality aspects:\n1. **Layered config** - env vars override file override defaults\n2. **Type safety** with dataclasses and type hints\n3. **Sensitive data handling** - password hidden from repr\n4. **Dot notation** for nested access\n5. **Automatic type conversion** from env vars\n6. **Required vs optional** distinction\n7. **Custom exceptions** for clear error messages\n8. **Properties** for derived values"}
{"instruction": "Write code to process a list of items", "output": "Here's how to properly structure list processing:\n\n```python\nfrom typing import TypeVar, Callable, Iterator, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\nR = TypeVar('R')\n\nclass ProcessingStatus(Enum):\n    SUCCESS = 'success'\n    FAILED = 'failed'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass ProcessingResult(Generic[T, R]):\n    \"\"\"Result of processing a single item.\"\"\"\n    item: T\n    status: ProcessingStatus\n    result: Optional[R] = None\n    error: Optional[str] = None\n\n@dataclass\nclass BatchResult(Generic[T, R]):\n    \"\"\"Summary of batch processing.\"\"\"\n    total: int\n    successful: int\n    failed: int\n    skipped: int\n    results: list[ProcessingResult[T, R]]\n    \n    @property\n    def success_rate(self) -> float:\n        return self.successful / self.total if self.total > 0 else 0.0\n\ndef process_items(\n    items: list[T],\n    processor: Callable[[T], R],\n    *,\n    skip_condition: Optional[Callable[[T], bool]] = None,\n    on_error: str = 'continue',  # 'continue', 'raise', 'collect'\n    max_workers: int = 1,\n    progress_callback: Optional[Callable[[int, int], None]] = None\n) -> BatchResult[T, R]:\n    \"\"\"\n    Process a list of items with error handling and progress tracking.\n    \n    Args:\n        items: Items to process\n        processor: Function to apply to each item\n        skip_condition: Optional function to skip items\n        on_error: How to handle errors - continue, raise, or collect\n        max_workers: Parallel workers (1 = sequential)\n        progress_callback: Called with (completed, total) after each item\n    \n    Returns:\n        BatchResult with detailed processing summary\n    \"\"\"\n    results: list[ProcessingResult[T, R]] = []\n    total = len(items)\n    \n    def process_single(item: T, index: int) -> ProcessingResult[T, R]:\n        # Check skip condition\n        if skip_condition and skip_condition(item):\n            logger.debug(f'Skipping item {index}: {item}')\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SKIPPED\n            )\n        \n        # Process item\n        try:\n            result = processor(item)\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SUCCESS,\n                result=result\n            )\n        except Exception as e:\n            logger.warning(f'Error processing item {index}: {e}')\n            \n            if on_error == 'raise':\n                raise\n            \n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.FAILED,\n                error=str(e)\n            )\n    \n    # Sequential processing\n    if max_workers == 1:\n        for i, item in enumerate(items):\n            result = process_single(item, i)\n            results.append(result)\n            \n            if progress_callback:\n                progress_callback(i + 1, total)\n    \n    # Parallel processing\n    else:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {\n                executor.submit(process_single, item, i): i\n                for i, item in enumerate(items)\n            }\n            \n            completed = 0\n            for future in as_completed(futures):\n                results.append(future.result())\n                completed += 1\n                \n                if progress_callback:\n                    progress_callback(completed, total)\n    \n    # Build summary\n    successful = sum(1 for r in results if r.status == ProcessingStatus.SUCCESS)\n    failed = sum(1 for r in results if r.status == ProcessingStatus.FAILED)\n    skipped = sum(1 for r in results if r.status == ProcessingStatus.SKIPPED)\n    \n    return BatchResult(\n        total=total,\n        successful=successful,\n        failed=failed,\n        skipped=skipped,\n        results=results\n    )\n\n# Usage example:\norders = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': -50}, {'id': 3, 'amount': 200}]\n\ndef process_order(order: dict) -> dict:\n    if order['amount'] <= 0:\n        raise ValueError('Invalid amount')\n    return {'order_id': order['id'], 'processed': True}\n\nresult = process_items(\n    orders,\n    process_order,\n    skip_condition=lambda o: o.get('skip', False),\n    on_error='continue',\n    progress_callback=lambda done, total: print(f'{done}/{total}')\n)\n\nprint(f'Success rate: {result.success_rate:.1%}')\nfor r in result.results:\n    if r.status == ProcessingStatus.FAILED:\n        print(f'Failed: {r.item} - {r.error}')\n```\n\nQuality aspects:\n1. **Generic types** for reusability\n2. **Dataclasses** for structured results\n3. **Enum** for status instead of magic strings\n4. **Configurable error handling** - don't assume one strategy\n5. **Progress tracking** callback\n6. **Optional parallelization**\n7. **Detailed summary** with success rate\n8. **Docstring** explaining all options"}
{"instruction": "Create a simple database class", "output": "Here's a properly structured database wrapper:\n\n```python\nfrom typing import Optional, Any, TypeVar, Type, Generic\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nfrom abc import ABC, abstractmethod\nimport asyncpg\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection configuration.\"\"\"\n    host: str\n    port: int\n    database: str\n    user: str\n    password: str\n    min_connections: int = 5\n    max_connections: int = 20\n    \n    @property\n    def dsn(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}'\n\nclass DatabaseError(Exception):\n    \"\"\"Base database error.\"\"\"\n    pass\n\nclass ConnectionError(DatabaseError):\n    \"\"\"Failed to connect to database.\"\"\"\n    pass\n\nclass QueryError(DatabaseError):\n    \"\"\"Query execution failed.\"\"\"\n    pass\n\nclass Database:\n    \"\"\"\n    Async PostgreSQL database wrapper with connection pooling.\n    \n    Usage:\n        db = Database(config)\n        await db.connect()\n        \n        users = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\n        \n        async with db.transaction():\n            await db.execute('UPDATE users SET active = $1 WHERE id = $2', False, 123)\n        \n        await db.disconnect()\n    \"\"\"\n    \n    def __init__(self, config: DatabaseConfig):\n        self._config = config\n        self._pool: Optional[asyncpg.Pool] = None\n    \n    @property\n    def is_connected(self) -> bool:\n        return self._pool is not None\n    \n    async def connect(self) -> None:\n        \"\"\"Initialize connection pool.\"\"\"\n        if self._pool is not None:\n            logger.warning('Database already connected')\n            return\n        \n        try:\n            self._pool = await asyncpg.create_pool(\n                dsn=self._config.dsn,\n                min_size=self._config.min_connections,\n                max_size=self._config.max_connections\n            )\n            logger.info(f'Connected to {self._config.host}:{self._config.port}/{self._config.database}')\n        \n        except asyncpg.InvalidCatalogNameError:\n            raise ConnectionError(f'Database {self._config.database} does not exist')\n        \n        except asyncpg.InvalidPasswordError:\n            raise ConnectionError('Invalid database credentials')\n        \n        except OSError as e:\n            raise ConnectionError(f'Cannot reach database server: {e}')\n    \n    async def disconnect(self) -> None:\n        \"\"\"Close all connections.\"\"\"\n        if self._pool is None:\n            return\n        \n        await self._pool.close()\n        self._pool = None\n        logger.info('Database disconnected')\n    \n    def _ensure_connected(self) -> asyncpg.Pool:\n        if self._pool is None:\n            raise DatabaseError('Database not connected. Call connect() first.')\n        return self._pool\n    \n    async def execute(self, query: str, *args: Any) -> str:\n        \"\"\"\n        Execute a query that doesn't return rows.\n        \n        Returns:\n            Status string (e.g., 'UPDATE 5')\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.execute(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_one(self, query: str, *args: Any) -> Optional[dict]:\n        \"\"\"Fetch a single row as dict, or None if not found.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            row = await pool.fetchrow(query, *args)\n            return dict(row) if row else None\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_all(self, query: str, *args: Any) -> list[dict]:\n        \"\"\"Fetch all rows as list of dicts.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            rows = await pool.fetch(query, *args)\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_value(self, query: str, *args: Any) -> Any:\n        \"\"\"Fetch a single value (first column of first row).\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.fetchval(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    @asynccontextmanager\n    async def transaction(self):\n        \"\"\"\n        Transaction context manager with automatic rollback on error.\n        \n        Usage:\n            async with db.transaction():\n                await db.execute(...)\n                await db.execute(...)\n                # Commits if no error, rolls back otherwise\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        async with pool.acquire() as conn:\n            async with conn.transaction():\n                # Temporarily replace pool operations to use this connection\n                original_pool = self._pool\n                \n                class SingleConnectionPool:\n                    async def execute(self, *a): return await conn.execute(*a)\n                    async def fetch(self, *a): return await conn.fetch(*a)\n                    async def fetchrow(self, *a): return await conn.fetchrow(*a)\n                    async def fetchval(self, *a): return await conn.fetchval(*a)\n                \n                self._pool = SingleConnectionPool()\n                try:\n                    yield\n                finally:\n                    self._pool = original_pool\n    \n    async def table_exists(self, table_name: str) -> bool:\n        \"\"\"Check if a table exists.\"\"\"\n        result = await self.fetch_value(\n            \"SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_name = $1)\",\n            table_name\n        )\n        return result\n\n# Usage:\nconfig = DatabaseConfig(\n    host='localhost',\n    port=5432,\n    database='myapp',\n    user='postgres',\n    password='secret'\n)\n\ndb = Database(config)\nawait db.connect()\n\n# Simple queries\nusers = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\ncount = await db.fetch_value('SELECT COUNT(*) FROM users')\n\n# Transactions\nasync with db.transaction():\n    await db.execute('INSERT INTO audit_log (action) VALUES ($1)', 'user_created')\n    await db.execute('INSERT INTO users (name) VALUES ($1)', 'Simon')\n\nawait db.disconnect()\n```\n\nQuality aspects:\n1. **Connection pooling** for performance\n2. **Custom exceptions** hierarchy\n3. **Context manager** for transactions\n4. **Multiple fetch methods** for different use cases\n5. **Connection state checking**\n6. **Detailed logging**\n7. **DSN property** for clean URL building\n8. **Type hints** throughout"}
{"instruction": "Write a function to validate user input", "output": "Here's a comprehensive validation system:\n\n```python\nfrom typing import Any, Callable, Optional, Union, TypeVar\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport re\nfrom enum import Enum\n\nT = TypeVar('T')\n\nclass ValidationSeverity(Enum):\n    ERROR = 'error'      # Blocks submission\n    WARNING = 'warning'  # Allows submission with notice\n\n@dataclass\nclass ValidationError:\n    \"\"\"Single validation error.\"\"\"\n    field: str\n    message: str\n    code: str\n    severity: ValidationSeverity = ValidationSeverity.ERROR\n    value: Any = None\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Complete validation result.\"\"\"\n    is_valid: bool\n    errors: list[ValidationError] = field(default_factory=list)\n    warnings: list[ValidationError] = field(default_factory=list)\n    \n    def add_error(self, field: str, message: str, code: str, value: Any = None):\n        self.errors.append(ValidationError(field, message, code, ValidationSeverity.ERROR, value))\n        self.is_valid = False\n    \n    def add_warning(self, field: str, message: str, code: str, value: Any = None):\n        self.warnings.append(ValidationError(field, message, code, ValidationSeverity.WARNING, value))\n    \n    def merge(self, other: 'ValidationResult') -> 'ValidationResult':\n        \"\"\"Combine two validation results.\"\"\"\n        return ValidationResult(\n            is_valid=self.is_valid and other.is_valid,\n            errors=self.errors + other.errors,\n            warnings=self.warnings + other.warnings\n        )\n\nclass Validator(ABC):\n    \"\"\"Base validator class.\"\"\"\n    \n    @abstractmethod\n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        pass\n\nclass Required(Validator):\n    \"\"\"Validates that value is present and not empty.\"\"\"\n    \n    def __init__(self, message: str = 'This field is required'):\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None or value == '' or (isinstance(value, (list, dict)) and len(value) == 0):\n            result.add_error(field_name, self.message, 'required')\n        \n        return result\n\nclass MinLength(Validator):\n    \"\"\"Validates minimum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be at least {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) < self.length:\n            result.add_error(field_name, self.message, 'min_length', value)\n        \n        return result\n\nclass MaxLength(Validator):\n    \"\"\"Validates maximum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be no more than {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) > self.length:\n            result.add_error(field_name, self.message, 'max_length', value)\n        \n        return result\n\nclass Pattern(Validator):\n    \"\"\"Validates against regex pattern.\"\"\"\n    \n    def __init__(self, pattern: str, message: str = 'Invalid format'):\n        self.pattern = re.compile(pattern)\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.pattern.match(str(value)):\n            result.add_error(field_name, self.message, 'pattern', value)\n        \n        return result\n\nclass Email(Pattern):\n    \"\"\"Validates email format.\"\"\"\n    \n    def __init__(self, message: str = 'Invalid email address'):\n        super().__init__(\n            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n            message\n        )\n\nclass Range(Validator):\n    \"\"\"Validates numeric range.\"\"\"\n    \n    def __init__(self, min_val: Optional[float] = None, max_val: Optional[float] = None):\n        self.min_val = min_val\n        self.max_val = max_val\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None:\n            return result\n        \n        try:\n            num = float(value)\n            \n            if self.min_val is not None and num < self.min_val:\n                result.add_error(field_name, f'Must be at least {self.min_val}', 'min_value', value)\n            \n            if self.max_val is not None and num > self.max_val:\n                result.add_error(field_name, f'Must be no more than {self.max_val}', 'max_value', value)\n        \n        except (TypeError, ValueError):\n            result.add_error(field_name, 'Must be a number', 'not_numeric', value)\n        \n        return result\n\nclass Custom(Validator):\n    \"\"\"Custom validation function.\"\"\"\n    \n    def __init__(self, func: Callable[[Any], bool], message: str, code: str = 'custom'):\n        self.func = func\n        self.message = message\n        self.code = code\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.func(value):\n            result.add_error(field_name, self.message, self.code, value)\n        \n        return result\n\n@dataclass\nclass Field:\n    \"\"\"Field definition with validators.\"\"\"\n    name: str\n    validators: list[Validator] = field(default_factory=list)\n    \n    def validate(self, value: Any) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for validator in self.validators:\n            result = result.merge(validator.validate(value, self.name))\n        \n        return result\n\nclass Schema:\n    \"\"\"Validation schema for objects.\"\"\"\n    \n    def __init__(self, fields: list[Field]):\n        self._fields = {f.name: f for f in fields}\n    \n    def validate(self, data: dict) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for field_name, field in self._fields.items():\n            value = data.get(field_name)\n            result = result.merge(field.validate(value))\n        \n        return result\n\n# Usage example:\nuser_schema = Schema([\n    Field('email', [Required(), Email()]),\n    Field('password', [Required(), MinLength(8), MaxLength(100)]),\n    Field('age', [Range(min_val=18, max_val=120)]),\n    Field('username', [\n        Required(),\n        MinLength(3),\n        MaxLength(20),\n        Pattern(r'^[a-zA-Z0-9_]+$', 'Only letters, numbers, and underscores allowed'),\n        Custom(\n            lambda x: x.lower() not in ['admin', 'root', 'system'],\n            'This username is reserved',\n            'reserved_username'\n        )\n    ])\n])\n\n# Validate\ndata = {\n    'email': 'invalid-email',\n    'password': '123',\n    'age': 15,\n    'username': 'admin'\n}\n\nresult = user_schema.validate(data)\n\nif not result.is_valid:\n    for error in result.errors:\n        print(f'{error.field}: {error.message} (code: {error.code})')\n\n# Output:\n# email: Invalid email address (code: pattern)\n# password: Must be at least 8 characters (code: min_length)\n# age: Must be at least 18 (code: min_value)\n# username: This username is reserved (code: reserved_username)\n```\n\nQuality aspects:\n1. **Composable validators** - mix and match\n2. **Custom validators** - easy extension\n3. **Error codes** for programmatic handling\n4. **Severity levels** - errors vs warnings\n5. **Schema-based** validation for objects\n6. **Detailed error info** including invalid value\n7. **Immutable results** with merge support\n8. **Pre-built common validators** (email, range, pattern)"}
{"instruction": "Write a logging setup for an application", "output": "Here's a production-ready logging configuration:\n\n```python\nimport logging\nimport logging.handlers\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport traceback\n\nclass LogLevel(Enum):\n    DEBUG = 'DEBUG'\n    INFO = 'INFO'\n    WARNING = 'WARNING'\n    ERROR = 'ERROR'\n    CRITICAL = 'CRITICAL'\n\n@dataclass\nclass LogConfig:\n    \"\"\"Logging configuration.\"\"\"\n    app_name: str\n    level: LogLevel = LogLevel.INFO\n    log_dir: Optional[Path] = None\n    max_file_size_mb: int = 10\n    backup_count: int = 5\n    json_format: bool = False\n    include_hostname: bool = False\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"\n    JSON log formatter for structured logging.\n    Ideal for log aggregation systems (ELK, Datadog, etc.)\n    \"\"\"\n    \n    def __init__(self, app_name: str, include_hostname: bool = False):\n        super().__init__()\n        self.app_name = app_name\n        self.include_hostname = include_hostname\n        \n        if include_hostname:\n            import socket\n            self.hostname = socket.gethostname()\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_data = {\n            'timestamp': datetime.utcnow().isoformat() + 'Z',\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'app': self.app_name,\n        }\n        \n        # Add location info\n        if record.levelno >= logging.WARNING:\n            log_data['location'] = {\n                'file': record.filename,\n                'line': record.lineno,\n                'function': record.funcName\n            }\n        \n        # Add hostname if configured\n        if self.include_hostname:\n            log_data['hostname'] = self.hostname\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_data['exception'] = {\n                'type': record.exc_info[0].__name__ if record.exc_info[0] else None,\n                'message': str(record.exc_info[1]) if record.exc_info[1] else None,\n                'traceback': traceback.format_exception(*record.exc_info)\n            }\n        \n        # Add extra fields\n        for key, value in record.__dict__.items():\n            if key not in [\n                'name', 'msg', 'args', 'created', 'filename', 'funcName',\n                'levelname', 'levelno', 'lineno', 'module', 'msecs',\n                'pathname', 'process', 'processName', 'relativeCreated',\n                'stack_info', 'exc_info', 'exc_text', 'thread', 'threadName',\n                'message', 'taskName'\n            ]:\n                log_data[key] = value\n        \n        return json.dumps(log_data, default=str)\n\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Colored console output for development.\"\"\"\n    \n    COLORS = {\n        'DEBUG': '\\033[36m',     # Cyan\n        'INFO': '\\033[32m',      # Green\n        'WARNING': '\\033[33m',   # Yellow\n        'ERROR': '\\033[31m',     # Red\n        'CRITICAL': '\\033[35m',  # Magenta\n    }\n    RESET = '\\033[0m'\n    \n    def format(self, record: logging.LogRecord) -> str:\n        color = self.COLORS.get(record.levelname, '')\n        \n        # Format: [TIME] LEVEL logger: message\n        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')\n        \n        formatted = f'{color}[{timestamp}] {record.levelname:8}{self.RESET} '\n        formatted += f'{record.name}: {record.getMessage()}'\n        \n        if record.exc_info:\n            formatted += '\\n' + self.formatException(record.exc_info)\n        \n        return formatted\n\ndef setup_logging(config: LogConfig) -> logging.Logger:\n    \"\"\"\n    Configure application logging.\n    \n    Sets up:\n    - Console handler (colored in dev, JSON optional)\n    - File handler with rotation (if log_dir specified)\n    - Separate error log file\n    \n    Returns:\n        Root logger for the application\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(config.app_name)\n    logger.setLevel(getattr(logging, config.level.value))\n    \n    # Remove existing handlers\n    logger.handlers.clear()\n    \n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    \n    if config.json_format:\n        console_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n    else:\n        console_handler.setFormatter(ColoredFormatter())\n    \n    logger.addHandler(console_handler)\n    \n    # File handlers (if log directory specified)\n    if config.log_dir:\n        config.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Main log file (rotating)\n        main_log = config.log_dir / f'{config.app_name}.log'\n        file_handler = logging.handlers.RotatingFileHandler(\n            main_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(file_handler)\n        \n        # Error log file (errors and above only)\n        error_log = config.log_dir / f'{config.app_name}.error.log'\n        error_handler = logging.handlers.RotatingFileHandler(\n            error_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        error_handler.setLevel(logging.ERROR)\n        error_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(error_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    logger.info(f'Logging initialized', extra={'config': config.__dict__})\n    \n    return logger\n\nclass LoggerMixin:\n    \"\"\"Mixin to add logger to any class.\"\"\"\n    \n    @property\n    def logger(self) -> logging.Logger:\n        if not hasattr(self, '_logger'):\n            self._logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')\n        return self._logger\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a child logger.\"\"\"\n    return logging.getLogger(name)\n\n# Context manager for adding extra fields\nclass LogContext:\n    \"\"\"Add extra fields to all logs within context.\"\"\"\n    \n    def __init__(self, logger: logging.Logger, **extra: Any):\n        self.logger = logger\n        self.extra = extra\n        self.old_factory = None\n    \n    def __enter__(self):\n        self.old_factory = logging.getLogRecordFactory()\n        extra = self.extra\n        \n        def factory(*args, **kwargs):\n            record = self.old_factory(*args, **kwargs)\n            for key, value in extra.items():\n                setattr(record, key, value)\n            return record\n        \n        logging.setLogRecordFactory(factory)\n        return self\n    \n    def __exit__(self, *args):\n        logging.setLogRecordFactory(self.old_factory)\n\n# Usage:\nconfig = LogConfig(\n    app_name='myapp',\n    level=LogLevel.DEBUG,\n    log_dir=Path('./logs'),\n    json_format=False  # Set True for production\n)\n\nlogger = setup_logging(config)\n\n# Basic logging\nlogger.info('Application started')\nlogger.warning('Low memory', extra={'available_mb': 256})\n\n# With context\nwith LogContext(logger, request_id='abc-123', user_id=42):\n    logger.info('Processing request')  # Includes request_id and user_id\n\n# In classes\nclass UserService(LoggerMixin):\n    def create_user(self, name: str):\n        self.logger.info(f'Creating user: {name}')\n\ntry:\n    raise ValueError('Something went wrong')\nexcept Exception:\n    logger.exception('Failed to process')  # Includes full traceback\n```\n\nQuality aspects:\n1. **Structured JSON logging** for production\n2. **Colored output** for development\n3. **Log rotation** to prevent disk fill\n4. **Separate error log** for easy monitoring\n5. **Context manager** for request tracing\n6. **LoggerMixin** for easy class integration\n7. **Extra fields** support\n8. **Configurable** via dataclass"}
{"instruction": "Create a health check endpoint", "output": "Here's a comprehensive health check system:\n\n```python\nfrom typing import Optional, Callable, Any\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nimport asyncio\nimport time\nimport logging\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\nclass HealthStatus(str, Enum):\n    HEALTHY = 'healthy'\n    DEGRADED = 'degraded'  # Working but with issues\n    UNHEALTHY = 'unhealthy'\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"Health status of a single component.\"\"\"\n    name: str\n    status: HealthStatus\n    message: Optional[str] = None\n    response_time_ms: Optional[float] = None\n    last_check: Optional[datetime] = None\n    details: dict = field(default_factory=dict)\n\nclass HealthCheckResponse(BaseModel):\n    \"\"\"API response model.\"\"\"\n    status: HealthStatus\n    timestamp: str\n    version: str\n    uptime_seconds: float\n    components: dict[str, dict]\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Individual health check definition.\"\"\"\n    name: str\n    check_func: Callable[[], Any]  # Returns True/False or raises\n    timeout_seconds: float = 5.0\n    critical: bool = True  # If False, failure only causes DEGRADED\n    \n    async def run(self) -> ComponentHealth:\n        \"\"\"Execute health check with timeout.\"\"\"\n        start = time.time()\n        \n        try:\n            if asyncio.iscoroutinefunction(self.check_func):\n                result = await asyncio.wait_for(\n                    self.check_func(),\n                    timeout=self.timeout_seconds\n                )\n            else:\n                result = await asyncio.wait_for(\n                    asyncio.to_thread(self.check_func),\n                    timeout=self.timeout_seconds\n                )\n            \n            elapsed_ms = (time.time() - start) * 1000\n            \n            if result is True or result is None:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n            elif isinstance(result, dict):\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow(),\n                    details=result\n                )\n            else:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n        \n        except asyncio.TimeoutError:\n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=f'Timeout after {self.timeout_seconds}s',\n                response_time_ms=self.timeout_seconds * 1000,\n                last_check=datetime.utcnow()\n            )\n        \n        except Exception as e:\n            elapsed_ms = (time.time() - start) * 1000\n            logger.warning(f'Health check {self.name} failed: {e}')\n            \n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=str(e),\n                response_time_ms=elapsed_ms,\n                last_check=datetime.utcnow()\n            )\n\nclass HealthChecker:\n    \"\"\"\n    Manages health checks for an application.\n    \n    Usage:\n        health = HealthChecker(app_version='1.0.0')\n        \n        health.add_check('database', check_database, critical=True)\n        health.add_check('cache', check_redis, critical=False)\n        \n        result = await health.check_all()\n    \"\"\"\n    \n    def __init__(self, app_version: str = '1.0.0'):\n        self._checks: list[HealthCheck] = []\n        self._start_time = datetime.utcnow()\n        self._version = app_version\n        self._last_result: Optional[HealthCheckResponse] = None\n        self._cache_ttl = timedelta(seconds=5)  # Don't check more than every 5s\n        self._last_check_time: Optional[datetime] = None\n    \n    def add_check(\n        self,\n        name: str,\n        check_func: Callable,\n        timeout: float = 5.0,\n        critical: bool = True\n    ) -> 'HealthChecker':\n        \"\"\"Add a health check. Returns self for chaining.\"\"\"\n        self._checks.append(HealthCheck(\n            name=name,\n            check_func=check_func,\n            timeout_seconds=timeout,\n            critical=critical\n        ))\n        return self\n    \n    async def check_all(self, use_cache: bool = True) -> HealthCheckResponse:\n        \"\"\"\n        Run all health checks.\n        \n        Args:\n            use_cache: Return cached result if within TTL\n        \"\"\"\n        # Return cached result if fresh\n        if use_cache and self._last_result and self._last_check_time:\n            if datetime.utcnow() - self._last_check_time < self._cache_ttl:\n                return self._last_result\n        \n        # Run all checks in parallel\n        results = await asyncio.gather(\n            *[check.run() for check in self._checks],\n            return_exceptions=True\n        )\n        \n        # Process results\n        components = {}\n        overall_status = HealthStatus.HEALTHY\n        \n        for check, result in zip(self._checks, results):\n            if isinstance(result, Exception):\n                result = ComponentHealth(\n                    name=check.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    last_check=datetime.utcnow()\n                )\n            \n            components[result.name] = {\n                'status': result.status.value,\n                'message': result.message,\n                'response_time_ms': result.response_time_ms,\n                'details': result.details\n            }\n            \n            # Update overall status\n            if result.status == HealthStatus.UNHEALTHY:\n                if check.critical:\n                    overall_status = HealthStatus.UNHEALTHY\n                elif overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n            elif result.status == HealthStatus.DEGRADED:\n                if overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n        \n        uptime = (datetime.utcnow() - self._start_time).total_seconds()\n        \n        response = HealthCheckResponse(\n            status=overall_status,\n            timestamp=datetime.utcnow().isoformat() + 'Z',\n            version=self._version,\n            uptime_seconds=uptime,\n            components=components\n        )\n        \n        self._last_result = response\n        self._last_check_time = datetime.utcnow()\n        \n        return response\n    \n    async def is_healthy(self) -> bool:\n        \"\"\"Quick check if system is healthy.\"\"\"\n        result = await self.check_all()\n        return result.status == HealthStatus.HEALTHY\n\n# FastAPI integration\ndef setup_health_routes(app: FastAPI, health_checker: HealthChecker):\n    \"\"\"Add health check routes to FastAPI app.\"\"\"\n    \n    @app.get('/health', response_model=HealthCheckResponse)\n    async def health_check(response: Response):\n        \"\"\"Full health check with component details.\"\"\"\n        result = await health_checker.check_all()\n        \n        if result.status == HealthStatus.UNHEALTHY:\n            response.status_code = 503\n        elif result.status == HealthStatus.DEGRADED:\n            response.status_code = 200  # Still serving requests\n        \n        return result\n    \n    @app.get('/health/live')\n    async def liveness():\n        \"\"\"Kubernetes liveness probe - is the app running?\"\"\"\n        return {'status': 'alive'}\n    \n    @app.get('/health/ready')\n    async def readiness(response: Response):\n        \"\"\"Kubernetes readiness probe - can it serve traffic?\"\"\"\n        is_ready = await health_checker.is_healthy()\n        \n        if not is_ready:\n            response.status_code = 503\n            return {'status': 'not_ready'}\n        \n        return {'status': 'ready'}\n\n# Example check functions:\nasync def check_database(db) -> bool:\n    await db.execute('SELECT 1')\n    return True\n\nasync def check_redis(redis) -> dict:\n    info = await redis.info()\n    return {\n        'connected_clients': info['connected_clients'],\n        'used_memory_mb': info['used_memory'] / 1024 / 1024\n    }\n\ndef check_disk_space() -> bool:\n    import shutil\n    usage = shutil.disk_usage('/')\n    free_percent = usage.free / usage.total * 100\n    \n    if free_percent < 10:\n        raise Exception(f'Low disk space: {free_percent:.1f}% free')\n    \n    return True\n\n# Usage:\napp = FastAPI()\n\nhealth = HealthChecker(app_version='2.1.0')\nhealth.add_check('database', lambda: check_database(db), critical=True)\nhealth.add_check('redis', lambda: check_redis(redis), critical=False)\nhealth.add_check('disk', check_disk_space, timeout=2.0, critical=True)\n\nsetup_health_routes(app, health)\n```\n\nQuality aspects:\n1. **Three status levels** - healthy, degraded, unhealthy\n2. **Critical vs non-critical** - degraded mode support\n3. **Kubernetes-ready** - liveness and readiness probes\n4. **Response caching** - don't overload checks\n5. **Parallel execution** - fast even with many checks\n6. **Detailed component info** - timing, messages, details\n7. **Timeout handling** - prevent hanging checks\n8. **Proper HTTP status codes** - 503 when unhealthy"}
{"instruction": "Write a function that parses a config file and handles errors", "output": "Here's how to write helpful error messages when parsing config:\n\n```python\nfrom pathlib import Path\nfrom typing import Any\nimport json\nimport yaml\n\nclass ConfigError(Exception):\n    \"\"\"Base configuration error with helpful context.\"\"\"\n    pass\n\nclass ConfigFileNotFoundError(ConfigError):\n    \"\"\"Config file doesn't exist.\"\"\"\n    def __init__(self, path: Path, searched_locations: list[Path] = None):\n        self.path = path\n        self.searched_locations = searched_locations or []\n        \n        msg = f\"Config file not found: {path}\"\n        \n        if self.searched_locations:\n            msg += f\"\\n\\nSearched locations:\\n\"\n            for loc in self.searched_locations:\n                exists = \"✓ exists\" if loc.exists() else \"✗ not found\"\n                msg += f\"  - {loc} ({exists})\\n\"\n        \n        msg += f\"\\nTo fix: Create the config file or set CONFIG_PATH environment variable.\"\n        \n        super().__init__(msg)\n\nclass ConfigParseError(ConfigError):\n    \"\"\"Config file has invalid syntax.\"\"\"\n    def __init__(self, path: Path, line: int = None, column: int = None, detail: str = None):\n        self.path = path\n        self.line = line\n        self.column = column\n        \n        msg = f\"Failed to parse config file: {path}\"\n        \n        if line:\n            msg += f\" at line {line}\"\n            if column:\n                msg += f\", column {column}\"\n        \n        if detail:\n            msg += f\"\\n\\nError: {detail}\"\n        \n        # Show the problematic line if possible\n        if line and path.exists():\n            try:\n                lines = path.read_text().splitlines()\n                if 0 < line <= len(lines):\n                    problem_line = lines[line - 1]\n                    msg += f\"\\n\\nProblematic line {line}:\\n  {problem_line}\"\n                    if column:\n                        msg += f\"\\n  {' ' * (column - 1)}^\"\n            except:\n                pass\n        \n        msg += f\"\\n\\nTo fix: Check the syntax of your config file. \"\n        if str(path).endswith('.json'):\n            msg += \"Validate at https://jsonlint.com/\"\n        elif str(path).endswith(('.yml', '.yaml')):\n            msg += \"Validate at https://yamlvalidator.com/\"\n        \n        super().__init__(msg)\n\nclass ConfigValidationError(ConfigError):\n    \"\"\"Config values are invalid.\"\"\"\n    def __init__(self, field: str, value: Any, expected: str, path: Path = None):\n        self.field = field\n        self.value = value\n        self.expected = expected\n        \n        msg = f\"Invalid config value for '{field}'\\n\"\n        msg += f\"  Got: {repr(value)} ({type(value).__name__})\\n\"\n        msg += f\"  Expected: {expected}\\n\"\n        \n        if path:\n            msg += f\"  File: {path}\\n\"\n        \n        # Suggest fixes based on common mistakes\n        if expected == 'int' and isinstance(value, str):\n            if value.isdigit():\n                msg += f\"\\nTo fix: Remove quotes around the number: {field}: {value}\"\n            else:\n                msg += f\"\\nTo fix: Use a valid integer, e.g., {field}: 8080\"\n        elif expected == 'bool' and isinstance(value, str):\n            msg += f\"\\nTo fix: Use true/false without quotes: {field}: true\"\n        elif 'url' in expected.lower() and isinstance(value, str):\n            if not value.startswith(('http://', 'https://')):\n                msg += f\"\\nTo fix: Add protocol: {field}: https://{value}\"\n        \n        super().__init__(msg)\n\nclass ConfigMissingKeyError(ConfigError):\n    \"\"\"Required config key is missing.\"\"\"\n    def __init__(self, key: str, available_keys: list[str] = None, path: Path = None):\n        self.key = key\n        \n        msg = f\"Required config key missing: '{key}'\"\n        \n        if path:\n            msg += f\" in {path}\"\n        \n        if available_keys:\n            # Check for typos\n            similar = self._find_similar(key, available_keys)\n            if similar:\n                msg += f\"\\n\\nDid you mean: '{similar}'?\"\n            \n            msg += f\"\\n\\nAvailable keys: {', '.join(sorted(available_keys))}\"\n        \n        msg += f\"\\n\\nTo fix: Add the '{key}' key to your config file.\"\n        \n        super().__init__(msg)\n    \n    @staticmethod\n    def _find_similar(key: str, candidates: list[str]) -> str:\n        \"\"\"Find most similar key (simple Levenshtein).\"\"\"\n        key_lower = key.lower()\n        for candidate in candidates:\n            if candidate.lower().startswith(key_lower[:3]):\n                return candidate\n            if key_lower in candidate.lower():\n                return candidate\n        return None\n\ndef load_config(path: Path) -> dict:\n    \"\"\"Load and validate config with helpful errors.\"\"\"\n    \n    # Check file exists\n    if not path.exists():\n        search_paths = [\n            Path.cwd() / 'config.json',\n            Path.cwd() / 'config.yml',\n            Path.home() / '.config' / 'myapp' / 'config.json',\n        ]\n        raise ConfigFileNotFoundError(path, search_paths)\n    \n    # Parse file\n    content = path.read_text()\n    \n    try:\n        if path.suffix == '.json':\n            config = json.loads(content)\n        elif path.suffix in ('.yml', '.yaml'):\n            config = yaml.safe_load(content)\n        else:\n            raise ConfigError(f\"Unknown config format: {path.suffix}. Use .json or .yml\")\n    \n    except json.JSONDecodeError as e:\n        raise ConfigParseError(path, e.lineno, e.colno, e.msg)\n    except yaml.YAMLError as e:\n        line = e.problem_mark.line + 1 if hasattr(e, 'problem_mark') else None\n        raise ConfigParseError(path, line, detail=str(e))\n    \n    # Validate required keys\n    required = ['database_url', 'api_port', 'secret_key']\n    for key in required:\n        if key not in config:\n            raise ConfigMissingKeyError(key, list(config.keys()), path)\n    \n    # Validate types\n    if not isinstance(config.get('api_port'), int):\n        raise ConfigValidationError('api_port', config['api_port'], 'int', path)\n    \n    if not config.get('database_url', '').startswith(('postgres://', 'postgresql://')):\n        raise ConfigValidationError(\n            'database_url', \n            config['database_url'], \n            'PostgreSQL URL (postgresql://user:pass@host:port/db)',\n            path\n        )\n    \n    return config\n```\n\nKey error message principles:\n1. **What went wrong** - Clear description of the error\n2. **Where it happened** - File, line, column when possible\n3. **What was expected** - Show valid format/values\n4. **How to fix it** - Actionable next steps\n5. **Context** - Show available options, similar keys, etc."}
{"instruction": "Write an API endpoint that validates input", "output": "Here's how to return helpful validation errors from an API:\n\n```python\nfrom fastapi import FastAPI, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, validator, Field\nfrom typing import Optional, Any\nfrom dataclasses import dataclass\nimport re\n\napp = FastAPI()\n\n@dataclass\nclass APIError:\n    \"\"\"Structured API error response.\"\"\"\n    code: str\n    message: str\n    field: Optional[str] = None\n    received: Optional[Any] = None\n    expected: Optional[str] = None\n    suggestion: Optional[str] = None\n    docs_url: Optional[str] = None\n\nclass APIErrorResponse(BaseModel):\n    \"\"\"Standard error response format.\"\"\"\n    error: str\n    message: str\n    details: list[dict] = []\n    request_id: Optional[str] = None\n    docs: Optional[str] = None\n\nclass CreateUserRequest(BaseModel):\n    email: str = Field(..., description=\"User's email address\")\n    password: str = Field(..., min_length=8, description=\"Minimum 8 characters\")\n    age: Optional[int] = Field(None, ge=13, le=120, description=\"User's age (13-120)\")\n    username: str = Field(..., min_length=3, max_length=20)\n    \n    @validator('email')\n    def validate_email(cls, v):\n        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        if not re.match(pattern, v):\n            raise ValueError(\n                f\"Invalid email format. \"\n                f\"Got '{v}'. \"\n                f\"Expected format: user@example.com\"\n            )\n        return v.lower()\n    \n    @validator('password')\n    def validate_password(cls, v):\n        errors = []\n        \n        if len(v) < 8:\n            errors.append(f\"at least 8 characters (got {len(v)})\")\n        if not re.search(r'[A-Z]', v):\n            errors.append(\"at least one uppercase letter\")\n        if not re.search(r'[a-z]', v):\n            errors.append(\"at least one lowercase letter\")\n        if not re.search(r'\\d', v):\n            errors.append(\"at least one number\")\n        \n        if errors:\n            raise ValueError(\n                f\"Password must contain: {', '.join(errors)}. \"\n                f\"Example of valid password: 'MySecure123'\"\n            )\n        return v\n    \n    @validator('username')\n    def validate_username(cls, v):\n        if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', v):\n            if v[0].isdigit():\n                raise ValueError(\n                    f\"Username cannot start with a number. \"\n                    f\"Got '{v}'. Try '{v[1:]}' or 'user_{v}' instead.\"\n                )\n            invalid_chars = set(re.findall(r'[^a-zA-Z0-9_]', v))\n            raise ValueError(\n                f\"Username contains invalid characters: {invalid_chars}. \"\n                f\"Only letters, numbers, and underscores allowed. \"\n                f\"Try: '{re.sub(r'[^a-zA-Z0-9_]', '_', v)}'\"\n            )\n        return v\n\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request: Request, exc: HTTPException):\n    \"\"\"Format HTTP exceptions with helpful details.\"\"\"\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\n            'error': _get_error_code(exc.status_code),\n            'message': exc.detail,\n            'status_code': exc.status_code,\n            'path': str(request.url.path),\n            'method': request.method,\n            'docs': 'https://api.example.com/docs#errors'\n        }\n    )\n\nfrom fastapi.exceptions import RequestValidationError\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    \"\"\"Transform Pydantic errors into helpful messages.\"\"\"\n    errors = []\n    \n    for error in exc.errors():\n        field = '.'.join(str(loc) for loc in error['loc'] if loc != 'body')\n        msg = error['msg']\n        error_type = error['type']\n        \n        detail = {\n            'field': field,\n            'message': msg,\n            'code': error_type,\n        }\n        \n        # Add context based on error type\n        if error_type == 'value_error.missing':\n            detail['suggestion'] = f\"Add '{field}' to your request body\"\n            detail['example'] = _get_field_example(field)\n        \n        elif error_type == 'type_error.integer':\n            detail['suggestion'] = f\"'{field}' must be a number without quotes\"\n            detail['example'] = f'\"{field}\": 25'\n        \n        elif 'min_length' in error_type:\n            detail['suggestion'] = f\"'{field}' is too short\"\n        \n        elif 'max_length' in error_type:\n            detail['suggestion'] = f\"'{field}' is too long\"\n        \n        errors.append(detail)\n    \n    return JSONResponse(\n        status_code=422,\n        content={\n            'error': 'validation_error',\n            'message': f'Request validation failed with {len(errors)} error(s)',\n            'details': errors,\n            'docs': 'https://api.example.com/docs#create-user',\n            'example_request': {\n                'email': 'user@example.com',\n                'password': 'SecurePass123',\n                'username': 'johndoe',\n                'age': 25\n            }\n        }\n    )\n\ndef _get_error_code(status_code: int) -> str:\n    codes = {\n        400: 'bad_request',\n        401: 'unauthorized',\n        403: 'forbidden',\n        404: 'not_found',\n        409: 'conflict',\n        422: 'validation_error',\n        429: 'rate_limited',\n        500: 'internal_error',\n    }\n    return codes.get(status_code, 'error')\n\ndef _get_field_example(field: str) -> str:\n    examples = {\n        'email': '\"email\": \"user@example.com\"',\n        'password': '\"password\": \"SecurePass123\"',\n        'username': '\"username\": \"johndoe\"',\n        'age': '\"age\": 25',\n    }\n    return examples.get(field, f'\"{field}\": \"...\"')\n\n@app.post('/users')\nasync def create_user(user: CreateUserRequest):\n    # Check if email already exists\n    if await email_exists(user.email):\n        raise HTTPException(\n            status_code=409,\n            detail=(\n                f\"Email '{user.email}' is already registered. \"\n                f\"Did you mean to login? Try POST /auth/login. \"\n                f\"Or use a different email address.\"\n            )\n        )\n    \n    # Check username availability\n    if await username_exists(user.username):\n        suggestions = await get_username_suggestions(user.username)\n        raise HTTPException(\n            status_code=409,\n            detail=(\n                f\"Username '{user.username}' is taken. \"\n                f\"Try one of these: {', '.join(suggestions)}\"\n            )\n        )\n    \n    return {'id': 1, 'email': user.email, 'username': user.username}\n```\n\nKey principles for API error messages:\n1. **Structured format** - Consistent error response schema\n2. **Error codes** - Machine-readable codes for client handling\n3. **Field-level errors** - Point to exact problematic field\n4. **Examples** - Show what valid input looks like\n5. **Suggestions** - Offer alternatives (available usernames, etc.)\n6. **Docs links** - Point to relevant documentation"}
{"instruction": "Write a function that connects to a database", "output": "Here's how to handle database connection errors helpfully:\n\n```python\nimport asyncpg\nimport logging\nfrom typing import Optional\nfrom dataclasses import dataclass\nfrom urllib.parse import urlparse\nimport socket\nimport asyncio\n\nlogger = logging.getLogger(__name__)\n\nclass DatabaseConnectionError(Exception):\n    \"\"\"Base class for database connection errors.\"\"\"\n    pass\n\nclass DatabaseHostUnreachableError(DatabaseConnectionError):\n    \"\"\"Cannot reach the database server.\"\"\"\n    def __init__(self, host: str, port: int, original_error: Exception = None):\n        self.host = host\n        self.port = port\n        \n        msg = f\"Cannot connect to database server at {host}:{port}\\n\"\n        msg += \"\\nPossible causes:\\n\"\n        msg += f\"  1. Database server is not running\\n\"\n        msg += f\"  2. Wrong host/port (check DATABASE_URL)\\n\"\n        msg += f\"  3. Firewall blocking connection\\n\"\n        msg += f\"  4. Network issue\\n\"\n        \n        msg += \"\\nDiagnostic steps:\\n\"\n        msg += f\"  - Check if server is running: pg_isready -h {host} -p {port}\\n\"\n        msg += f\"  - Test connectivity: nc -zv {host} {port}\\n\"\n        msg += f\"  - Check your DATABASE_URL environment variable\\n\"\n        \n        if original_error:\n            msg += f\"\\nOriginal error: {original_error}\"\n        \n        super().__init__(msg)\n\nclass DatabaseAuthenticationError(DatabaseConnectionError):\n    \"\"\"Authentication failed.\"\"\"\n    def __init__(self, user: str, database: str, auth_method: str = None):\n        self.user = user\n        self.database = database\n        \n        msg = f\"Authentication failed for user '{user}' on database '{database}'\\n\"\n        msg += \"\\nPossible causes:\\n\"\n        msg += f\"  1. Wrong password\\n\"\n        msg += f\"  2. User '{user}' doesn't exist\\n\"\n        msg += f\"  3. User lacks permission for database '{database}'\\n\"\n        \n        if auth_method:\n            msg += f\"  4. Auth method mismatch (server expects: {auth_method})\\n\"\n        \n        msg += \"\\nTo fix:\\n\"\n        msg += f\"  - Verify credentials in DATABASE_URL\\n\"\n        msg += f\"  - Check user exists: psql -c \\\"SELECT usename FROM pg_user WHERE usename='{user}'\\\"\\n\"\n        msg += f\"  - Check pg_hba.conf authentication settings\\n\"\n        \n        super().__init__(msg)\n\nclass DatabaseNotFoundError(DatabaseConnectionError):\n    \"\"\"Database doesn't exist.\"\"\"\n    def __init__(self, database: str, available: list[str] = None):\n        self.database = database\n        \n        msg = f\"Database '{database}' does not exist\\n\"\n        \n        if available:\n            msg += f\"\\nAvailable databases: {', '.join(available)}\\n\"\n            \n            # Check for typos\n            for db in available:\n                if db.lower() == database.lower():\n                    msg += f\"\\nDid you mean '{db}'? (case mismatch)\\n\"\n                    break\n        \n        msg += \"\\nTo fix:\\n\"\n        msg += f\"  - Create the database: createdb {database}\\n\"\n        msg += f\"  - Or update DATABASE_URL to use an existing database\\n\"\n        \n        super().__init__(msg)\n\nclass DatabaseSSLError(DatabaseConnectionError):\n    \"\"\"SSL/TLS connection issue.\"\"\"\n    def __init__(self, detail: str, require_ssl: bool = True):\n        msg = f\"SSL connection error: {detail}\\n\"\n        \n        if require_ssl:\n            msg += \"\\nPossible causes:\\n\"\n            msg += \"  1. Server doesn't support SSL\\n\"\n            msg += \"  2. SSL certificate verification failed\\n\"\n            msg += \"  3. Wrong SSL mode in connection string\\n\"\n            msg += \"\\nTo fix:\\n\"\n            msg += \"  - Try adding '?sslmode=require' to DATABASE_URL\\n\"\n            msg += \"  - Or '?sslmode=disable' if SSL not needed (dev only!)\\n\"\n        \n        super().__init__(msg)\n\n@dataclass\nclass DatabaseURL:\n    \"\"\"Parsed database URL with validation.\"\"\"\n    host: str\n    port: int\n    database: str\n    user: str\n    password: str\n    ssl_mode: str = 'prefer'\n    \n    @classmethod\n    def parse(cls, url: str) -> 'DatabaseURL':\n        \"\"\"Parse DATABASE_URL with helpful errors.\"\"\"\n        if not url:\n            raise DatabaseConnectionError(\n                \"DATABASE_URL is empty or not set.\\n\\n\"\n                \"To fix:\\n\"\n                \"  export DATABASE_URL='postgresql://user:pass@localhost:5432/mydb'\\n\\n\"\n                \"Or add to .env file:\\n\"\n                \"  DATABASE_URL=postgresql://user:pass@localhost:5432/mydb\"\n            )\n        \n        if not url.startswith(('postgres://', 'postgresql://')):\n            raise DatabaseConnectionError(\n                f\"Invalid DATABASE_URL format.\\n\\n\"\n                f\"Got: {url[:50]}...\\n\"\n                f\"Expected: postgresql://user:password@host:port/database\\n\\n\"\n                f\"Make sure it starts with 'postgresql://' or 'postgres://'\"\n            )\n        \n        try:\n            parsed = urlparse(url)\n            \n            if not parsed.hostname:\n                raise DatabaseConnectionError(\n                    f\"Missing host in DATABASE_URL.\\n\"\n                    f\"Got: {url}\\n\"\n                    f\"Expected: postgresql://user:pass@HOST:port/db\"\n                )\n            \n            if not parsed.path or parsed.path == '/':\n                raise DatabaseConnectionError(\n                    f\"Missing database name in DATABASE_URL.\\n\"\n                    f\"Got: {url}\\n\"\n                    f\"Expected: postgresql://user:pass@host:port/DATABASE\"\n                )\n            \n            return cls(\n                host=parsed.hostname,\n                port=parsed.port or 5432,\n                database=parsed.path.lstrip('/'),\n                user=parsed.username or 'postgres',\n                password=parsed.password or '',\n                ssl_mode=dict(q.split('=') for q in parsed.query.split('&') if '=' in q).get('sslmode', 'prefer')\n            )\n        \n        except Exception as e:\n            raise DatabaseConnectionError(\n                f\"Cannot parse DATABASE_URL: {e}\\n\\n\"\n                f\"URL: {url[:50]}...\\n\"\n                f\"Format: postgresql://user:password@host:port/database?sslmode=require\"\n            )\n\nasync def connect_database(url: str, timeout: float = 10.0) -> asyncpg.Pool:\n    \"\"\"\n    Connect to database with helpful error messages.\n    \n    Args:\n        url: PostgreSQL connection URL\n        timeout: Connection timeout in seconds\n    \n    Returns:\n        Connection pool\n    \n    Raises:\n        DatabaseConnectionError: With helpful diagnostic info\n    \"\"\"\n    db_url = DatabaseURL.parse(url)\n    \n    # First, check if host is reachable\n    try:\n        sock = socket.create_connection((db_url.host, db_url.port), timeout=5)\n        sock.close()\n    except socket.timeout:\n        raise DatabaseHostUnreachableError(db_url.host, db_url.port, \"Connection timed out\")\n    except socket.error as e:\n        raise DatabaseHostUnreachableError(db_url.host, db_url.port, e)\n    \n    # Try to connect\n    try:\n        pool = await asyncio.wait_for(\n            asyncpg.create_pool(\n                url,\n                min_size=2,\n                max_size=10,\n            ),\n            timeout=timeout\n        )\n        \n        # Test the connection\n        async with pool.acquire() as conn:\n            await conn.execute('SELECT 1')\n        \n        logger.info(f\"Connected to database '{db_url.database}' at {db_url.host}:{db_url.port}\")\n        return pool\n    \n    except asyncio.TimeoutError:\n        raise DatabaseConnectionError(\n            f\"Connection timed out after {timeout}s.\\n\\n\"\n            f\"The server at {db_url.host}:{db_url.port} is reachable but not responding.\\n\"\n            f\"This could mean:\\n\"\n            f\"  - Database is overloaded\\n\"\n            f\"  - Too many connections\\n\"\n            f\"  - Slow authentication (check pg_hba.conf)\\n\"\n        )\n    \n    except asyncpg.InvalidPasswordError:\n        raise DatabaseAuthenticationError(db_url.user, db_url.database)\n    \n    except asyncpg.InvalidCatalogNameError:\n        # Try to list available databases\n        try:\n            temp_conn = await asyncpg.connect(\n                host=db_url.host,\n                port=db_url.port,\n                user=db_url.user,\n                password=db_url.password,\n                database='postgres'\n            )\n            rows = await temp_conn.fetch(\"SELECT datname FROM pg_database WHERE datistemplate = false\")\n            available = [row['datname'] for row in rows]\n            await temp_conn.close()\n        except:\n            available = None\n        \n        raise DatabaseNotFoundError(db_url.database, available)\n    \n    except Exception as e:\n        if 'SSL' in str(e) or 'ssl' in str(e):\n            raise DatabaseSSLError(str(e))\n        \n        raise DatabaseConnectionError(\n            f\"Database connection failed: {e}\\n\\n\"\n            f\"Connection details:\\n\"\n            f\"  Host: {db_url.host}\\n\"\n            f\"  Port: {db_url.port}\\n\"\n            f\"  Database: {db_url.database}\\n\"\n            f\"  User: {db_url.user}\\n\"\n            f\"  SSL: {db_url.ssl_mode}\\n\"\n        )\n```\n\nKey principles:\n1. **Specific exception types** for different failure modes\n2. **Diagnostic info** - what to check, commands to run\n3. **Context** - show the values that failed\n4. **Suggestions** - offer alternatives and fixes\n5. **Pre-flight checks** - verify connectivity before attempting connection"}
{"instruction": "Write a file upload handler", "output": "Here's how to handle file upload errors helpfully:\n\n```python\nfrom fastapi import FastAPI, UploadFile, HTTPException\nfrom pathlib import Path\nfrom typing import Optional\nimport magic\nimport hashlib\n\napp = FastAPI()\n\nclass FileUploadError(Exception):\n    \"\"\"Base file upload error.\"\"\"\n    def __init__(self, message: str, code: str, details: dict = None):\n        self.message = message\n        self.code = code\n        self.details = details or {}\n        super().__init__(message)\n\nclass FileTooLargeError(FileUploadError):\n    \"\"\"File exceeds size limit.\"\"\"\n    def __init__(self, size: int, max_size: int, filename: str):\n        size_mb = size / (1024 * 1024)\n        max_mb = max_size / (1024 * 1024)\n        \n        super().__init__(\n            message=(\n                f\"File '{filename}' is too large ({size_mb:.1f} MB). \"\n                f\"Maximum allowed size is {max_mb:.0f} MB.\\n\\n\"\n                f\"To upload this file:\\n\"\n                f\"  - Compress the file (zip, gzip)\\n\"\n                f\"  - Reduce image resolution if it's an image\\n\"\n                f\"  - Split into smaller parts\\n\"\n                f\"  - Contact support for increased limit\"\n            ),\n            code='file_too_large',\n            details={\n                'size_bytes': size,\n                'max_bytes': max_size,\n                'size_mb': round(size_mb, 2),\n                'max_mb': round(max_mb, 0),\n                'filename': filename\n            }\n        )\n\nclass InvalidFileTypeError(FileUploadError):\n    \"\"\"File type not allowed.\"\"\"\n    def __init__(\n        self, \n        filename: str, \n        detected_type: str, \n        allowed_types: list[str],\n        detected_extension: str = None\n    ):\n        allowed_str = ', '.join(allowed_types)\n        \n        message = f\"File type not allowed for '{filename}'.\\n\\n\"\n        message += f\"  Detected type: {detected_type}\\n\"\n        \n        if detected_extension:\n            message += f\"  File extension: {detected_extension}\\n\"\n            \n            # Check for extension mismatch\n            if detected_extension.lower() not in detected_type.lower():\n                message += f\"\\n⚠️  Warning: Extension doesn't match content type.\\n\"\n                message += f\"   The file claims to be '{detected_extension}' but contains '{detected_type}'.\\n\"\n                message += f\"   This could indicate a renamed file or potential security issue.\\n\"\n        \n        message += f\"\\nAllowed types: {allowed_str}\\n\"\n        \n        # Suggest alternatives\n        if 'image' in detected_type:\n            if 'image/png' in allowed_types or 'image/jpeg' in allowed_types:\n                message += \"\\nTry converting your image to PNG or JPEG format.\"\n        elif 'pdf' in detected_type.lower() and 'application/pdf' not in allowed_types:\n            message += \"\\nPDF files are not accepted. Try converting to an allowed format.\"\n        \n        super().__init__(\n            message=message,\n            code='invalid_file_type',\n            details={\n                'filename': filename,\n                'detected_type': detected_type,\n                'detected_extension': detected_extension,\n                'allowed_types': allowed_types\n            }\n        )\n\nclass EmptyFileError(FileUploadError):\n    \"\"\"File is empty.\"\"\"\n    def __init__(self, filename: str):\n        super().__init__(\n            message=(\n                f\"File '{filename}' is empty (0 bytes).\\n\\n\"\n                f\"This could mean:\\n\"\n                f\"  - The file wasn't selected properly\\n\"\n                f\"  - Upload was interrupted\\n\"\n                f\"  - The original file is empty\\n\\n\"\n                f\"Please select the file again and retry.\"\n            ),\n            code='empty_file',\n            details={'filename': filename}\n        )\n\nclass DuplicateFileError(FileUploadError):\n    \"\"\"File already exists.\"\"\"\n    def __init__(self, filename: str, existing_path: str, existing_id: str = None):\n        message = f\"A file with the same content already exists.\\n\\n\"\n        message += f\"  Uploaded: {filename}\\n\"\n        message += f\"  Existing: {existing_path}\\n\"\n        \n        if existing_id:\n            message += f\"  File ID: {existing_id}\\n\"\n            message += f\"\\nYou can use the existing file instead of uploading again.\"\n        \n        super().__init__(\n            message=message,\n            code='duplicate_file',\n            details={\n                'filename': filename,\n                'existing_path': existing_path,\n                'existing_id': existing_id\n            }\n        )\n\nclass FileUploadConfig:\n    MAX_SIZE = 10 * 1024 * 1024  # 10 MB\n    ALLOWED_TYPES = [\n        'image/jpeg', 'image/png', 'image/gif', 'image/webp',\n        'application/pdf',\n        'text/plain', 'text/csv',\n    ]\n    ALLOWED_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.txt', '.csv']\n\nasync def validate_upload(file: UploadFile) -> tuple[bytes, str]:\n    \"\"\"\n    Validate uploaded file with helpful errors.\n    \n    Returns:\n        Tuple of (file_content, detected_mime_type)\n    \"\"\"\n    filename = file.filename or 'unknown'\n    extension = Path(filename).suffix.lower() if filename else None\n    \n    # Read content\n    content = await file.read()\n    \n    # Check for empty file\n    if len(content) == 0:\n        raise EmptyFileError(filename)\n    \n    # Check size\n    if len(content) > FileUploadConfig.MAX_SIZE:\n        raise FileTooLargeError(len(content), FileUploadConfig.MAX_SIZE, filename)\n    \n    # Detect actual file type (not just extension)\n    detected_type = magic.from_buffer(content, mime=True)\n    \n    # Check if type is allowed\n    if detected_type not in FileUploadConfig.ALLOWED_TYPES:\n        raise InvalidFileTypeError(\n            filename=filename,\n            detected_type=detected_type,\n            allowed_types=FileUploadConfig.ALLOWED_TYPES,\n            detected_extension=extension\n        )\n    \n    # Check extension matches content (security)\n    if extension:\n        expected_extensions = {\n            'image/jpeg': ['.jpg', '.jpeg'],\n            'image/png': ['.png'],\n            'image/gif': ['.gif'],\n            'application/pdf': ['.pdf'],\n        }\n        \n        valid_extensions = expected_extensions.get(detected_type, [])\n        if valid_extensions and extension not in valid_extensions:\n            raise InvalidFileTypeError(\n                filename=filename,\n                detected_type=detected_type,\n                allowed_types=FileUploadConfig.ALLOWED_TYPES,\n                detected_extension=extension\n            )\n    \n    # Check for duplicates (by content hash)\n    content_hash = hashlib.sha256(content).hexdigest()\n    existing = await find_file_by_hash(content_hash)  # Your lookup function\n    \n    if existing:\n        raise DuplicateFileError(\n            filename=filename,\n            existing_path=existing['path'],\n            existing_id=existing['id']\n        )\n    \n    return content, detected_type\n\n@app.post('/upload')\nasync def upload_file(file: UploadFile):\n    \"\"\"Upload a file with validation.\"\"\"\n    try:\n        content, mime_type = await validate_upload(file)\n        \n        # Save file...\n        return {\n            'filename': file.filename,\n            'size': len(content),\n            'type': mime_type,\n            'message': 'Upload successful'\n        }\n    \n    except FileUploadError as e:\n        raise HTTPException(\n            status_code=400,\n            detail={\n                'error': e.code,\n                'message': e.message,\n                'details': e.details\n            }\n        )\n```\n\nKey error handling patterns:\n1. **Size errors** - Show current vs max, suggest compression\n2. **Type errors** - Detect actual type vs extension mismatch\n3. **Empty files** - Explain why this might happen\n4. **Duplicates** - Offer to use existing file\n5. **Security** - Check content matches extension (prevent .exe disguised as .jpg)"}
{"instruction": "Write a function that makes HTTP requests to external APIs", "output": "Here's how to handle external API errors helpfully:\n\n```python\nimport httpx\nimport logging\nfrom typing import Optional, Any, Type\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\n\nlogger = logging.getLogger(__name__)\n\nclass APIErrorCategory(Enum):\n    NETWORK = 'network'\n    AUTHENTICATION = 'authentication'\n    AUTHORIZATION = 'authorization'\n    NOT_FOUND = 'not_found'\n    RATE_LIMITED = 'rate_limited'\n    SERVER_ERROR = 'server_error'\n    INVALID_REQUEST = 'invalid_request'\n    TIMEOUT = 'timeout'\n\n@dataclass\nclass APIErrorContext:\n    \"\"\"Context for API errors.\"\"\"\n    service: str\n    endpoint: str\n    method: str\n    status_code: Optional[int] = None\n    response_body: Optional[str] = None\n    request_id: Optional[str] = None\n\nclass ExternalAPIError(Exception):\n    \"\"\"Base class for external API errors with helpful diagnostics.\"\"\"\n    \n    category: APIErrorCategory = APIErrorCategory.SERVER_ERROR\n    \n    def __init__(self, message: str, context: APIErrorContext, original_error: Exception = None):\n        self.context = context\n        self.original_error = original_error\n        \n        # Build detailed message\n        full_message = f\"{message}\\n\"\n        full_message += f\"\\nAPI Details:\\n\"\n        full_message += f\"  Service: {context.service}\\n\"\n        full_message += f\"  Endpoint: {context.method} {context.endpoint}\\n\"\n        \n        if context.status_code:\n            full_message += f\"  Status: {context.status_code}\\n\"\n        \n        if context.request_id:\n            full_message += f\"  Request ID: {context.request_id}\\n\"\n        \n        if context.response_body:\n            # Try to extract error message from response\n            try:\n                body = json.loads(context.response_body)\n                if 'error' in body:\n                    full_message += f\"  API Error: {body.get('error', {}).get('message', body['error'])}\\n\"\n                elif 'message' in body:\n                    full_message += f\"  API Message: {body['message']}\\n\"\n            except:\n                if len(context.response_body) < 200:\n                    full_message += f\"  Response: {context.response_body}\\n\"\n        \n        full_message += self._get_troubleshooting_steps()\n        \n        super().__init__(full_message)\n    \n    def _get_troubleshooting_steps(self) -> str:\n        return \"\"\n\nclass APIConnectionError(ExternalAPIError):\n    \"\"\"Cannot connect to API server.\"\"\"\n    category = APIErrorCategory.NETWORK\n    \n    def _get_troubleshooting_steps(self) -> str:\n        return (\n            f\"\\nTroubleshooting:\\n\"\n            f\"  1. Check your internet connection\\n\"\n            f\"  2. Verify the API URL is correct\\n\"\n            f\"  3. Check if {self.context.service} is experiencing outages\\n\"\n            f\"  4. Try again in a few seconds\\n\"\n        )\n\nclass APITimeoutError(ExternalAPIError):\n    \"\"\"Request timed out.\"\"\"\n    category = APIErrorCategory.TIMEOUT\n    \n    def __init__(self, context: APIErrorContext, timeout: float, original_error: Exception = None):\n        self.timeout = timeout\n        message = f\"Request to {context.service} timed out after {timeout}s\"\n        super().__init__(message, context, original_error)\n    \n    def _get_troubleshooting_steps(self) -> str:\n        return (\n            f\"\\nTroubleshooting:\\n\"\n            f\"  1. The {self.context.service} server may be slow or overloaded\\n\"\n            f\"  2. Try increasing timeout (current: {self.timeout}s)\\n\"\n            f\"  3. Check if request is too complex (large payload?)\\n\"\n            f\"  4. Retry with exponential backoff\\n\"\n        )\n\nclass APIAuthenticationError(ExternalAPIError):\n    \"\"\"Authentication failed (401).\"\"\"\n    category = APIErrorCategory.AUTHENTICATION\n    \n    def _get_troubleshooting_steps(self) -> str:\n        return (\n            f\"\\nTroubleshooting:\\n\"\n            f\"  1. Check your API key is correct\\n\"\n            f\"  2. Verify the key hasn't expired\\n\"\n            f\"  3. Ensure you're using the right environment (prod vs dev)\\n\"\n            f\"  4. Check API key permissions/scopes\\n\"\n            f\"  5. Regenerate API key if needed\\n\"\n        )\n\nclass APIAuthorizationError(ExternalAPIError):\n    \"\"\"Access denied (403).\"\"\"\n    category = APIErrorCategory.AUTHORIZATION\n    \n    def _get_troubleshooting_steps(self) -> str:\n        return (\n            f\"\\nTroubleshooting:\\n\"\n            f\"  1. Your API key may lack permissions for this endpoint\\n\"\n            f\"  2. Check if your account has access to this feature\\n\"\n            f\"  3. Verify you're not accessing resources owned by others\\n\"\n            f\"  4. Contact {self.context.service} support if you believe this is an error\\n\"\n        )\n\nclass APINotFoundError(ExternalAPIError):\n    \"\"\"Resource not found (404).\"\"\"\n    category = APIErrorCategory.NOT_FOUND\n    \n    def __init__(self, context: APIErrorContext, resource_type: str = None, resource_id: str = None):\n        self.resource_type = resource_type\n        self.resource_id = resource_id\n        \n        message = f\"Resource not found on {context.service}\"\n        if resource_type and resource_id:\n            message = f\"{resource_type} '{resource_id}' not found on {context.service}\"\n        \n        super().__init__(message, context)\n    \n    def _get_troubleshooting_steps(self) -> str:\n        msg = f\"\\nTroubleshooting:\\n\"\n        msg += f\"  1. Verify the endpoint URL is correct\\n\"\n        \n        if self.resource_id:\n            msg += f\"  2. Check that '{self.resource_id}' exists and you have access\\n\"\n            msg += f\"  3. The resource may have been deleted\\n\"\n        else:\n            msg += f\"  2. Check the API documentation for correct endpoint\\n\"\n        \n        msg += f\"  4. API version may have changed - check for deprecations\\n\"\n        return msg\n\nclass APIRateLimitError(ExternalAPIError):\n    \"\"\"Rate limited (429).\"\"\"\n    category = APIErrorCategory.RATE_LIMITED\n    \n    def __init__(self, context: APIErrorContext, retry_after: int = None):\n        self.retry_after = retry_after\n        message = f\"Rate limited by {context.service}\"\n        if retry_after:\n            message += f\" - retry after {retry_after} seconds\"\n        super().__init__(message, context)\n    \n    def _get_troubleshooting_steps(self) -> str:\n        msg = f\"\\nTroubleshooting:\\n\"\n        msg += f\"  1. You've exceeded the API rate limit\\n\"\n        \n        if self.retry_after:\n            msg += f\"  2. Wait {self.retry_after} seconds before retrying\\n\"\n        else:\n            msg += f\"  2. Wait before retrying (start with 60 seconds)\\n\"\n        \n        msg += f\"  3. Implement request batching to reduce API calls\\n\"\n        msg += f\"  4. Consider caching responses\\n\"\n        msg += f\"  5. Upgrade your API plan for higher limits\\n\"\n        return msg\n\nclass APIClient:\n    \"\"\"HTTP client with helpful error handling.\"\"\"\n    \n    def __init__(self, service_name: str, base_url: str, api_key: str = None):\n        self.service_name = service_name\n        self.base_url = base_url.rstrip('/')\n        self.api_key = api_key\n        self.timeout = 30.0\n    \n    def _create_context(self, method: str, endpoint: str, response: httpx.Response = None) -> APIErrorContext:\n        return APIErrorContext(\n            service=self.service_name,\n            endpoint=endpoint,\n            method=method,\n            status_code=response.status_code if response else None,\n            response_body=response.text if response else None,\n            request_id=response.headers.get('x-request-id') if response else None\n        )\n    \n    async def request(\n        self, \n        method: str, \n        endpoint: str, \n        **kwargs\n    ) -> dict:\n        \"\"\"Make API request with helpful error handling.\"\"\"\n        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n        \n        headers = kwargs.pop('headers', {})\n        if self.api_key:\n            headers['Authorization'] = f'Bearer {self.api_key}'\n        \n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.request(\n                    method, \n                    url, \n                    headers=headers,\n                    timeout=self.timeout,\n                    **kwargs\n                )\n        \n        except httpx.ConnectError as e:\n            raise APIConnectionError(\n                f\"Cannot connect to {self.service_name}\",\n                self._create_context(method, endpoint),\n                e\n            )\n        \n        except httpx.TimeoutException as e:\n            raise APITimeoutError(\n                self._create_context(method, endpoint),\n                self.timeout,\n                e\n            )\n        \n        context = self._create_context(method, endpoint, response)\n        \n        # Handle error status codes\n        if response.status_code == 401:\n            raise APIAuthenticationError(\n                \"Authentication failed - check your API key\",\n                context\n            )\n        \n        elif response.status_code == 403:\n            raise APIAuthorizationError(\n                \"Access denied - insufficient permissions\",\n                context\n            )\n        \n        elif response.status_code == 404:\n            raise APINotFoundError(context)\n        \n        elif response.status_code == 429:\n            retry_after = int(response.headers.get('Retry-After', 60))\n            raise APIRateLimitError(context, retry_after)\n        \n        elif response.status_code >= 500:\n            raise ExternalAPIError(\n                f\"{self.service_name} server error ({response.status_code})\",\n                context\n            )\n        \n        elif response.status_code >= 400:\n            raise ExternalAPIError(\n                f\"Bad request to {self.service_name} ({response.status_code})\",\n                context\n            )\n        \n        return response.json()\n\n# Usage:\nstripe = APIClient('Stripe', 'https://api.stripe.com/v1', api_key='sk_test_...')\n\ntry:\n    customer = await stripe.request('GET', '/customers/cus_123')\nexcept APINotFoundError as e:\n    print(e)  # Helpful message with troubleshooting steps\nexcept APIRateLimitError as e:\n    print(f\"Rate limited, retry in {e.retry_after}s\")\n```\n\nKey patterns:\n1. **Categorized errors** - Different exception types for different failures\n2. **Rich context** - Service name, endpoint, status code, request ID\n3. **Troubleshooting steps** - Actionable guidance for each error type\n4. **Retry info** - Rate limit errors include wait time\n5. **Original error preservation** - For debugging"}
{"instruction": "Write a CLI tool that accepts arguments", "output": "Here's how to provide helpful CLI error messages:\n\n```python\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom typing import Optional, List\nimport os\n\nclass CLIError(Exception):\n    \"\"\"CLI error with helpful message and exit code.\"\"\"\n    def __init__(self, message: str, exit_code: int = 1, suggestions: List[str] = None):\n        self.exit_code = exit_code\n        self.suggestions = suggestions or []\n        super().__init__(message)\n    \n    def format_message(self) -> str:\n        msg = f\"Error: {self.args[0]}\\n\"\n        if self.suggestions:\n            msg += \"\\nDid you mean:\\n\"\n            for suggestion in self.suggestions:\n                msg += f\"  {suggestion}\\n\"\n        return msg\n\nclass HelpfulArgumentParser(argparse.ArgumentParser):\n    \"\"\"ArgumentParser with better error messages.\"\"\"\n    \n    def error(self, message: str):\n        \"\"\"Override to provide more helpful errors.\"\"\"\n        self.print_usage(sys.stderr)\n        \n        # Enhance common error messages\n        if 'the following arguments are required' in message:\n            args = message.split(':')[1].strip()\n            sys.stderr.write(f\"\\nError: Missing required argument(s): {args}\\n\")\n            sys.stderr.write(f\"\\nRun '{self.prog} --help' to see all options.\\n\")\n        \n        elif 'invalid choice' in message:\n            # Extract the invalid value and valid choices\n            parts = message.split(\"'\")\n            if len(parts) >= 2:\n                invalid = parts[1]\n                sys.stderr.write(f\"\\nError: '{invalid}' is not a valid choice.\\n\")\n                \n                # Find similar options\n                if 'choose from' in message:\n                    choices_str = message.split('choose from')[1]\n                    sys.stderr.write(f\"\\nValid options are:{choices_str}\\n\")\n                    \n                    # Suggest similar\n                    similar = self._find_similar(invalid, choices_str)\n                    if similar:\n                        sys.stderr.write(f\"\\nDid you mean: {similar}?\\n\")\n        \n        elif 'unrecognized arguments' in message:\n            unknown = message.replace('unrecognized arguments:', '').strip()\n            sys.stderr.write(f\"\\nError: Unknown argument(s): {unknown}\\n\")\n            \n            # Suggest similar known arguments\n            for arg in unknown.split():\n                if arg.startswith('-'):\n                    similar = self._find_similar_option(arg)\n                    if similar:\n                        sys.stderr.write(f\"  Did you mean: {similar}?\\n\")\n            \n            sys.stderr.write(f\"\\nRun '{self.prog} --help' to see all options.\\n\")\n        \n        else:\n            sys.stderr.write(f\"\\nError: {message}\\n\")\n        \n        sys.exit(2)\n    \n    def _find_similar(self, value: str, choices_str: str) -> Optional[str]:\n        \"\"\"Find similar choice.\"\"\"\n        choices = [c.strip().strip(\"'\") for c in choices_str.split(',')]\n        value_lower = value.lower()\n        \n        for choice in choices:\n            if choice.lower().startswith(value_lower[:2]):\n                return choice\n            if value_lower in choice.lower():\n                return choice\n        return None\n    \n    def _find_similar_option(self, arg: str) -> Optional[str]:\n        \"\"\"Find similar option from registered options.\"\"\"\n        arg_lower = arg.lower().lstrip('-')\n        \n        for action in self._actions:\n            for opt in action.option_strings:\n                opt_lower = opt.lower().lstrip('-')\n                if opt_lower.startswith(arg_lower[:3]):\n                    return opt\n                if arg_lower in opt_lower:\n                    return opt\n        return None\n\ndef validate_file_path(path_str: str, must_exist: bool = True, file_type: str = None) -> Path:\n    \"\"\"Validate file path with helpful errors.\"\"\"\n    path = Path(path_str).expanduser().resolve()\n    \n    if must_exist and not path.exists():\n        error_msg = f\"File not found: {path_str}\"\n        suggestions = []\n        \n        # Check if it's a relative path issue\n        cwd = Path.cwd()\n        if not path_str.startswith(('/', '~')):\n            suggestions.append(f\"Check your current directory: {cwd}\")\n        \n        # Check for typos in filename\n        parent = path.parent\n        if parent.exists():\n            similar_files = [f.name for f in parent.iterdir() \n                           if f.name.lower().startswith(path.name.lower()[:3])]\n            if similar_files:\n                suggestions.append(f\"Similar files in {parent}: {', '.join(similar_files[:3])}\")\n        \n        # Check if parent directory exists\n        if not parent.exists():\n            suggestions.append(f\"Directory does not exist: {parent}\")\n        \n        raise CLIError(error_msg, suggestions=suggestions)\n    \n    if file_type:\n        suffix = path.suffix.lower()\n        expected = f'.{file_type.lower().lstrip(\".\")}'\n        \n        if suffix != expected:\n            raise CLIError(\n                f\"Expected {file_type} file, got '{suffix or 'no extension'}'\",\n                suggestions=[\n                    f\"Use a file with {expected} extension\",\n                    f\"Your file: {path.name}\"\n                ]\n            )\n    \n    return path\n\ndef validate_output_path(path_str: str) -> Path:\n    \"\"\"Validate output path with helpful errors.\"\"\"\n    path = Path(path_str).expanduser().resolve()\n    \n    # Check parent directory exists\n    if not path.parent.exists():\n        raise CLIError(\n            f\"Output directory does not exist: {path.parent}\",\n            suggestions=[\n                f\"Create it: mkdir -p {path.parent}\",\n                \"Or specify a different output path\"\n            ]\n        )\n    \n    # Check write permission\n    if path.parent.exists() and not os.access(path.parent, os.W_OK):\n        raise CLIError(\n            f\"No write permission for directory: {path.parent}\",\n            suggestions=[\n                f\"Change permissions: chmod u+w {path.parent}\",\n                \"Or specify a different output path\"\n            ]\n        )\n    \n    # Warn if file exists\n    if path.exists():\n        print(f\"Warning: Output file exists and will be overwritten: {path}\", file=sys.stderr)\n    \n    return path\n\ndef create_parser() -> HelpfulArgumentParser:\n    parser = HelpfulArgumentParser(\n        prog='mytool',\n        description='Process data files with helpful error messages.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s input.csv -o output.json\n  %(prog)s data.csv --format json --verbose\n  %(prog)s ~/data/input.csv -o ./results/output.json\n\nFor more help, visit: https://docs.example.com/mytool\n\"\"\"\n    )\n    \n    parser.add_argument(\n        'input',\n        help='Input file path (CSV format)'\n    )\n    \n    parser.add_argument(\n        '-o', '--output',\n        required=True,\n        help='Output file path'\n    )\n    \n    parser.add_argument(\n        '-f', '--format',\n        choices=['json', 'csv', 'parquet'],\n        default='json',\n        help='Output format (default: json)'\n    )\n    \n    parser.add_argument(\n        '-v', '--verbose',\n        action='store_true',\n        help='Enable verbose output'\n    )\n    \n    parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Show what would be done without making changes'\n    )\n    \n    return parser\n\ndef main():\n    parser = create_parser()\n    args = parser.parse_args()\n    \n    try:\n        # Validate inputs\n        input_path = validate_file_path(args.input, must_exist=True, file_type='csv')\n        output_path = validate_output_path(args.output)\n        \n        if args.verbose:\n            print(f\"Input: {input_path}\")\n            print(f\"Output: {output_path}\")\n            print(f\"Format: {args.format}\")\n        \n        if args.dry_run:\n            print(f\"Would process {input_path} -> {output_path}\")\n            return 0\n        \n        # Do actual work...\n        print(f\"Processing complete: {output_path}\")\n        return 0\n    \n    except CLIError as e:\n        print(e.format_message(), file=sys.stderr)\n        return e.exit_code\n    \n    except KeyboardInterrupt:\n        print(\"\\nOperation cancelled by user.\", file=sys.stderr)\n        return 130\n    \n    except Exception as e:\n        print(f\"\\nUnexpected error: {e}\", file=sys.stderr)\n        print(\"\\nIf this persists, please report at: https://github.com/example/mytool/issues\", file=sys.stderr)\n        return 1\n\nif __name__ == '__main__':\n    sys.exit(main())\n```\n\nCLI error message principles:\n1. **Show what's wrong** - Clear error description\n2. **Suggest fixes** - \"Did you mean...?\"\n3. **Show context** - Current directory, similar files\n4. **Provide examples** - In help text\n5. **Exit codes** - Proper codes for different errors (2 for usage errors)\n6. **Validation order** - Check early, fail fast"}
{"instruction": "Write code that processes user data", "output": "Here's how to handle data processing errors helpfully:\n\n```python\nfrom typing import Any, Optional, TypeVar, Generic\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport traceback\n\nT = TypeVar('T')\n\nclass ErrorSeverity(Enum):\n    WARNING = 'warning'    # Processing can continue\n    ERROR = 'error'        # Item skipped\n    CRITICAL = 'critical'  # Processing must stop\n\n@dataclass\nclass ProcessingError:\n    \"\"\"Detailed error for a single item.\"\"\"\n    field: Optional[str]\n    message: str\n    severity: ErrorSeverity\n    value: Any = None\n    suggestion: Optional[str] = None\n    row_number: Optional[int] = None\n    \n    def format(self) -> str:\n        parts = []\n        \n        if self.row_number:\n            parts.append(f\"Row {self.row_number}\")\n        if self.field:\n            parts.append(f\"field '{self.field}'\")\n        \n        location = \", \".join(parts)\n        msg = f\"[{self.severity.value.upper()}] \"\n        \n        if location:\n            msg += f\"{location}: \"\n        \n        msg += self.message\n        \n        if self.value is not None:\n            msg += f\" (got: {repr(self.value)[:50]})\"\n        \n        if self.suggestion:\n            msg += f\"\\n  → {self.suggestion}\"\n        \n        return msg\n\n@dataclass\nclass ProcessingResult(Generic[T]):\n    \"\"\"Result of processing with detailed errors.\"\"\"\n    success: bool\n    data: Optional[T] = None\n    errors: list[ProcessingError] = field(default_factory=list)\n    warnings: list[ProcessingError] = field(default_factory=list)\n    \n    def add_error(self, **kwargs):\n        kwargs['severity'] = ErrorSeverity.ERROR\n        self.errors.append(ProcessingError(**kwargs))\n        self.success = False\n    \n    def add_warning(self, **kwargs):\n        kwargs['severity'] = ErrorSeverity.WARNING\n        self.warnings.append(ProcessingError(**kwargs))\n    \n    def format_errors(self) -> str:\n        lines = []\n        \n        if self.errors:\n            lines.append(f\"\\n{len(self.errors)} error(s):\")\n            for err in self.errors:\n                lines.append(f\"  • {err.format()}\")\n        \n        if self.warnings:\n            lines.append(f\"\\n{len(self.warnings)} warning(s):\")\n            for warn in self.warnings:\n                lines.append(f\"  • {warn.format()}\")\n        \n        return \"\\n\".join(lines)\n\n@dataclass\nclass BatchResult(Generic[T]):\n    \"\"\"Result of processing multiple items.\"\"\"\n    total: int\n    successful: int\n    failed: int\n    results: list[ProcessingResult[T]]\n    \n    @property\n    def success_rate(self) -> float:\n        return self.successful / self.total if self.total > 0 else 0\n    \n    def format_summary(self) -> str:\n        msg = f\"\\nProcessing Summary:\\n\"\n        msg += f\"  Total: {self.total}\\n\"\n        msg += f\"  Successful: {self.successful} ({self.success_rate:.1%})\\n\"\n        msg += f\"  Failed: {self.failed}\\n\"\n        \n        # Group errors by type\n        error_counts = {}\n        for result in self.results:\n            for error in result.errors:\n                key = (error.field, error.message.split('.')[0])\n                error_counts[key] = error_counts.get(key, 0) + 1\n        \n        if error_counts:\n            msg += f\"\\nMost common errors:\\n\"\n            for (field, error_type), count in sorted(error_counts.items(), key=lambda x: -x[1])[:5]:\n                msg += f\"  • {field or 'general'}: {error_type} ({count} occurrences)\\n\"\n        \n        return msg\n\nclass UserDataProcessor:\n    \"\"\"Process user data with helpful error messages.\"\"\"\n    \n    VALID_ROLES = ['admin', 'user', 'moderator', 'guest']\n    VALID_STATUSES = ['active', 'inactive', 'pending', 'suspended']\n    \n    def process_user(self, data: dict, row_number: int = None) -> ProcessingResult[dict]:\n        result = ProcessingResult(success=True)\n        processed = {}\n        \n        # Email validation\n        email = data.get('email')\n        if not email:\n            result.add_error(\n                field='email',\n                message='Email is required',\n                row_number=row_number,\n                suggestion='Every user must have an email address'\n            )\n        elif not isinstance(email, str):\n            result.add_error(\n                field='email',\n                message='Email must be a string',\n                value=email,\n                row_number=row_number,\n                suggestion=f\"Convert to string: '{str(email)}'\"\n            )\n        elif '@' not in email:\n            result.add_error(\n                field='email',\n                message='Invalid email format - missing @',\n                value=email,\n                row_number=row_number,\n                suggestion=f\"Did you mean: {email}@company.com?\"\n            )\n        elif not email.split('@')[1].count('.') >= 1:\n            result.add_error(\n                field='email',\n                message='Invalid email domain',\n                value=email,\n                row_number=row_number,\n                suggestion='Email domain should contain at least one dot (e.g., @example.com)'\n            )\n        else:\n            processed['email'] = email.lower().strip()\n        \n        # Age validation\n        age = data.get('age')\n        if age is not None:\n            if isinstance(age, str):\n                if age.isdigit():\n                    result.add_warning(\n                        field='age',\n                        message='Age provided as string, converting to integer',\n                        value=age,\n                        row_number=row_number\n                    )\n                    age = int(age)\n                else:\n                    result.add_error(\n                        field='age',\n                        message='Age must be a number',\n                        value=age,\n                        row_number=row_number,\n                        suggestion=\"Remove quotes and non-numeric characters\"\n                    )\n                    age = None\n            \n            if age is not None:\n                if not isinstance(age, (int, float)):\n                    result.add_error(\n                        field='age',\n                        message=f'Age must be a number, got {type(age).__name__}',\n                        value=age,\n                        row_number=row_number\n                    )\n                elif age < 0:\n                    result.add_error(\n                        field='age',\n                        message='Age cannot be negative',\n                        value=age,\n                        row_number=row_number,\n                        suggestion='Use absolute value or check data source'\n                    )\n                elif age > 150:\n                    result.add_error(\n                        field='age',\n                        message='Age seems unrealistic (>150)',\n                        value=age,\n                        row_number=row_number,\n                        suggestion='Check if this is years or another unit'\n                    )\n                else:\n                    processed['age'] = int(age)\n        \n        # Role validation\n        role = data.get('role')\n        if role:\n            role_lower = str(role).lower().strip()\n            if role_lower not in self.VALID_ROLES:\n                # Find similar\n                similar = self._find_similar(role_lower, self.VALID_ROLES)\n                suggestion = f\"Did you mean: '{similar}'?\" if similar else f\"Valid roles: {', '.join(self.VALID_ROLES)}\"\n                \n                result.add_error(\n                    field='role',\n                    message=f\"Invalid role: '{role}'\",\n                    value=role,\n                    row_number=row_number,\n                    suggestion=suggestion\n                )\n            else:\n                processed['role'] = role_lower\n        else:\n            processed['role'] = 'user'  # Default\n            result.add_warning(\n                field='role',\n                message=\"No role specified, defaulting to 'user'\",\n                row_number=row_number\n            )\n        \n        # Status validation  \n        status = data.get('status')\n        if status:\n            status_lower = str(status).lower().strip()\n            if status_lower not in self.VALID_STATUSES:\n                similar = self._find_similar(status_lower, self.VALID_STATUSES)\n                suggestion = f\"Did you mean: '{similar}'?\" if similar else f\"Valid statuses: {', '.join(self.VALID_STATUSES)}\"\n                \n                result.add_error(\n                    field='status',\n                    message=f\"Invalid status: '{status}'\",\n                    value=status,\n                    row_number=row_number,\n                    suggestion=suggestion\n                )\n            else:\n                processed['status'] = status_lower\n        else:\n            processed['status'] = 'pending'  # Default for new users\n        \n        if result.success:\n            result.data = processed\n        \n        return result\n    \n    def process_batch(self, users: list[dict]) -> BatchResult[dict]:\n        \"\"\"Process multiple users with summary.\"\"\"\n        results = []\n        successful = 0\n        \n        for i, user_data in enumerate(users, start=1):\n            try:\n                result = self.process_user(user_data, row_number=i)\n                results.append(result)\n                if result.success:\n                    successful += 1\n            except Exception as e:\n                # Catch unexpected errors\n                result = ProcessingResult(success=False)\n                result.add_error(\n                    field=None,\n                    message=f'Unexpected error: {str(e)}',\n                    row_number=i,\n                    suggestion='Check data format. This may be a bug - please report it.'\n                )\n                results.append(result)\n        \n        return BatchResult(\n            total=len(users),\n            successful=successful,\n            failed=len(users) - successful,\n            results=results\n        )\n    \n    @staticmethod\n    def _find_similar(value: str, options: list[str]) -> Optional[str]:\n        \"\"\"Find most similar option.\"\"\"\n        value = value.lower()\n        for opt in options:\n            if opt.startswith(value[:2]):\n                return opt\n            if value in opt:\n                return opt\n        return None\n\n# Usage:\nprocessor = UserDataProcessor()\n\nusers = [\n    {'email': 'valid@example.com', 'age': 25, 'role': 'admin'},\n    {'email': 'invalid-email', 'age': -5, 'role': 'superuser'},\n    {'email': 'user@test.com', 'age': '30', 'role': 'usr'},  # typo in role\n    {'age': 25},  # missing email\n]\n\nresult = processor.process_batch(users)\nprint(result.format_summary())\n\nfor i, r in enumerate(result.results, 1):\n    if not r.success:\n        print(f\"\\nUser {i} failed:{r.format_errors()}\")\n```\n\nData processing error principles:\n1. **Field-level errors** - Point to exact problematic field\n2. **Row numbers** - For batch processing, know which row failed\n3. **Warnings vs errors** - Some issues are recoverable\n4. **Suggestions** - Offer fixes based on common mistakes\n5. **Similar matches** - \"Did you mean...?\" for typos\n6. **Batch summaries** - Aggregate stats and common errors\n7. **Default values** - Use sensible defaults with warnings"}
