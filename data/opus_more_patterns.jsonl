{"instruction": "Write a REST API endpoint for user registration", "output": "```python\nfrom fastapi import APIRouter, HTTPException, status, Depends\nfrom pydantic import BaseModel, EmailStr, validator\nfrom sqlalchemy.orm import Session\nimport re\n\nfrom app.database import get_db\nfrom app.models import User\nfrom app.auth import hash_password, create_access_token\n\nrouter = APIRouter(prefix=\"/auth\", tags=[\"authentication\"])\n\nclass RegisterRequest(BaseModel):\n    email: EmailStr\n    password: str\n    name: str\n    \n    @validator('password')\n    def validate_password(cls, v):\n        if len(v) < 8:\n            raise ValueError('Password must be at least 8 characters')\n        if not re.search(r'[A-Z]', v):\n            raise ValueError('Password must contain uppercase letter')\n        if not re.search(r'[0-9]', v):\n            raise ValueError('Password must contain a number')\n        return v\n    \n    @validator('name')\n    def validate_name(cls, v):\n        if len(v.strip()) < 2:\n            raise ValueError('Name must be at least 2 characters')\n        return v.strip()\n\nclass RegisterResponse(BaseModel):\n    id: int\n    email: str\n    name: str\n    access_token: str\n    token_type: str = \"bearer\"\n\n@router.post(\"/register\", response_model=RegisterResponse, status_code=status.HTTP_201_CREATED)\nasync def register(request: RegisterRequest, db: Session = Depends(get_db)):\n    # Check if email exists\n    existing = db.query(User).filter(User.email == request.email.lower()).first()\n    if existing:\n        raise HTTPException(\n            status_code=status.HTTP_409_CONFLICT,\n            detail=f\"Email '{request.email}' is already registered. Try logging in instead.\"\n        )\n    \n    # Create user\n    user = User(\n        email=request.email.lower(),\n        hashed_password=hash_password(request.password),\n        name=request.name,\n        is_active=True\n    )\n    \n    db.add(user)\n    db.commit()\n    db.refresh(user)\n    \n    # Generate token\n    token = create_access_token(user_id=user.id)\n    \n    return RegisterResponse(\n        id=user.id,\n        email=user.email,\n        name=user.name,\n        access_token=token\n    )\n```\n\nKey aspects:\n1. Pydantic validation with custom validators\n2. Email normalization (lowercase)\n3. Conflict detection with helpful message\n4. Returns token immediately (no separate login needed)\n5. Proper HTTP status codes"}
{"instruction": "Write a function to send emails with templates", "output": "```python\nfrom pathlib import Path\nfrom typing import Optional, Any\nfrom dataclasses import dataclass\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom jinja2 import Environment, FileSystemLoader, TemplateNotFound\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass EmailConfig:\n    smtp_host: str\n    smtp_port: int\n    username: str\n    password: str\n    from_email: str\n    from_name: str = \"MyApp\"\n    use_tls: bool = True\n\nclass EmailError(Exception):\n    \"\"\"Email sending failed.\"\"\"\n    pass\n\nclass TemplateNotFoundError(EmailError):\n    \"\"\"Email template not found.\"\"\"\n    def __init__(self, template_name: str, searched_paths: list[Path]):\n        msg = f\"Email template '{template_name}' not found.\\n\"\n        msg += f\"Searched in: {', '.join(str(p) for p in searched_paths)}\\n\"\n        msg += f\"Available templates: Create '{template_name}.html' in templates/email/\"\n        super().__init__(msg)\n\nclass EmailService:\n    def __init__(self, config: EmailConfig, template_dir: Path = None):\n        self.config = config\n        self.template_dir = template_dir or Path(\"templates/email\")\n        \n        self._env = Environment(\n            loader=FileSystemLoader(self.template_dir),\n            autoescape=True\n        )\n    \n    def _render_template(self, template_name: str, context: dict) -> tuple[str, str]:\n        \"\"\"Render HTML and plain text versions.\"\"\"\n        try:\n            html_template = self._env.get_template(f\"{template_name}.html\")\n            html_content = html_template.render(**context)\n        except TemplateNotFound:\n            raise TemplateNotFoundError(template_name, [self.template_dir])\n        \n        # Try to get plain text version, fall back to stripping HTML\n        try:\n            text_template = self._env.get_template(f\"{template_name}.txt\")\n            text_content = text_template.render(**context)\n        except TemplateNotFound:\n            # Simple HTML to text conversion\n            import re\n            text_content = re.sub(r'<[^>]+>', '', html_content)\n            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n        \n        return html_content, text_content\n    \n    def send(\n        self,\n        to: str | list[str],\n        subject: str,\n        template: str,\n        context: dict[str, Any] = None,\n        reply_to: Optional[str] = None\n    ) -> bool:\n        \"\"\"Send templated email.\"\"\"\n        context = context or {}\n        recipients = [to] if isinstance(to, str) else to\n        \n        # Render template\n        html_content, text_content = self._render_template(template, context)\n        \n        # Build message\n        msg = MIMEMultipart(\"alternative\")\n        msg[\"Subject\"] = subject\n        msg[\"From\"] = f\"{self.config.from_name} <{self.config.from_email}>\"\n        msg[\"To\"] = \", \".join(recipients)\n        \n        if reply_to:\n            msg[\"Reply-To\"] = reply_to\n        \n        msg.attach(MIMEText(text_content, \"plain\"))\n        msg.attach(MIMEText(html_content, \"html\"))\n        \n        # Send\n        try:\n            with smtplib.SMTP(self.config.smtp_host, self.config.smtp_port) as server:\n                if self.config.use_tls:\n                    server.starttls()\n                server.login(self.config.username, self.config.password)\n                server.sendmail(self.config.from_email, recipients, msg.as_string())\n            \n            logger.info(f\"Email sent to {recipients}: {subject}\")\n            return True\n        \n        except smtplib.SMTPAuthenticationError:\n            raise EmailError(\n                \"SMTP authentication failed. Check your email credentials.\\n\"\n                \"If using Gmail, you may need an App Password.\"\n            )\n        except smtplib.SMTPRecipientsRefused as e:\n            raise EmailError(f\"Recipients refused: {e.recipients}\")\n        except Exception as e:\n            raise EmailError(f\"Failed to send email: {e}\")\n    \n    # Convenience methods for common emails\n    def send_welcome(self, to: str, name: str):\n        return self.send(to, \"Welcome!\", \"welcome\", {\"name\": name})\n    \n    def send_password_reset(self, to: str, reset_link: str):\n        return self.send(to, \"Reset Your Password\", \"password_reset\", {\"reset_link\": reset_link})\n    \n    def send_order_confirmation(self, to: str, order: dict):\n        return self.send(to, f\"Order #{order['id']} Confirmed\", \"order_confirmation\", {\"order\": order})\n```\n\nKey aspects:\n1. Template rendering with Jinja2\n2. HTML + plain text fallback\n3. Custom exceptions with helpful messages\n4. TLS support\n5. Convenience methods for common emails"}
{"instruction": "Create a background job scheduler", "output": "```python\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Callable, Optional, Any\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport logging\nimport uuid\nfrom croniter import croniter\n\nlogger = logging.getLogger(__name__)\n\nclass JobStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass Job:\n    id: str\n    name: str\n    func: Callable\n    args: tuple = ()\n    kwargs: dict = field(default_factory=dict)\n    schedule: Optional[str] = None  # Cron expression\n    interval: Optional[timedelta] = None\n    run_at: Optional[datetime] = None\n    status: JobStatus = JobStatus.PENDING\n    last_run: Optional[datetime] = None\n    next_run: Optional[datetime] = None\n    error: Optional[str] = None\n    retries: int = 0\n    max_retries: int = 3\n\nclass Scheduler:\n    def __init__(self):\n        self._jobs: dict[str, Job] = {}\n        self._running = False\n        self._task: Optional[asyncio.Task] = None\n    \n    def add_job(\n        self,\n        func: Callable,\n        name: str = None,\n        schedule: str = None,\n        interval: timedelta = None,\n        run_at: datetime = None,\n        args: tuple = (),\n        kwargs: dict = None,\n        max_retries: int = 3\n    ) -> str:\n        \"\"\"Add a job to the scheduler.\n        \n        Args:\n            func: Function to run\n            name: Job name for logging\n            schedule: Cron expression (e.g., \"0 9 * * *\" for daily 9am)\n            interval: Run every X time (e.g., timedelta(hours=1))\n            run_at: Run once at specific time\n            args: Positional arguments for func\n            kwargs: Keyword arguments for func\n            max_retries: Retry count on failure\n        \n        Returns:\n            Job ID\n        \"\"\"\n        job_id = str(uuid.uuid4())[:8]\n        \n        job = Job(\n            id=job_id,\n            name=name or func.__name__,\n            func=func,\n            args=args,\n            kwargs=kwargs or {},\n            schedule=schedule,\n            interval=interval,\n            run_at=run_at,\n            max_retries=max_retries\n        )\n        \n        # Calculate next run time\n        job.next_run = self._calculate_next_run(job)\n        \n        self._jobs[job_id] = job\n        logger.info(f\"Job '{job.name}' ({job_id}) scheduled for {job.next_run}\")\n        \n        return job_id\n    \n    def _calculate_next_run(self, job: Job) -> Optional[datetime]:\n        now = datetime.now()\n        \n        if job.run_at:\n            return job.run_at if job.run_at > now else None\n        \n        if job.schedule:\n            cron = croniter(job.schedule, now)\n            return cron.get_next(datetime)\n        \n        if job.interval:\n            if job.last_run:\n                return job.last_run + job.interval\n            return now + job.interval\n        \n        return None\n    \n    def remove_job(self, job_id: str) -> bool:\n        if job_id in self._jobs:\n            del self._jobs[job_id]\n            logger.info(f\"Job {job_id} removed\")\n            return True\n        return False\n    \n    def get_job(self, job_id: str) -> Optional[Job]:\n        return self._jobs.get(job_id)\n    \n    def list_jobs(self) -> list[dict]:\n        return [\n            {\n                \"id\": job.id,\n                \"name\": job.name,\n                \"status\": job.status.value,\n                \"next_run\": job.next_run.isoformat() if job.next_run else None,\n                \"last_run\": job.last_run.isoformat() if job.last_run else None,\n            }\n            for job in self._jobs.values()\n        ]\n    \n    async def _run_job(self, job: Job):\n        job.status = JobStatus.RUNNING\n        logger.info(f\"Running job '{job.name}' ({job.id})\")\n        \n        try:\n            if asyncio.iscoroutinefunction(job.func):\n                await job.func(*job.args, **job.kwargs)\n            else:\n                await asyncio.to_thread(job.func, *job.args, **job.kwargs)\n            \n            job.status = JobStatus.COMPLETED\n            job.error = None\n            job.retries = 0\n            logger.info(f\"Job '{job.name}' completed\")\n        \n        except Exception as e:\n            job.retries += 1\n            job.error = str(e)\n            \n            if job.retries < job.max_retries:\n                job.status = JobStatus.PENDING\n                logger.warning(f\"Job '{job.name}' failed, retry {job.retries}/{job.max_retries}: {e}\")\n            else:\n                job.status = JobStatus.FAILED\n                logger.error(f\"Job '{job.name}' failed permanently: {e}\")\n        \n        finally:\n            job.last_run = datetime.now()\n            job.next_run = self._calculate_next_run(job)\n    \n    async def _loop(self):\n        while self._running:\n            now = datetime.now()\n            \n            for job in list(self._jobs.values()):\n                if job.status in (JobStatus.PENDING, JobStatus.COMPLETED):\n                    if job.next_run and job.next_run <= now:\n                        asyncio.create_task(self._run_job(job))\n            \n            await asyncio.sleep(1)\n    \n    async def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        self._task = asyncio.create_task(self._loop())\n        logger.info(\"Scheduler started\")\n    \n    async def stop(self):\n        self._running = False\n        if self._task:\n            self._task.cancel()\n        logger.info(\"Scheduler stopped\")\n\n# Usage:\nscheduler = Scheduler()\n\n# Daily at 9am\nscheduler.add_job(send_daily_report, schedule=\"0 9 * * *\")\n\n# Every hour\nscheduler.add_job(cleanup_temp_files, interval=timedelta(hours=1))\n\n# One-time\nscheduler.add_job(send_reminder, run_at=datetime(2024, 12, 25, 10, 0))\n\nawait scheduler.start()\n```\n\nKey aspects:\n1. Cron expressions for complex schedules\n2. Interval-based and one-time jobs\n3. Retry logic with max attempts\n4. Async support\n5. Job status tracking"}
{"instruction": "Write a function to handle file uploads securely", "output": "```python\nfrom pathlib import Path\nfrom typing import Optional\nfrom dataclasses import dataclass\nimport hashlib\nimport uuid\nimport magic\nimport aiofiles\nfrom fastapi import UploadFile, HTTPException, status\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass UploadConfig:\n    upload_dir: Path\n    max_size_mb: int = 10\n    allowed_types: list[str] = None\n    \n    def __post_init__(self):\n        self.upload_dir.mkdir(parents=True, exist_ok=True)\n        if self.allowed_types is None:\n            self.allowed_types = [\n                \"image/jpeg\", \"image/png\", \"image/gif\",\n                \"application/pdf\",\n                \"text/plain\", \"text/csv\"\n            ]\n\n@dataclass\nclass UploadResult:\n    file_id: str\n    filename: str\n    path: Path\n    size: int\n    mime_type: str\n    checksum: str\n\nclass FileUploadError(Exception):\n    \"\"\"File upload failed.\"\"\"\n    pass\n\nclass FileUploadService:\n    def __init__(self, config: UploadConfig):\n        self.config = config\n    \n    async def upload(self, file: UploadFile, subfolder: str = None) -> UploadResult:\n        \"\"\"Upload file securely with validation.\"\"\"\n        \n        # Read file content\n        content = await file.read()\n        \n        # Validate size\n        size = len(content)\n        max_bytes = self.config.max_size_mb * 1024 * 1024\n        \n        if size == 0:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"File is empty\"\n            )\n        \n        if size > max_bytes:\n            raise HTTPException(\n                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,\n                detail=f\"File too large. Max size: {self.config.max_size_mb}MB, got: {size / 1024 / 1024:.1f}MB\"\n            )\n        \n        # Detect actual MIME type (not trusting client)\n        detected_type = magic.from_buffer(content, mime=True)\n        \n        if detected_type not in self.config.allowed_types:\n            raise HTTPException(\n                status_code=status.HTTP_415_UNSUPPORTED_MEDIA_TYPE,\n                detail=f\"File type '{detected_type}' not allowed. Allowed: {', '.join(self.config.allowed_types)}\"\n            )\n        \n        # Validate extension matches content\n        original_name = file.filename or \"unnamed\"\n        extension = Path(original_name).suffix.lower()\n        \n        expected_extensions = {\n            \"image/jpeg\": [\".jpg\", \".jpeg\"],\n            \"image/png\": [\".png\"],\n            \"image/gif\": [\".gif\"],\n            \"application/pdf\": [\".pdf\"],\n            \"text/plain\": [\".txt\"],\n            \"text/csv\": [\".csv\"],\n        }\n        \n        valid_exts = expected_extensions.get(detected_type, [])\n        if valid_exts and extension not in valid_exts:\n            logger.warning(f\"Extension mismatch: {extension} vs {detected_type}\")\n            # Use correct extension\n            extension = valid_exts[0]\n        \n        # Generate secure filename\n        file_id = str(uuid.uuid4())\n        safe_filename = f\"{file_id}{extension}\"\n        \n        # Determine destination\n        dest_dir = self.config.upload_dir\n        if subfolder:\n            # Sanitize subfolder to prevent path traversal\n            subfolder = Path(subfolder).name  # Only take last component\n            dest_dir = dest_dir / subfolder\n            dest_dir.mkdir(exist_ok=True)\n        \n        dest_path = dest_dir / safe_filename\n        \n        # Calculate checksum\n        checksum = hashlib.sha256(content).hexdigest()\n        \n        # Check for duplicates\n        existing = await self._find_by_checksum(checksum)\n        if existing:\n            logger.info(f\"Duplicate file detected: {checksum}\")\n            return existing\n        \n        # Save file\n        async with aiofiles.open(dest_path, \"wb\") as f:\n            await f.write(content)\n        \n        logger.info(f\"File uploaded: {safe_filename} ({size} bytes)\")\n        \n        return UploadResult(\n            file_id=file_id,\n            filename=original_name,\n            path=dest_path,\n            size=size,\n            mime_type=detected_type,\n            checksum=checksum\n        )\n    \n    async def _find_by_checksum(self, checksum: str) -> Optional[UploadResult]:\n        \"\"\"Find existing file by checksum (implement with your DB).\"\"\"\n        # TODO: Query database for existing file\n        return None\n    \n    async def delete(self, file_id: str) -> bool:\n        \"\"\"Delete uploaded file.\"\"\"\n        # Find file by ID\n        for path in self.config.upload_dir.rglob(f\"{file_id}.*\"):\n            path.unlink()\n            logger.info(f\"File deleted: {path}\")\n            return True\n        return False\n    \n    def get_path(self, file_id: str) -> Optional[Path]:\n        \"\"\"Get file path by ID.\"\"\"\n        for path in self.config.upload_dir.rglob(f\"{file_id}.*\"):\n            return path\n        return None\n\n# FastAPI endpoint:\nupload_service = FileUploadService(UploadConfig(\n    upload_dir=Path(\"uploads\"),\n    max_size_mb=10\n))\n\n@app.post(\"/upload\")\nasync def upload_file(file: UploadFile):\n    result = await upload_service.upload(file)\n    return {\n        \"id\": result.file_id,\n        \"filename\": result.filename,\n        \"size\": result.size,\n        \"type\": result.mime_type\n    }\n```\n\nSecurity aspects:\n1. MIME type detection (not trusting client)\n2. Extension validation against content\n3. Size limits\n4. UUID filenames (no user-controlled names)\n5. Path traversal prevention\n6. Duplicate detection via checksum"}
{"instruction": "Create a pagination helper for database queries", "output": "```python\nfrom typing import TypeVar, Generic, Optional, Any\nfrom dataclasses import dataclass\nfrom math import ceil\nfrom sqlalchemy.orm import Query\nfrom pydantic import BaseModel\nfrom fastapi import Query as QueryParam\n\nT = TypeVar(\"T\")\n\n@dataclass\nclass PageParams:\n    \"\"\"Pagination parameters.\"\"\"\n    page: int = 1\n    size: int = 20\n    \n    def __post_init__(self):\n        if self.page < 1:\n            raise ValueError(f\"Page must be >= 1, got {self.page}\")\n        if self.size < 1:\n            raise ValueError(f\"Size must be >= 1, got {self.size}\")\n        if self.size > 100:\n            raise ValueError(f\"Size must be <= 100, got {self.size}\")\n    \n    @property\n    def offset(self) -> int:\n        return (self.page - 1) * self.size\n    \n    @property\n    def limit(self) -> int:\n        return self.size\n\n@dataclass\nclass Page(Generic[T]):\n    \"\"\"Paginated result.\"\"\"\n    items: list[T]\n    total: int\n    page: int\n    size: int\n    pages: int\n    \n    @property\n    def has_next(self) -> bool:\n        return self.page < self.pages\n    \n    @property\n    def has_prev(self) -> bool:\n        return self.page > 1\n    \n    @property\n    def next_page(self) -> Optional[int]:\n        return self.page + 1 if self.has_next else None\n    \n    @property\n    def prev_page(self) -> Optional[int]:\n        return self.page - 1 if self.has_prev else None\n\nclass PageResponse(BaseModel, Generic[T]):\n    \"\"\"Pydantic model for API response.\"\"\"\n    items: list[Any]\n    total: int\n    page: int\n    size: int\n    pages: int\n    has_next: bool\n    has_prev: bool\n\ndef paginate(query: Query, params: PageParams) -> Page:\n    \"\"\"Apply pagination to SQLAlchemy query.\"\"\"\n    total = query.count()\n    pages = ceil(total / params.size) if total > 0 else 1\n    \n    items = query.offset(params.offset).limit(params.limit).all()\n    \n    return Page(\n        items=items,\n        total=total,\n        page=params.page,\n        size=params.size,\n        pages=pages\n    )\n\n# FastAPI dependency for pagination params\ndef get_pagination(\n    page: int = QueryParam(1, ge=1, description=\"Page number\"),\n    size: int = QueryParam(20, ge=1, le=100, description=\"Items per page\")\n) -> PageParams:\n    return PageParams(page=page, size=size)\n\n# Usage in endpoint:\n@app.get(\"/users\", response_model=PageResponse)\ndef list_users(\n    db: Session = Depends(get_db),\n    pagination: PageParams = Depends(get_pagination),\n    search: Optional[str] = None\n):\n    query = db.query(User)\n    \n    if search:\n        query = query.filter(User.name.ilike(f\"%{search}%\"))\n    \n    query = query.order_by(User.created_at.desc())\n    \n    result = paginate(query, pagination)\n    \n    return PageResponse(\n        items=[UserResponse.from_orm(u) for u in result.items],\n        total=result.total,\n        page=result.page,\n        size=result.size,\n        pages=result.pages,\n        has_next=result.has_next,\n        has_prev=result.has_prev\n    )\n```\n\nKey aspects:\n1. Generic Page class for any model\n2. FastAPI dependency for params\n3. Automatic offset/limit calculation\n4. Page count and navigation helpers\n5. Size limits (max 100)\n6. Pydantic response model"}
{"instruction": "Write a middleware for request logging", "output": "```python\nimport time\nimport uuid\nimport logging\nfrom typing import Callable\nfrom fastapi import FastAPI, Request, Response\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport json\n\nlogger = logging.getLogger(\"api.requests\")\n\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Log all incoming requests with timing and details.\"\"\"\n    \n    def __init__(self, app: FastAPI, exclude_paths: list[str] = None):\n        super().__init__(app)\n        self.exclude_paths = exclude_paths or [\"/health\", \"/metrics\"]\n    \n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n        # Skip excluded paths\n        if request.url.path in self.exclude_paths:\n            return await call_next(request)\n        \n        # Generate request ID\n        request_id = str(uuid.uuid4())[:8]\n        request.state.request_id = request_id\n        \n        # Start timing\n        start_time = time.time()\n        \n        # Get request details\n        method = request.method\n        path = request.url.path\n        query = str(request.query_params) if request.query_params else \"\"\n        client_ip = request.client.host if request.client else \"unknown\"\n        user_agent = request.headers.get(\"user-agent\", \"unknown\")[:100]\n        \n        # Log request start\n        logger.info(\n            f\"[{request_id}] --> {method} {path}\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": method,\n                \"path\": path,\n                \"query\": query,\n                \"client_ip\": client_ip,\n                \"user_agent\": user_agent,\n                \"event\": \"request_start\"\n            }\n        )\n        \n        # Process request\n        try:\n            response = await call_next(request)\n            status_code = response.status_code\n            error = None\n        except Exception as e:\n            status_code = 500\n            error = str(e)\n            raise\n        finally:\n            # Calculate duration\n            duration_ms = (time.time() - start_time) * 1000\n            \n            # Determine log level based on status\n            if status_code >= 500:\n                log_func = logger.error\n            elif status_code >= 400:\n                log_func = logger.warning\n            else:\n                log_func = logger.info\n            \n            # Log response\n            log_func(\n                f\"[{request_id}] <-- {status_code} ({duration_ms:.1f}ms)\",\n                extra={\n                    \"request_id\": request_id,\n                    \"method\": method,\n                    \"path\": path,\n                    \"status_code\": status_code,\n                    \"duration_ms\": round(duration_ms, 2),\n                    \"client_ip\": client_ip,\n                    \"error\": error,\n                    \"event\": \"request_complete\"\n                }\n            )\n        \n        # Add request ID to response headers\n        response.headers[\"X-Request-ID\"] = request_id\n        \n        return response\n\nclass RequestBodyLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Log request bodies for debugging (use carefully - sensitive data!).\"\"\"\n    \n    def __init__(self, app: FastAPI, max_body_size: int = 10000):\n        super().__init__(app)\n        self.max_body_size = max_body_size\n    \n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n        # Only log POST/PUT/PATCH\n        if request.method in (\"POST\", \"PUT\", \"PATCH\"):\n            body = await request.body()\n            \n            if len(body) <= self.max_body_size:\n                try:\n                    body_json = json.loads(body)\n                    # Mask sensitive fields\n                    body_json = self._mask_sensitive(body_json)\n                    logger.debug(f\"Request body: {json.dumps(body_json)}\")\n                except:\n                    logger.debug(f\"Request body: {body[:200]}...\")\n        \n        return await call_next(request)\n    \n    def _mask_sensitive(self, data: dict) -> dict:\n        \"\"\"Mask sensitive fields in request body.\"\"\"\n        sensitive_fields = [\"password\", \"token\", \"secret\", \"api_key\", \"credit_card\"]\n        \n        if isinstance(data, dict):\n            return {\n                k: \"***MASKED***\" if any(s in k.lower() for s in sensitive_fields) else self._mask_sensitive(v)\n                for k, v in data.items()\n            }\n        elif isinstance(data, list):\n            return [self._mask_sensitive(item) for item in data]\n        return data\n\n# Usage:\napp = FastAPI()\napp.add_middleware(RequestLoggingMiddleware, exclude_paths=[\"/health\", \"/docs\"])\n\n# Get request ID in endpoints:\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int, request: Request):\n    request_id = getattr(request.state, \"request_id\", \"unknown\")\n    logger.info(f\"[{request_id}] Fetching user {user_id}\")\n    ...\n```\n\nKey aspects:\n1. Request ID for tracing\n2. Timing measurement\n3. Structured logging with extra fields\n4. Log level based on status code\n5. Sensitive data masking\n6. Exclude paths (health checks)"}
{"instruction": "Create a simple caching layer for API responses", "output": "```python\nfrom typing import Optional, Callable, Any\nfrom functools import wraps\nimport hashlib\nimport json\nimport time\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass CacheEntry:\n    value: Any\n    expires_at: float\n    \n    @property\n    def is_expired(self) -> bool:\n        return time.time() > self.expires_at\n\nclass SimpleCache:\n    \"\"\"In-memory cache with TTL support.\"\"\"\n    \n    def __init__(self, default_ttl: int = 300):\n        self._cache: dict[str, CacheEntry] = {}\n        self._default_ttl = default_ttl\n        self._lock = asyncio.Lock()\n    \n    async def get(self, key: str) -> Optional[Any]:\n        async with self._lock:\n            entry = self._cache.get(key)\n            \n            if entry is None:\n                return None\n            \n            if entry.is_expired:\n                del self._cache[key]\n                return None\n            \n            return entry.value\n    \n    async def set(self, key: str, value: Any, ttl: int = None) -> None:\n        async with self._lock:\n            expires_at = time.time() + (ttl or self._default_ttl)\n            self._cache[key] = CacheEntry(value=value, expires_at=expires_at)\n    \n    async def delete(self, key: str) -> bool:\n        async with self._lock:\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n    \n    async def clear(self) -> None:\n        async with self._lock:\n            self._cache.clear()\n    \n    async def cleanup_expired(self) -> int:\n        \"\"\"Remove expired entries. Returns count of removed entries.\"\"\"\n        async with self._lock:\n            expired_keys = [\n                k for k, v in self._cache.items() if v.is_expired\n            ]\n            for key in expired_keys:\n                del self._cache[key]\n            return len(expired_keys)\n\n# Global cache instance\ncache = SimpleCache(default_ttl=300)\n\ndef cached(\n    ttl: int = 300,\n    key_prefix: str = \"\",\n    key_builder: Callable[..., str] = None\n):\n    \"\"\"Decorator to cache function results.\"\"\"\n    \n    def decorator(func: Callable):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Build cache key\n            if key_builder:\n                cache_key = key_builder(*args, **kwargs)\n            else:\n                # Default key from function name and arguments\n                key_parts = [key_prefix or func.__name__]\n                key_parts.extend(str(arg) for arg in args)\n                key_parts.extend(f\"{k}={v}\" for k, v in sorted(kwargs.items()))\n                cache_key = hashlib.md5(\":\".join(key_parts).encode()).hexdigest()\n            \n            # Try cache\n            cached_value = await cache.get(cache_key)\n            if cached_value is not None:\n                logger.debug(f\"Cache HIT: {cache_key}\")\n                return cached_value\n            \n            logger.debug(f\"Cache MISS: {cache_key}\")\n            \n            # Call function\n            if asyncio.iscoroutinefunction(func):\n                result = await func(*args, **kwargs)\n            else:\n                result = func(*args, **kwargs)\n            \n            # Store in cache\n            await cache.set(cache_key, result, ttl)\n            \n            return result\n        \n        # Add cache control methods\n        wrapper.cache_clear = lambda: cache.clear()\n        wrapper.cache_key = lambda *a, **kw: key_builder(*a, **kw) if key_builder else None\n        \n        return wrapper\n    return decorator\n\n# Usage examples:\n\n@cached(ttl=60)\nasync def get_user_profile(user_id: int) -> dict:\n    \"\"\"Cached for 60 seconds.\"\"\"\n    # Expensive database query\n    return await db.fetch_user(user_id)\n\n@cached(ttl=300, key_prefix=\"search\")\nasync def search_products(query: str, page: int = 1) -> list:\n    \"\"\"Search results cached for 5 minutes.\"\"\"\n    return await product_service.search(query, page)\n\n# Custom key builder\n@cached(\n    ttl=3600,\n    key_builder=lambda user_id, **kw: f\"user_permissions:{user_id}\"\n)\nasync def get_user_permissions(user_id: int) -> list[str]:\n    \"\"\"Permissions cached for 1 hour.\"\"\"\n    return await auth_service.get_permissions(user_id)\n\n# Manual cache invalidation:\nasync def update_user(user_id: int, data: dict):\n    await db.update_user(user_id, data)\n    # Invalidate cache\n    await cache.delete(f\"user_permissions:{user_id}\")\n\n# Cache with FastAPI dependency:\nfrom fastapi import Depends\n\nasync def get_cached_settings() -> dict:\n    cached_value = await cache.get(\"app_settings\")\n    if cached_value:\n        return cached_value\n    \n    settings = await load_settings_from_db()\n    await cache.set(\"app_settings\", settings, ttl=600)\n    return settings\n\n@app.get(\"/config\")\nasync def get_config(settings: dict = Depends(get_cached_settings)):\n    return settings\n```\n\nKey aspects:\n1. TTL (time-to-live) support\n2. Async-safe with locks\n3. Decorator for easy use\n4. Custom key builders\n5. Manual invalidation\n6. Cleanup of expired entries"}
{"instruction": "Write a health check endpoint with dependency status", "output": "```python\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\nfrom typing import Optional\nfrom enum import Enum\nfrom datetime import datetime\nimport asyncio\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass HealthStatus(str, Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\nclass DependencyHealth(BaseModel):\n    name: str\n    status: HealthStatus\n    latency_ms: Optional[float] = None\n    message: Optional[str] = None\n\nclass HealthResponse(BaseModel):\n    status: HealthStatus\n    timestamp: str\n    version: str\n    uptime_seconds: float\n    dependencies: list[DependencyHealth]\n\nclass HealthChecker:\n    def __init__(self, app_version: str = \"1.0.0\"):\n        self._start_time = time.time()\n        self._version = app_version\n        self._checks: list[tuple[str, callable, bool]] = []  # name, check_func, critical\n    \n    def add_check(self, name: str, check_func: callable, critical: bool = True):\n        \"\"\"Add a health check.\n        \n        Args:\n            name: Display name for the dependency\n            check_func: Async function that returns True if healthy\n            critical: If True, failure makes overall status UNHEALTHY\n        \"\"\"\n        self._checks.append((name, check_func, critical))\n    \n    async def _run_check(self, name: str, check_func: callable) -> DependencyHealth:\n        start = time.time()\n        \n        try:\n            result = await asyncio.wait_for(check_func(), timeout=5.0)\n            latency = (time.time() - start) * 1000\n            \n            if result is True:\n                return DependencyHealth(\n                    name=name,\n                    status=HealthStatus.HEALTHY,\n                    latency_ms=round(latency, 2)\n                )\n            elif isinstance(result, str):\n                return DependencyHealth(\n                    name=name,\n                    status=HealthStatus.DEGRADED,\n                    latency_ms=round(latency, 2),\n                    message=result\n                )\n            else:\n                return DependencyHealth(\n                    name=name,\n                    status=HealthStatus.UNHEALTHY,\n                    latency_ms=round(latency, 2),\n                    message=\"Check returned False\"\n                )\n        \n        except asyncio.TimeoutError:\n            return DependencyHealth(\n                name=name,\n                status=HealthStatus.UNHEALTHY,\n                latency_ms=5000,\n                message=\"Health check timed out after 5s\"\n            )\n        except Exception as e:\n            latency = (time.time() - start) * 1000\n            return DependencyHealth(\n                name=name,\n                status=HealthStatus.UNHEALTHY,\n                latency_ms=round(latency, 2),\n                message=str(e)\n            )\n    \n    async def check(self) -> HealthResponse:\n        # Run all checks in parallel\n        tasks = [\n            self._run_check(name, func)\n            for name, func, _ in self._checks\n        ]\n        results = await asyncio.gather(*tasks)\n        \n        # Determine overall status\n        overall = HealthStatus.HEALTHY\n        \n        for (name, _, critical), result in zip(self._checks, results):\n            if result.status == HealthStatus.UNHEALTHY:\n                if critical:\n                    overall = HealthStatus.UNHEALTHY\n                elif overall == HealthStatus.HEALTHY:\n                    overall = HealthStatus.DEGRADED\n            elif result.status == HealthStatus.DEGRADED:\n                if overall == HealthStatus.HEALTHY:\n                    overall = HealthStatus.DEGRADED\n        \n        return HealthResponse(\n            status=overall,\n            timestamp=datetime.utcnow().isoformat() + \"Z\",\n            version=self._version,\n            uptime_seconds=round(time.time() - self._start_time, 2),\n            dependencies=results\n        )\n\n# Create health checker\nhealth = HealthChecker(app_version=\"2.1.0\")\n\n# Add checks\nasync def check_database():\n    try:\n        await db.execute(\"SELECT 1\")\n        return True\n    except Exception as e:\n        raise Exception(f\"Database unreachable: {e}\")\n\nasync def check_redis():\n    try:\n        await redis.ping()\n        return True\n    except:\n        return \"Redis connection failed\"\n\nasync def check_external_api():\n    try:\n        async with httpx.AsyncClient() as client:\n            r = await client.get(\"https://api.example.com/health\", timeout=3)\n            return r.status_code == 200\n    except:\n        return False\n\nhealth.add_check(\"database\", check_database, critical=True)\nhealth.add_check(\"redis\", check_redis, critical=False)  # Non-critical\nhealth.add_check(\"external_api\", check_external_api, critical=False)\n\n# Endpoints\napp = FastAPI()\n\n@app.get(\"/health\", response_model=HealthResponse)\nasync def health_check(response: Response):\n    result = await health.check()\n    \n    if result.status == HealthStatus.UNHEALTHY:\n        response.status_code = 503\n    elif result.status == HealthStatus.DEGRADED:\n        response.status_code = 200  # Still serving\n    \n    return result\n\n@app.get(\"/health/live\")\nasync def liveness():\n    \"\"\"Kubernetes liveness probe - is app running?\"\"\"\n    return {\"status\": \"alive\"}\n\n@app.get(\"/health/ready\")\nasync def readiness(response: Response):\n    \"\"\"Kubernetes readiness probe - can app serve traffic?\"\"\"\n    result = await health.check()\n    \n    if result.status == HealthStatus.UNHEALTHY:\n        response.status_code = 503\n        return {\"status\": \"not_ready\", \"reason\": \"Critical dependency unhealthy\"}\n    \n    return {\"status\": \"ready\"}\n```\n\nKey aspects:\n1. Critical vs non-critical dependencies\n2. Parallel health checks with timeouts\n3. Latency measurement\n4. Degraded state (partial failure)\n5. Kubernetes-compatible probes\n6. Proper HTTP status codes"}
{"instruction": "Write a data validation decorator", "output": "```python\nfrom typing import Any, Callable, TypeVar, get_type_hints\nfrom functools import wraps\nfrom dataclasses import dataclass\nimport inspect\n\nT = TypeVar(\"T\")\n\nclass ValidationError(Exception):\n    \"\"\"Validation failed.\"\"\"\n    def __init__(self, errors: list[dict]):\n        self.errors = errors\n        messages = [f\"{e['field']}: {e['message']}\" for e in errors]\n        super().__init__(f\"Validation failed: {'; '.join(messages)}\")\n\n@dataclass\nclass ValidationRule:\n    field: str\n    check: Callable[[Any], bool]\n    message: str\n\ndef validate(*rules: ValidationRule):\n    \"\"\"Decorator to validate function arguments.\n    \n    Usage:\n        @validate(\n            ValidationRule(\"email\", lambda x: \"@\" in x, \"Invalid email format\"),\n            ValidationRule(\"age\", lambda x: 0 < x < 150, \"Age must be 1-149\")\n        )\n        def create_user(email: str, age: int):\n            ...\n    \"\"\"\n    def decorator(func: Callable[..., T]) -> Callable[..., T]:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Get argument names and values\n            sig = inspect.signature(func)\n            bound = sig.bind(*args, **kwargs)\n            bound.apply_defaults()\n            \n            errors = []\n            \n            for rule in rules:\n                if rule.field in bound.arguments:\n                    value = bound.arguments[rule.field]\n                    try:\n                        if not rule.check(value):\n                            errors.append({\n                                \"field\": rule.field,\n                                \"message\": rule.message,\n                                \"value\": value\n                            })\n                    except Exception as e:\n                        errors.append({\n                            \"field\": rule.field,\n                            \"message\": f\"Validation error: {e}\",\n                            \"value\": value\n                        })\n            \n            if errors:\n                raise ValidationError(errors)\n            \n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# Shorthand validators\ndef required(field: str) -> ValidationRule:\n    return ValidationRule(field, lambda x: x is not None and x != \"\", f\"{field} is required\")\n\ndef min_length(field: str, length: int) -> ValidationRule:\n    return ValidationRule(field, lambda x: len(str(x)) >= length, f\"{field} must be at least {length} characters\")\n\ndef max_length(field: str, length: int) -> ValidationRule:\n    return ValidationRule(field, lambda x: len(str(x)) <= length, f\"{field} must be at most {length} characters\")\n\ndef in_range(field: str, min_val: float, max_val: float) -> ValidationRule:\n    return ValidationRule(field, lambda x: min_val <= x <= max_val, f\"{field} must be between {min_val} and {max_val}\")\n\ndef matches_pattern(field: str, pattern: str, description: str = \"pattern\") -> ValidationRule:\n    import re\n    return ValidationRule(field, lambda x: bool(re.match(pattern, str(x))), f\"{field} must match {description}\")\n\ndef is_email(field: str) -> ValidationRule:\n    import re\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return ValidationRule(field, lambda x: bool(re.match(pattern, str(x))), f\"{field} must be a valid email\")\n\ndef one_of(field: str, choices: list) -> ValidationRule:\n    return ValidationRule(field, lambda x: x in choices, f\"{field} must be one of: {', '.join(map(str, choices))}\")\n\n# Usage examples:\n\n@validate(\n    is_email(\"email\"),\n    min_length(\"password\", 8),\n    min_length(\"name\", 2),\n    max_length(\"name\", 50)\n)\ndef register_user(email: str, password: str, name: str):\n    print(f\"Registering {name} with {email}\")\n    return {\"email\": email, \"name\": name}\n\n@validate(\n    required(\"user_id\"),\n    in_range(\"amount\", 0.01, 10000),\n    one_of(\"currency\", [\"USD\", \"EUR\", \"GBP\"])\n)\ndef create_payment(user_id: int, amount: float, currency: str):\n    return {\"user_id\": user_id, \"amount\": amount, \"currency\": currency}\n\n# Custom validation\n@validate(\n    ValidationRule(\n        \"items\",\n        lambda x: len(x) > 0 and all(isinstance(i, dict) for i in x),\n        \"Items must be a non-empty list of objects\"\n    )\n)\ndef process_order(items: list[dict]):\n    return {\"item_count\": len(items)}\n\n# Test\ntry:\n    register_user(\"invalid-email\", \"short\", \"\")\nexcept ValidationError as e:\n    for error in e.errors:\n        print(f\"  {error['field']}: {error['message']}\")\n# Output:\n#   email: email must be a valid email\n#   password: password must be at least 8 characters\n#   name: name must be at least 2 characters\n```\n\nKey aspects:\n1. Declarative validation rules\n2. Multiple errors collected\n3. Shorthand validators for common cases\n4. Works with positional and keyword args\n5. Custom validation functions\n6. Clear error messages"}
