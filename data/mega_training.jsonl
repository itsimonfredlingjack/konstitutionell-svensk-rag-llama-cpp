{"instruction": "Skapa en FastAPI endpoint som streamar AI-svar via Server-Sent Events", "output": "```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\napp = FastAPI()\n\nasync def generate_stream(prompt: str):\n    \"\"\"Generator som streamar AI-svar chunk f√∂r chunk.\"\"\"\n    async for chunk in ollama_client.chat_stream(prompt):\n        yield f\"data: {chunk}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n@app.get(\"/api/stream\")\nasync def stream_response(prompt: str):\n    return StreamingResponse(\n        generate_stream(prompt),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n```\n\nDenna endpoint anv√§nder Server-Sent Events f√∂r att streama AI-svar i realtid. Klienten kan l√§sa str√∂mmen med EventSource API."}
{"instruction": "Implementera en React hook f√∂r WebSocket-anslutning med automatisk reconnect", "output": "```typescript\nimport { useState, useEffect, useCallback, useRef } from 'react';\n\ninterface UseWebSocketOptions {\n  url: string;\n  onMessage: (data: any) => void;\n  reconnectInterval?: number;\n}\n\nexport function useWebSocket({ url, onMessage, reconnectInterval = 3000 }: UseWebSocketOptions) {\n  const [isConnected, setIsConnected] = useState(false);\n  const wsRef = useRef<WebSocket | null>(null);\n  const reconnectTimeoutRef = useRef<NodeJS.Timeout>();\n\n  const connect = useCallback(() => {\n    const ws = new WebSocket(url);\n    \n    ws.onopen = () => {\n      setIsConnected(true);\n      console.log('WebSocket connected');\n    };\n    \n    ws.onmessage = (event) => {\n      const data = JSON.parse(event.data);\n      onMessage(data);\n    };\n    \n    ws.onclose = () => {\n      setIsConnected(false);\n      reconnectTimeoutRef.current = setTimeout(connect, reconnectInterval);\n    };\n    \n    wsRef.current = ws;\n  }, [url, onMessage, reconnectInterval]);\n\n  useEffect(() => {\n    connect();\n    return () => {\n      clearTimeout(reconnectTimeoutRef.current);\n      wsRef.current?.close();\n    };\n  }, [connect]);\n\n  const send = useCallback((data: any) => {\n    wsRef.current?.send(JSON.stringify(data));\n  }, []);\n\n  return { isConnected, send };\n}\n```"}
{"instruction": "Skapa en Python async context manager f√∂r Ollama model loading med timeout", "output": "```python\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import Optional\nimport httpx\n\nclass OllamaModelLoader:\n    def __init__(self, base_url: str = \"http://localhost:11434\"):\n        self.base_url = base_url\n        self.client = httpx.AsyncClient(timeout=60.0)\n    \n    @asynccontextmanager\n    async def load_model(self, model_name: str, timeout: float = 120.0):\n        \"\"\"Load model with timeout, unload on exit.\"\"\"\n        try:\n            # Ladda modellen\n            async with asyncio.timeout(timeout):\n                await self.client.post(\n                    f\"{self.base_url}/api/generate\",\n                    json={\"model\": model_name, \"prompt\": \"\", \"keep_alive\": \"5m\"}\n                )\n            yield model_name\n        finally:\n            # Unload model f√∂r att frig√∂ra VRAM\n            await self.client.post(\n                f\"{self.base_url}/api/generate\",\n                json={\"model\": model_name, \"keep_alive\": \"0\"}\n            )\n\n# Anv√§ndning:\nasync with loader.load_model(\"qwen2.5-coder:14b\") as model:\n    response = await ollama.chat(model=model, messages=[...])\n```"}
{"instruction": "Implementera en Zustand store f√∂r chat-historik med persistens", "output": "```typescript\nimport { create } from 'zustand';\nimport { persist } from 'zustand/middleware';\n\ninterface Message {\n  id: string;\n  role: 'user' | 'assistant';\n  content: string;\n  timestamp: number;\n}\n\ninterface ChatStore {\n  messages: Message[];\n  isLoading: boolean;\n  addMessage: (message: Omit<Message, 'id' | 'timestamp'>) => void;\n  updateMessage: (id: string, content: string) => void;\n  clearHistory: () => void;\n  setLoading: (loading: boolean) => void;\n}\n\nexport const useChatStore = create<ChatStore>()(\n  persist(\n    (set) => ({\n      messages: [],\n      isLoading: false,\n      \n      addMessage: (message) => set((state) => ({\n        messages: [...state.messages, {\n          ...message,\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n        }]\n      })),\n      \n      updateMessage: (id, content) => set((state) => ({\n        messages: state.messages.map(m => \n          m.id === id ? { ...m, content } : m\n        )\n      })),\n      \n      clearHistory: () => set({ messages: [] }),\n      setLoading: (loading) => set({ isLoading: loading }),\n    }),\n    { name: 'chat-history' }\n  )\n);\n```"}
{"instruction": "Skapa en Python decorator f√∂r rate limiting med sliding window", "output": "```python\nimport asyncio\nimport time\nfrom functools import wraps\nfrom collections import deque\n\ndef rate_limit(calls: int, period: float):\n    \"\"\"Sliding window rate limiter decorator.\"\"\"\n    def decorator(func):\n        call_times = deque()\n        lock = asyncio.Lock()\n        \n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            async with lock:\n                now = time.monotonic()\n                \n                # Ta bort gamla anrop utanf√∂r f√∂nstret\n                while call_times and call_times[0] < now - period:\n                    call_times.popleft()\n                \n                # Kolla om vi √∂verskrider gr√§nsen\n                if len(call_times) >= calls:\n                    wait_time = call_times[0] + period - now\n                    await asyncio.sleep(wait_time)\n                    call_times.popleft()\n                \n                call_times.append(time.monotonic())\n            \n            return await func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# Anv√§ndning:\n@rate_limit(calls=10, period=60.0)  # Max 10 anrop per minut\nasync def call_ollama(prompt: str):\n    return await ollama.chat(prompt=prompt)\n```"}
{"instruction": "Implementera error boundary komponent i React med fallback UI", "output": "```typescript\nimport React, { Component, ErrorInfo, ReactNode } from 'react';\n\ninterface Props {\n  children: ReactNode;\n  fallback?: ReactNode;\n}\n\ninterface State {\n  hasError: boolean;\n  error: Error | null;\n}\n\nexport class ErrorBoundary extends Component<Props, State> {\n  constructor(props: Props) {\n    super(props);\n    this.state = { hasError: false, error: null };\n  }\n\n  static getDerivedStateFromError(error: Error): State {\n    return { hasError: true, error };\n  }\n\n  componentDidCatch(error: Error, errorInfo: ErrorInfo) {\n    console.error('ErrorBoundary caught:', error, errorInfo);\n  }\n\n  render() {\n    if (this.state.hasError) {\n      return this.props.fallback || (\n        <div className=\"p-4 bg-red-500/20 rounded-lg border border-red-500\">\n          <h2 className=\"text-red-400 font-bold\">N√•got gick fel</h2>\n          <p className=\"text-red-300 text-sm mt-2\">\n            {this.state.error?.message}\n          </p>\n          <button\n            onClick={() => this.setState({ hasError: false, error: null })}\n            className=\"mt-4 px-4 py-2 bg-red-500 rounded hover:bg-red-600\"\n          >\n            F√∂rs√∂k igen\n          </button>\n        </div>\n      );\n    }\n\n    return this.props.children;\n  }\n}\n```"}
{"instruction": "Skapa en Python service f√∂r GPU monitoring med NVIDIA SMI", "output": "```python\nimport subprocess\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass GPUStats:\n    name: str\n    memory_used: int  # MiB\n    memory_total: int  # MiB\n    utilization: int  # %\n    temperature: int  # Celsius\n    \n    @property\n    def memory_percent(self) -> float:\n        return (self.memory_used / self.memory_total) * 100\n    \n    @property\n    def is_memory_critical(self) -> bool:\n        return self.memory_percent > 90\n\nclass GPUMonitor:\n    async def get_stats(self) -> Optional[GPUStats]:\n        \"\"\"H√§mta GPU-statistik via nvidia-smi.\"\"\"\n        try:\n            result = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\",\n                \"--query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu\",\n                \"--format=csv,noheader,nounits\",\n                stdout=asyncio.subprocess.PIPE\n            )\n            stdout, _ = await result.communicate()\n            \n            parts = stdout.decode().strip().split(\", \")\n            return GPUStats(\n                name=parts[0],\n                memory_used=int(parts[1]),\n                memory_total=int(parts[2]),\n                utilization=int(parts[3]),\n                temperature=int(parts[4])\n            )\n        except Exception as e:\n            print(f\"GPU monitoring error: {e}\")\n            return None\n\n# Anv√§ndning:\nmonitor = GPUMonitor()\nstats = await monitor.get_stats()\nif stats and stats.is_memory_critical:\n    print(f\"VARNING: GPU minne p√• {stats.memory_percent:.1f}%\")\n```"}
{"instruction": "Implementera en multi-agent cascade pipeline med hot-swap model loading", "output": "```python\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import AsyncIterator\n\nclass CascadePhase(Enum):\n    PLANNING = \"planning\"\n    CODING = \"coding\"\n    REVIEWING = \"reviewing\"\n    COMPLETE = \"complete\"\n\n@dataclass\nclass CascadeResult:\n    phase: CascadePhase\n    content: str\n    model: str\n\nclass CascadeOrchestrator:\n    MODELS = {\n        CascadePhase.PLANNING: \"devstral-openhands:latest\",\n        CascadePhase.CODING: \"qwen-sonnet:latest\",\n        CascadePhase.REVIEWING: \"qwen-openhands:latest\",\n    }\n    \n    async def execute_cascade(self, task: str) -> AsyncIterator[CascadeResult]:\n        \"\"\"K√∂r multi-agent cascade med hot-swap.\"\"\"\n        context = {\"task\": task}\n        \n        for phase in [CascadePhase.PLANNING, CascadePhase.CODING, CascadePhase.REVIEWING]:\n            model = self.MODELS[phase]\n            \n            # Hot-swap: unload previous, load new\n            await self._switch_model(model)\n            \n            # Execute phase\n            prompt = self._build_prompt(phase, context)\n            async for chunk in self.ollama.chat_stream(model, prompt):\n                yield CascadeResult(phase=phase, content=chunk, model=model)\n                context[phase.value] = context.get(phase.value, \"\") + chunk\n        \n        yield CascadeResult(phase=CascadePhase.COMPLETE, content=\"\", model=\"\")\n    \n    async def _switch_model(self, new_model: str):\n        \"\"\"Unload current model, load new one.\"\"\"\n        await self.ollama.unload_all()\n        await self.ollama.load(new_model)\n```"}
{"instruction": "Skapa en TypeScript utility f√∂r att debounce async funktioner", "output": "```typescript\nexport function debounceAsync<T extends (...args: any[]) => Promise<any>>(\n  fn: T,\n  delay: number\n): (...args: Parameters<T>) => Promise<ReturnType<T>> {\n  let timeoutId: NodeJS.Timeout | null = null;\n  let pendingPromise: Promise<ReturnType<T>> | null = null;\n  let resolvePromise: ((value: ReturnType<T>) => void) | null = null;\n\n  return (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    // Cancel previous timeout\n    if (timeoutId) {\n      clearTimeout(timeoutId);\n    }\n\n    // Create new promise if none pending\n    if (!pendingPromise) {\n      pendingPromise = new Promise((resolve) => {\n        resolvePromise = resolve;\n      });\n    }\n\n    timeoutId = setTimeout(async () => {\n      try {\n        const result = await fn(...args);\n        resolvePromise?.(result);\n      } finally {\n        pendingPromise = null;\n        resolvePromise = null;\n      }\n    }, delay);\n\n    return pendingPromise;\n  };\n}\n\n// Anv√§ndning:\nconst debouncedSearch = debounceAsync(\n  async (query: string) => {\n    const response = await fetch(`/api/search?q=${query}`);\n    return response.json();\n  },\n  300\n);\n```"}
{"instruction": "Implementera en Python background task manager med graceful shutdown", "output": "```python\nimport asyncio\nimport signal\nfrom typing import Callable, Awaitable, Set\nfrom contextlib import asynccontextmanager\n\nclass BackgroundTaskManager:\n    def __init__(self):\n        self._tasks: Set[asyncio.Task] = set()\n        self._shutdown = asyncio.Event()\n    \n    def start_task(self, coro: Awaitable, name: str = None):\n        \"\"\"Starta en bakgrundsuppgift.\"\"\"\n        task = asyncio.create_task(coro, name=name)\n        self._tasks.add(task)\n        task.add_done_callback(self._tasks.discard)\n        return task\n    \n    async def shutdown(self, timeout: float = 10.0):\n        \"\"\"Graceful shutdown av alla tasks.\"\"\"\n        self._shutdown.set()\n        \n        if not self._tasks:\n            return\n        \n        # V√§nta p√• att tasks avslutas\n        done, pending = await asyncio.wait(\n            self._tasks,\n            timeout=timeout\n        )\n        \n        # Avbryt kvarvarande tasks\n        for task in pending:\n            task.cancel()\n        \n        if pending:\n            await asyncio.gather(*pending, return_exceptions=True)\n    \n    @property\n    def should_stop(self) -> bool:\n        return self._shutdown.is_set()\n\n# FastAPI integration:\ntask_manager = BackgroundTaskManager()\n\n@asynccontextmanager\nasync def lifespan(app):\n    # Starta bakgrundsuppgifter\n    task_manager.start_task(periodic_cleanup(), name=\"cleanup\")\n    yield\n    # Graceful shutdown\n    await task_manager.shutdown()\n```"}
{"instruction": "Skapa en React komponent f√∂r infinite scroll med virtualisering", "output": "```typescript\nimport { useRef, useEffect, useState, useCallback } from 'react';\n\ninterface VirtualListProps<T> {\n  items: T[];\n  itemHeight: number;\n  containerHeight: number;\n  renderItem: (item: T, index: number) => React.ReactNode;\n  onLoadMore?: () => void;\n  hasMore?: boolean;\n}\n\nexport function VirtualList<T>({\n  items,\n  itemHeight,\n  containerHeight,\n  renderItem,\n  onLoadMore,\n  hasMore = false,\n}: VirtualListProps<T>) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const [scrollTop, setScrollTop] = useState(0);\n\n  // Ber√§kna synliga items\n  const startIndex = Math.floor(scrollTop / itemHeight);\n  const endIndex = Math.min(\n    startIndex + Math.ceil(containerHeight / itemHeight) + 1,\n    items.length\n  );\n  const visibleItems = items.slice(startIndex, endIndex);\n\n  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {\n    const { scrollTop, scrollHeight, clientHeight } = e.currentTarget;\n    setScrollTop(scrollTop);\n\n    // Ladda mer n√§r n√§ra botten\n    if (hasMore && scrollHeight - scrollTop - clientHeight < 100) {\n      onLoadMore?.();\n    }\n  }, [hasMore, onLoadMore]);\n\n  return (\n    <div\n      ref={containerRef}\n      onScroll={handleScroll}\n      style={{ height: containerHeight, overflow: 'auto' }}\n    >\n      <div style={{ height: items.length * itemHeight, position: 'relative' }}>\n        {visibleItems.map((item, i) => (\n          <div\n            key={startIndex + i}\n            style={{\n              position: 'absolute',\n              top: (startIndex + i) * itemHeight,\n              height: itemHeight,\n              width: '100%',\n            }}\n          >\n            {renderItem(item, startIndex + i)}\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n```"}
{"instruction": "Implementera en Python retry decorator med exponential backoff", "output": "```python\nimport asyncio\nimport random\nfrom functools import wraps\nfrom typing import Type, Tuple, Optional\n\ndef retry_with_backoff(\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n):\n    \"\"\"Retry decorator med exponential backoff och jitter.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            last_exception: Optional[Exception] = None\n            \n            for attempt in range(max_retries + 1):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_retries:\n                        raise\n                    \n                    # Ber√§kna delay med exponential backoff\n                    delay = min(\n                        base_delay * (exponential_base ** attempt),\n                        max_delay\n                    )\n                    \n                    # L√§gg till jitter f√∂r att undvika thundering herd\n                    if jitter:\n                        delay *= (0.5 + random.random())\n                    \n                    print(f\"Retry {attempt + 1}/{max_retries} efter {delay:.2f}s: {e}\")\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Anv√§ndning:\n@retry_with_backoff(max_retries=5, exceptions=(httpx.TimeoutException,))\nasync def call_api(url: str):\n    async with httpx.AsyncClient() as client:\n        return await client.get(url, timeout=10.0)\n```"}
{"instruction": "Skapa en n8n workflow f√∂r AI-driven email kategorisering och auto-svar", "output": "```json\n{\n  \"name\": \"AI Email Categorizer and Auto-Responder\",\n  \"nodes\": [\n    {\n      \"name\": \"Gmail Trigger\",\n      \"type\": \"n8n-nodes-base.gmailTrigger\",\n      \"parameters\": {\n        \"pollTimes\": { \"item\": [{ \"mode\": \"everyMinute\" }] },\n        \"filters\": { \"labelIds\": [\"INBOX\"], \"q\": \"is:unread\" }\n      }\n    },\n    {\n      \"name\": \"Categorize with Ollama\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Kategorisera email som: URGENT, SUPPORT, SALES, NEWSLETTER, SPAM. Svara med JSON: {\\\"category\\\": \\\"...\\\", \\\"needs_reply\\\": true/false, \\\"suggested_reply\\\": \\\"...\\\"}\"\n          },\n          {\n            \"role\": \"user\", \n            \"content\": \"√Ñmne: {{$json.subject}}\\n\\nInneh√•ll: {{$json.body}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Route by Category\",\n      \"type\": \"n8n-nodes-base.switch\",\n      \"parameters\": {\n        \"rules\": [\n          { \"value\": \"URGENT\", \"output\": 0 },\n          { \"value\": \"SUPPORT\", \"output\": 1 },\n          { \"value\": \"SPAM\", \"output\": 2 }\n        ]\n      }\n    },\n    {\n      \"name\": \"Send Auto-Reply\",\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"parameters\": {\n        \"operation\": \"reply\",\n        \"message\": \"{{$json.suggested_reply}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\n1. Triggar p√• nya ol√§sta email\n2. Anv√§nder Ollama f√∂r att kategorisera\n3. Routar baserat p√• kategori\n4. Skickar auto-svar f√∂r URGENT/SUPPORT"}
{"instruction": "Skapa en n8n workflow f√∂r multi-source data aggregation med konflikthantering", "output": "```json\n{\n  \"name\": \"Multi-Source Data Aggregator with Conflict Resolution\",\n  \"nodes\": [\n    {\n      \"name\": \"Schedule Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": { \"rule\": { \"interval\": [{ \"field\": \"hours\", \"hoursInterval\": 1 }] } }\n    },\n    {\n      \"name\": \"Fetch from Source A\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": { \"url\": \"https://api-a.example.com/data\" }\n    },\n    {\n      \"name\": \"Fetch from Source B\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": { \"url\": \"https://api-b.example.com/data\" }\n    },\n    {\n      \"name\": \"Merge Sources\",\n      \"type\": \"n8n-nodes-base.merge\",\n      \"parameters\": { \"mode\": \"combine\", \"mergeByFields\": { \"values\": [{ \"field1\": \"id\", \"field2\": \"id\" }] } }\n    },\n    {\n      \"name\": \"Resolve Conflicts\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const items = $input.all();\\nreturn items.map(item => {\\n  const a = item.json.source_a;\\n  const b = item.json.source_b;\\n  \\n  // Anv√§nd senaste uppdaterad\\n  const resolved = a.updated_at > b.updated_at ? a : b;\\n  \\n  // Logga konflikter\\n  if (JSON.stringify(a) !== JSON.stringify(b)) {\\n    resolved.had_conflict = true;\\n    resolved.conflict_sources = [a, b];\\n  }\\n  \\n  return { json: resolved };\\n});\"\n      }\n    },\n    {\n      \"name\": \"Store to Database\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": { \"operation\": \"upsert\", \"table\": \"aggregated_data\" }\n    }\n  ]\n}\n```\n\nWorkflow aggregerar data fr√•n flera k√§llor, identifierar konflikter baserat p√• timestamp, och sparar resultatet med konfliktloggning."}
{"instruction": "Skapa en n8n workflow f√∂r intelligent task routing baserat p√• team capacity", "output": "```json\n{\n  \"name\": \"Intelligent Task Router with Capacity Analysis\",\n  \"nodes\": [\n    {\n      \"name\": \"Webhook Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": { \"path\": \"new-task\", \"httpMethod\": \"POST\" }\n    },\n    {\n      \"name\": \"Get Team Workload\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": { \"operation\": \"getAll\", \"resource\": \"task\", \"filters\": { \"statuses\": [\"in progress\"] } }\n    },\n    {\n      \"name\": \"Analyze with AI\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en task router. Analysera teamets arbetsbelastning och v√§lj b√§sta tilldelning.\\n\\nRegler:\\n1. Max 5 aktiva tasks per person\\n2. Prioritera baserat p√• skills\\n3. Balansera workload\\n\\nSvara med JSON: {\\\"assignee\\\": \\\"namn\\\", \\\"reason\\\": \\\"motivering\\\", \\\"priority\\\": 1-5}\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Ny task: {{$node['Webhook Trigger'].json.title}}\\n\\nBeskrivning: {{$node['Webhook Trigger'].json.description}}\\n\\nTeam workload:\\n{{$node['Get Team Workload'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Create Task in ClickUp\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"resource\": \"task\",\n        \"assignees\": [\"={{$json.assignee}}\"],\n        \"priority\": \"={{$json.priority}}\"\n      }\n    },\n    {\n      \"name\": \"Notify on Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#task-assignments\",\n        \"text\": \"üéØ Ny task tilldelad {{$json.assignee}}\\n\\nMotivering: {{$json.reason}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Skapa en n8n workflow f√∂r automatisk dokumentgenerering fr√•n ClickUp tasks", "output": "```json\n{\n  \"name\": \"Auto-Generated Documentation from ClickUp\",\n  \"nodes\": [\n    {\n      \"name\": \"Daily Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": { \"rule\": { \"interval\": [{ \"field\": \"days\", \"daysInterval\": 1 }] } }\n    },\n    {\n      \"name\": \"Get Completed Tasks\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"getAll\",\n        \"resource\": \"task\",\n        \"filters\": { \"statuses\": [\"complete\"], \"date_updated_gt\": \"={{$now.minus({days: 1}).toISO()}}\" }\n      }\n    },\n    {\n      \"name\": \"Generate Documentation\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Generera teknisk dokumentation i Markdown-format baserat p√• avslutade tasks. Inkludera:\\n- Sammanfattning av √§ndringar\\n- Tekniska detaljer\\n- Breaking changes\\n- Migrationssteg om till√§mpligt\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Avslutade tasks senaste 24h:\\n{{$json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Create Google Doc\",\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"title\": \"Changelog {{$now.format('yyyy-MM-dd')}}\",\n        \"content\": \"={{$json.documentation}}\"\n      }\n    },\n    {\n      \"name\": \"Post to Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#releases\",\n        \"text\": \"üìö Daglig dokumentation genererad: {{$json.docUrl}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Hur √∂vervakar jag Ollama model memory usage i produktionsmilj√∂?", "output": "## Ollama Memory Monitoring i Produktion\n\n### 1. Grundl√§ggande monitoring script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-monitor.sh\n\nwhile true; do\n  # GPU memory\n  GPU_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)\n  GPU_TOTAL=12288  # RTX 4070\n  GPU_PCT=$((GPU_MEM * 100 / GPU_TOTAL))\n  \n  # Ollama models\n  MODELS=$(ollama ps --format json | jq -r '.[] | \"\\(.name): \\(.size)\"')\n  \n  # Log\n  echo \"[$(date)] GPU: ${GPU_MEM}MB (${GPU_PCT}%) | Models: $MODELS\"\n  \n  # Alert om >90%\n  if [ $GPU_PCT -gt 90 ]; then\n    curl -X POST https://slack.webhook/... -d '{\"text\": \"‚ö†Ô∏è GPU memory critical: '$GPU_PCT'%\"}'\n  fi\n  \n  sleep 30\ndone\n```\n\n### 2. Systemd service f√∂r monitoring\n\n```ini\n# /etc/systemd/system/ollama-monitor.service\n[Unit]\nDescription=Ollama Memory Monitor\nAfter=ollama.service\n\n[Service]\nExecStart=/usr/local/bin/ollama-monitor.sh\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 3. Python monitoring med Prometheus metrics\n\n```python\nfrom prometheus_client import Gauge, start_http_server\nimport subprocess\nimport time\n\ngpu_memory = Gauge('ollama_gpu_memory_mb', 'GPU memory usage in MB')\nmodel_count = Gauge('ollama_loaded_models', 'Number of loaded models')\n\ndef collect_metrics():\n    # GPU memory\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    gpu_memory.set(int(result.stdout.strip()))\n    \n    # Model count\n    result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True)\n    model_count.set(len(result.stdout.strip().split('\\n')) - 1)\n\nif __name__ == '__main__':\n    start_http_server(9090)\n    while True:\n        collect_metrics()\n        time.sleep(15)\n```\n\n### 4. Automatisk VRAM-rensning vid kritisk niv√•\n\n```bash\n# L√§gg till i crontab\n*/5 * * * * GPU_PCT=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print int($1/122.88)}') && [ $GPU_PCT -gt 95 ] && curl -X POST http://localhost:11434/api/generate -d '{\"model\":\"dummy\",\"keep_alive\":\"0\"}' 2>/dev/null\n```"}
{"instruction": "Hur s√§tter jag upp automatisk backup av Ollama modeller och konfiguration?", "output": "## Automatisk Backup av Ollama Setup\n\n### 1. Backup script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-backup.sh\n\nBACKUP_DIR=\"/backup/ollama\"\nDATE=$(date +%Y%m%d)\nOLLAMA_HOME=\"/usr/share/ollama/.ollama\"\n\n# Skapa backup-katalog\nmkdir -p \"$BACKUP_DIR/$DATE\"\n\n# Backup Modelfiles (viktigast!)\necho \"Backing up Modelfiles...\"\ncp -r /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/ \"$BACKUP_DIR/$DATE/\"\n\n# Backup model manifest (inte weights - f√∂r stora)\necho \"Backing up model manifests...\"\nfind \"$OLLAMA_HOME/models/manifests\" -type f -exec cp --parents {} \"$BACKUP_DIR/$DATE/\" \\;\n\n# Lista installerade modeller\necho \"Saving model list...\"\nollama list > \"$BACKUP_DIR/$DATE/installed_models.txt\"\n\n# Backup custom configs\ncp /etc/systemd/system/ollama.service \"$BACKUP_DIR/$DATE/\" 2>/dev/null\n\n# Komprimera\ntar -czf \"$BACKUP_DIR/ollama-backup-$DATE.tar.gz\" -C \"$BACKUP_DIR\" \"$DATE\"\nrm -rf \"$BACKUP_DIR/$DATE\"\n\n# Beh√•ll senaste 7 backups\nls -t \"$BACKUP_DIR\"/*.tar.gz | tail -n +8 | xargs rm -f 2>/dev/null\n\necho \"Backup complete: $BACKUP_DIR/ollama-backup-$DATE.tar.gz\"\n```\n\n### 2. Restore script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-restore.sh\n\nBACKUP_FILE=$1\nif [ -z \"$BACKUP_FILE\" ]; then\n  echo \"Usage: $0 <backup-file.tar.gz>\"\n  exit 1\nfi\n\n# Extrahera\nTMP_DIR=$(mktemp -d)\ntar -xzf \"$BACKUP_FILE\" -C \"$TMP_DIR\"\nDATE_DIR=$(ls \"$TMP_DIR\")\n\n# √Öterst√§ll Modelfiles\ncp -r \"$TMP_DIR/$DATE_DIR/modelfiles/\"* /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/\n\n# Skapa modeller fr√•n Modelfiles\nfor mf in /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/Modelfile.*; do\n  MODEL_NAME=$(basename \"$mf\" | sed 's/Modelfile.//')\n  echo \"Creating model: $MODEL_NAME\"\n  ollama create \"$MODEL_NAME\" -f \"$mf\"\ndone\n\nrm -rf \"$TMP_DIR\"\necho \"Restore complete!\"\n```\n\n### 3. Cron f√∂r daglig backup\n\n```bash\n# crontab -e\n0 3 * * * /usr/local/bin/ollama-backup.sh >> /var/log/ollama-backup.log 2>&1\n```\n\n### 4. Sync till remote (optional)\n\n```bash\n# L√§gg till i backup-scriptet\nrsync -avz \"$BACKUP_DIR/\" backup-server:/backups/ollama/\n```"}
{"instruction": "Hur fels√∂ker jag n√§r Ollama inte svarar eller √§r l√•ngsam?", "output": "## Ollama Fels√∂kningsguide\n\n### Steg 1: Kolla status\n\n```bash\n# Service status\nsystemctl status ollama\n\n# Process\nps aux | grep ollama\n\n# Lyssnar p√• port?\nlsof -i :11434\n\n# API health\ncurl http://localhost:11434/api/tags\n```\n\n### Steg 2: GPU-status\n\n```bash\n# GPU memory och utilization\nnvidia-smi\n\n# Laddade modeller\nollama ps\n\n# Om GPU full - unload alla modeller\nfor model in $(ollama ps -q); do\n  curl -X POST http://localhost:11434/api/generate \\\n    -d \"{\\\"model\\\":\\\"$model\\\",\\\"keep_alive\\\":\\\"0\\\"}\"\ndone\n```\n\n### Steg 3: Vanliga problem och l√∂sningar\n\n**Problem: \"connection refused\"**\n```bash\n# Starta om Ollama\nsudo systemctl restart ollama\n\n# V√§nta p√• start\nsleep 5 && curl http://localhost:11434/api/tags\n```\n\n**Problem: \"CUDA out of memory\"**\n```bash\n# Kolla vad som anv√§nder GPU\nnvidia-smi --query-compute-apps=pid,name,used_memory --format=csv\n\n# D√∂da zombie-processer\nfuser -k /dev/nvidia*\n\n# Starta om\nsudo systemctl restart ollama\n```\n\n**Problem: L√•ngsam inference**\n```bash\n# Kolla om modellen √§r laddad\nollama ps\n\n# Om inte - pre-load\ncurl -X POST http://localhost:11434/api/generate \\\n  -d '{\"model\":\"qwen2.5-coder:14b\",\"prompt\":\"\",\"keep_alive\":\"10m\"}'\n\n# Kolla context size (l√§gre = snabbare)\n# I Modelfile: PARAMETER num_ctx 4096\n```\n\n**Problem: Modell hittas inte**\n```bash\n# Lista installerade\nollama list\n\n# Ladda ner\nollama pull qwen2.5-coder:14b\n\n# Eller skapa fr√•n Modelfile\nollama create qwen-sonnet -f modelfiles/Modelfile.qwen-sonnet\n```\n\n### Steg 4: Loggar\n\n```bash\n# Service loggar\njournalctl -u ollama -f\n\n# Med debug\nOLLAMA_DEBUG=1 ollama serve\n```\n\n### Quick Fix Script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/fix-ollama.sh\n\necho \"üîß Fixing Ollama...\"\n\n# Stop\nsudo systemctl stop ollama\nsleep 2\n\n# Clear GPU\nfuser -k /dev/nvidia* 2>/dev/null\n\n# Start\nsudo systemctl start ollama\nsleep 3\n\n# Verify\nif curl -s http://localhost:11434/api/tags > /dev/null; then\n  echo \"‚úÖ Ollama is running!\"\n  ollama list\nelse\n  echo \"‚ùå Ollama failed to start\"\n  journalctl -u ollama --since \"1 minute ago\"\nfi\n```"}
{"instruction": "Hur konfigurerar jag Ollama f√∂r optimal prestanda p√• RTX 4070 (12GB)?", "output": "## Optimal Ollama-konfiguration f√∂r RTX 4070 12GB\n\n### 1. Systemd service optimering\n\n```ini\n# /etc/systemd/system/ollama.service\n[Unit]\nDescription=Ollama Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=ollama\nGroup=ollama\n\n# Milj√∂variabler f√∂r prestanda\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=1\"\nEnvironment=\"CUDA_VISIBLE_DEVICES=0\"\n\nExecStart=/usr/local/bin/ollama serve\nRestart=always\nRestartSec=3\n\n# Resource limits\nLimitNOFILE=65535\nLimitNPROC=4096\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 2. Modelfile-optimeringar\n\n```dockerfile\n# Modelfile.qwen-optimized\nFROM qwen2.5-coder:14b\n\n# Context size - balans mellan kapacitet och VRAM\nPARAMETER num_ctx 8192\n\n# GPU layers - alla p√• GPU f√∂r 14B\nPARAMETER num_gpu 99\n\n# Batch size\nPARAMETER num_batch 512\n\n# Temperature f√∂r konsistens\nPARAMETER temperature 0.7\n\n# Stop tokens\nPARAMETER stop <|im_end|>\nPARAMETER stop <|endoftext|>\n```\n\n### 3. VRAM-budget f√∂r 12GB\n\n| Modell | VRAM | Context | Rekommendation |\n|--------|------|---------|----------------|\n| 7B Q4 | ~4GB | 8192 | ‚úÖ Kan k√∂ra 2 parallellt |\n| 14B Q4 | ~9GB | 8192 | ‚úÖ En √•t g√•ngen |\n| 14B Q4 | ~11GB | 16384 | ‚ö†Ô∏è Tight, minska context |\n| 32B Q4 | ~18GB | - | ‚ùå F√∂r stor |\n\n### 4. Hot-swap strategi f√∂r cascade\n\n```python\n# I cascade_orchestrator.py\nasync def switch_model(self, model: str):\n    # Unload current f√∂r att frig√∂ra VRAM\n    await self.unload_all()\n    \n    # Pre-load next model\n    await self.client.post(\n        f\"{self.base_url}/api/generate\",\n        json={\n            \"model\": model,\n            \"prompt\": \"\",\n            \"keep_alive\": \"5m\"  # 5 min timeout\n        }\n    )\n```\n\n### 5. Warmup script vid boot\n\n```bash\n#!/bin/bash\n# /usr/local/bin/warmup-models.sh\n\necho \"Warming up primary model...\"\ncurl -s -X POST http://localhost:11434/api/generate \\\n  -d '{\"model\":\"qwen2.5-coder:14b\",\"prompt\":\"Hello\",\"stream\":false}' \\\n  > /dev/null\n\necho \"Model loaded and ready!\"\nollama ps\n```\n\n### 6. Monitoring\n\n```bash\n# Alias f√∂r snabb koll\nalias gpu='nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv'\nalias models='ollama ps'\n```"}
{"instruction": "Skapa en TypeScript service f√∂r att hantera WebSocket connections med heartbeat", "output": "```typescript\nimport { WebSocket, WebSocketServer } from 'ws';\n\ninterface Client {\n  ws: WebSocket;\n  id: string;\n  isAlive: boolean;\n  lastPing: number;\n}\n\nexport class WebSocketManager {\n  private wss: WebSocketServer;\n  private clients: Map<string, Client> = new Map();\n  private heartbeatInterval: NodeJS.Timer;\n  private readonly HEARTBEAT_MS = 30000;\n  private readonly TIMEOUT_MS = 60000;\n\n  constructor(port: number) {\n    this.wss = new WebSocketServer({ port });\n    this.setupServer();\n    this.startHeartbeat();\n  }\n\n  private setupServer() {\n    this.wss.on('connection', (ws, req) => {\n      const clientId = this.generateId();\n      const client: Client = {\n        ws,\n        id: clientId,\n        isAlive: true,\n        lastPing: Date.now(),\n      };\n\n      this.clients.set(clientId, client);\n      console.log(`Client connected: ${clientId}`);\n\n      ws.on('pong', () => {\n        client.isAlive = true;\n        client.lastPing = Date.now();\n      });\n\n      ws.on('message', (data) => {\n        this.handleMessage(client, data.toString());\n      });\n\n      ws.on('close', () => {\n        this.clients.delete(clientId);\n        console.log(`Client disconnected: ${clientId}`);\n      });\n\n      // Send welcome message\n      ws.send(JSON.stringify({ type: 'connected', clientId }));\n    });\n  }\n\n  private startHeartbeat() {\n    this.heartbeatInterval = setInterval(() => {\n      const now = Date.now();\n\n      this.clients.forEach((client, id) => {\n        // Check timeout\n        if (now - client.lastPing > this.TIMEOUT_MS) {\n          console.log(`Client timed out: ${id}`);\n          client.ws.terminate();\n          this.clients.delete(id);\n          return;\n        }\n\n        // Send ping\n        if (client.isAlive) {\n          client.isAlive = false;\n          client.ws.ping();\n        }\n      });\n    }, this.HEARTBEAT_MS);\n  }\n\n  broadcast(message: object, excludeId?: string) {\n    const data = JSON.stringify(message);\n    this.clients.forEach((client, id) => {\n      if (id !== excludeId && client.ws.readyState === WebSocket.OPEN) {\n        client.ws.send(data);\n      }\n    });\n  }\n\n  send(clientId: string, message: object) {\n    const client = this.clients.get(clientId);\n    if (client?.ws.readyState === WebSocket.OPEN) {\n      client.ws.send(JSON.stringify(message));\n    }\n  }\n\n  private handleMessage(client: Client, data: string) {\n    try {\n      const message = JSON.parse(data);\n      // Handle different message types\n      console.log(`Message from ${client.id}:`, message);\n    } catch (e) {\n      console.error('Invalid message:', data);\n    }\n  }\n\n  private generateId(): string {\n    return `client_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  getConnectedClients(): number {\n    return this.clients.size;\n  }\n\n  shutdown() {\n    clearInterval(this.heartbeatInterval);\n    this.clients.forEach((client) => client.ws.close());\n    this.wss.close();\n  }\n}\n```"}
{"instruction": "Implementera en Python middleware f√∂r request logging och timing", "output": "```python\nimport time\nimport uuid\nimport logging\nfrom typing import Callable\nfrom fastapi import Request, Response\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.types import ASGIApp\n\nlogger = logging.getLogger(__name__)\n\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware f√∂r request logging, timing och request ID.\"\"\"\n    \n    def __init__(self, app: ASGIApp, exclude_paths: list[str] = None):\n        super().__init__(app)\n        self.exclude_paths = exclude_paths or [\"/health\", \"/metrics\"]\n    \n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n        # Skip excluded paths\n        if any(request.url.path.startswith(p) for p in self.exclude_paths):\n            return await call_next(request)\n        \n        # Generate request ID\n        request_id = str(uuid.uuid4())[:8]\n        request.state.request_id = request_id\n        \n        # Log request start\n        start_time = time.perf_counter()\n        logger.info(\n            f\"[{request_id}] {request.method} {request.url.path} started\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"client\": request.client.host if request.client else \"unknown\",\n            }\n        )\n        \n        # Process request\n        try:\n            response = await call_next(request)\n        except Exception as e:\n            duration_ms = (time.perf_counter() - start_time) * 1000\n            logger.error(\n                f\"[{request_id}] {request.method} {request.url.path} \"\n                f\"FAILED after {duration_ms:.2f}ms: {e}\"\n            )\n            raise\n        \n        # Calculate duration\n        duration_ms = (time.perf_counter() - start_time) * 1000\n        \n        # Add headers\n        response.headers[\"X-Request-ID\"] = request_id\n        response.headers[\"X-Response-Time\"] = f\"{duration_ms:.2f}ms\"\n        \n        # Log request completion\n        log_level = logging.WARNING if response.status_code >= 400 else logging.INFO\n        logger.log(\n            log_level,\n            f\"[{request_id}] {request.method} {request.url.path} \"\n            f\"{response.status_code} in {duration_ms:.2f}ms\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"status_code\": response.status_code,\n                \"duration_ms\": duration_ms,\n            }\n        )\n        \n        return response\n\n\n# Anv√§ndning i FastAPI:\nfrom fastapi import FastAPI\n\napp = FastAPI()\napp.add_middleware(\n    RequestLoggingMiddleware,\n    exclude_paths=[\"/health\", \"/metrics\", \"/favicon.ico\"]\n)\n```"}
{"instruction": "Skapa en React context f√∂r theme switching med system preference detection", "output": "```typescript\nimport React, { createContext, useContext, useEffect, useState } from 'react';\n\ntype Theme = 'light' | 'dark' | 'system';\ntype ResolvedTheme = 'light' | 'dark';\n\ninterface ThemeContextValue {\n  theme: Theme;\n  resolvedTheme: ResolvedTheme;\n  setTheme: (theme: Theme) => void;\n  toggleTheme: () => void;\n}\n\nconst ThemeContext = createContext<ThemeContextValue | null>(null);\n\nconst STORAGE_KEY = 'theme-preference';\n\nfunction getSystemTheme(): ResolvedTheme {\n  if (typeof window === 'undefined') return 'dark';\n  return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n}\n\nexport function ThemeProvider({ children }: { children: React.ReactNode }) {\n  const [theme, setThemeState] = useState<Theme>(() => {\n    if (typeof window === 'undefined') return 'system';\n    return (localStorage.getItem(STORAGE_KEY) as Theme) || 'system';\n  });\n\n  const [resolvedTheme, setResolvedTheme] = useState<ResolvedTheme>(() => {\n    if (theme === 'system') return getSystemTheme();\n    return theme;\n  });\n\n  // Listen for system theme changes\n  useEffect(() => {\n    const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');\n    \n    const handleChange = (e: MediaQueryListEvent) => {\n      if (theme === 'system') {\n        setResolvedTheme(e.matches ? 'dark' : 'light');\n      }\n    };\n\n    mediaQuery.addEventListener('change', handleChange);\n    return () => mediaQuery.removeEventListener('change', handleChange);\n  }, [theme]);\n\n  // Update resolved theme when theme changes\n  useEffect(() => {\n    const resolved = theme === 'system' ? getSystemTheme() : theme;\n    setResolvedTheme(resolved);\n  }, [theme]);\n\n  // Apply theme to document\n  useEffect(() => {\n    document.documentElement.classList.remove('light', 'dark');\n    document.documentElement.classList.add(resolvedTheme);\n    document.documentElement.style.colorScheme = resolvedTheme;\n  }, [resolvedTheme]);\n\n  const setTheme = (newTheme: Theme) => {\n    setThemeState(newTheme);\n    localStorage.setItem(STORAGE_KEY, newTheme);\n  };\n\n  const toggleTheme = () => {\n    const newTheme = resolvedTheme === 'dark' ? 'light' : 'dark';\n    setTheme(newTheme);\n  };\n\n  return (\n    <ThemeContext.Provider value={{ theme, resolvedTheme, setTheme, toggleTheme }}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}\n\nexport function useTheme() {\n  const context = useContext(ThemeContext);\n  if (!context) {\n    throw new Error('useTheme must be used within a ThemeProvider');\n  }\n  return context;\n}\n\n// Anv√§ndning:\n// <ThemeProvider>\n//   <App />\n// </ThemeProvider>\n//\n// const { theme, resolvedTheme, setTheme, toggleTheme } = useTheme();\n```"}
{"instruction": "Implementera en Python service f√∂r structured logging med JSON output", "output": "```python\nimport json\nimport logging\nimport sys\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional\nimport traceback\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter f√∂r structured logging.\"\"\"\n    \n    def __init__(self, service_name: str = \"app\", environment: str = \"development\"):\n        super().__init__()\n        self.service_name = service_name\n        self.environment = environment\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_entry: Dict[str, Any] = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n            \"service\": self.service_name,\n            \"environment\": self.environment,\n        }\n        \n        # Add location info\n        log_entry[\"location\"] = {\n            \"file\": record.filename,\n            \"line\": record.lineno,\n            \"function\": record.funcName,\n        }\n        \n        # Add extra fields\n        if hasattr(record, \"__dict__\"):\n            for key, value in record.__dict__.items():\n                if key not in [\n                    \"name\", \"msg\", \"args\", \"created\", \"filename\",\n                    \"funcName\", \"levelname\", \"levelno\", \"lineno\",\n                    \"module\", \"msecs\", \"pathname\", \"process\",\n                    \"processName\", \"relativeCreated\", \"stack_info\",\n                    \"thread\", \"threadName\", \"exc_info\", \"exc_text\",\n                    \"message\", \"getMessage\"\n                ]:\n                    log_entry[key] = value\n        \n        # Add exception info\n        if record.exc_info:\n            log_entry[\"exception\"] = {\n                \"type\": record.exc_info[0].__name__ if record.exc_info[0] else None,\n                \"message\": str(record.exc_info[1]) if record.exc_info[1] else None,\n                \"traceback\": traceback.format_exception(*record.exc_info),\n            }\n        \n        return json.dumps(log_entry, default=str, ensure_ascii=False)\n\n\ndef setup_logging(\n    service_name: str = \"app\",\n    level: str = \"INFO\",\n    json_output: bool = True\n) -> logging.Logger:\n    \"\"\"Konfigurera structured logging.\"\"\"\n    \n    # Remove existing handlers\n    root = logging.getLogger()\n    for handler in root.handlers[:]:\n        root.removeHandler(handler)\n    \n    # Create handler\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(getattr(logging, level.upper()))\n    \n    if json_output:\n        handler.setFormatter(JSONFormatter(service_name=service_name))\n    else:\n        handler.setFormatter(logging.Formatter(\n            \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n        ))\n    \n    root.addHandler(handler)\n    root.setLevel(getattr(logging, level.upper()))\n    \n    return root\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"H√§mta en logger instance.\"\"\"\n    return logging.getLogger(name)\n\n\n# Anv√§ndning:\nsetup_logging(service_name=\"ai-vibe-backend\", level=\"INFO\", json_output=True)\n\nlogger = get_logger(__name__)\nlogger.info(\"Server started\", extra={\"port\": 8000, \"version\": \"1.0.0\"})\nlogger.warning(\"High memory usage\", extra={\"memory_percent\": 85})\n\n# Output (JSON):\n# {\"timestamp\": \"2025-12-03T19:00:00Z\", \"level\": \"INFO\", \"message\": \"Server started\", \"port\": 8000, ...}\n```"}
{"instruction": "Skapa en n8n workflow f√∂r competitive intelligence monitoring", "output": "```json\n{\n  \"name\": \"Competitive Intelligence Monitor\",\n  \"nodes\": [\n    {\n      \"name\": \"Schedule Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": {\n        \"rule\": { \"interval\": [{ \"field\": \"hours\", \"hoursInterval\": 6 }] }\n      }\n    },\n    {\n      \"name\": \"Define Competitors\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"parameters\": {\n        \"values\": {\n          \"string\": [\n            { \"name\": \"competitors\", \"value\": \"[{\\\"name\\\":\\\"Competitor A\\\",\\\"url\\\":\\\"https://competitor-a.com\\\"},{\\\"name\\\":\\\"Competitor B\\\",\\\"url\\\":\\\"https://competitor-b.com\\\"}]\" }\n          ]\n        }\n      }\n    },\n    {\n      \"name\": \"Split Competitors\",\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"parameters\": { \"batchSize\": 1 }\n    },\n    {\n      \"name\": \"Fetch Website\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"={{$json.url}}\",\n        \"options\": { \"timeout\": 30000 }\n      }\n    },\n    {\n      \"name\": \"Take Screenshot\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"https://api.screenshotmachine.com\",\n        \"qs\": { \"url\": \"={{$json.url}}\", \"dimension\": \"1920x1080\" }\n      }\n    },\n    {\n      \"name\": \"Analyze Changes\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Analysera konkurrenters webbplats och identifiera:\\n1. Nya produkter/features\\n2. Pris√§ndringar\\n3. Design/UX f√∂r√§ndringar\\n4. Nytt inneh√•ll\\n\\nSvara med JSON: {\\\"changes\\\": [], \\\"significance\\\": \\\"low/medium/high\\\", \\\"action_items\\\": []}\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analysera {{$json.name}}:\\n\\n{{$json.body}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Filter Significant\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"string\": [{ \"value1\": \"={{$json.significance}}\", \"operation\": \"notEqual\", \"value2\": \"low\" }]\n        }\n      }\n    },\n    {\n      \"name\": \"Store to Database\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"insert\",\n        \"table\": \"competitive_intel\",\n        \"columns\": \"competitor,changes,significance,timestamp\"\n      }\n    },\n    {\n      \"name\": \"Alert Team\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#competitive-intel\",\n        \"text\": \"üîç *Konkurrensintelligens Alert*\\n\\n*{{$json.name}}* har gjort {{$json.significance}} f√∂r√§ndringar:\\n\\n{{$json.changes}}\\n\\n*Action items:*\\n{{$json.action_items}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. K√∂r var 6:e timme\\n2. H√§mtar konkurrenters webbplatser\\n3. Tar screenshots f√∂r visuell j√§mf√∂relse\\n4. Analyserar med AI f√∂r att hitta f√∂r√§ndringar\\n5. Filtrerar bort low-significance changes\\n6. Sparar till databas och alertar teamet"}
{"instruction": "Skapa en n8n workflow f√∂r automated lead scoring baserat p√• beteendedata", "output": "```json\n{\n  \"name\": \"AI-Powered Lead Scoring Engine\",\n  \"nodes\": [\n    {\n      \"name\": \"Webhook Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": {\n        \"path\": \"lead-activity\",\n        \"httpMethod\": \"POST\",\n        \"responseMode\": \"onReceived\"\n      }\n    },\n    {\n      \"name\": \"Get Lead History\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"executeQuery\",\n        \"query\": \"SELECT * FROM lead_activities WHERE lead_id = $1 ORDER BY timestamp DESC LIMIT 50\",\n        \"values\": \"={{$json.lead_id}}\"\n      }\n    },\n    {\n      \"name\": \"Calculate Engagement Score\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const activities = $input.all();\\nconst weights = {\\n  'page_view': 1,\\n  'download': 5,\\n  'email_open': 2,\\n  'email_click': 4,\\n  'demo_request': 20,\\n  'pricing_view': 10,\\n  'contact_form': 15\\n};\\n\\nlet score = 0;\\nlet recentActivity = false;\\nconst now = new Date();\\n\\nfor (const activity of activities) {\\n  const weight = weights[activity.json.type] || 1;\\n  const daysAgo = (now - new Date(activity.json.timestamp)) / (1000 * 60 * 60 * 24);\\n  \\n  // Decay: recent = full points, old = less\\n  const decayMultiplier = Math.max(0.1, 1 - (daysAgo / 30));\\n  score += weight * decayMultiplier;\\n  \\n  if (daysAgo < 7) recentActivity = true;\\n}\\n\\nreturn [{\\n  json: {\\n    lead_id: $input.first().json.lead_id,\\n    engagement_score: Math.round(score),\\n    recent_activity: recentActivity,\\n    activity_count: activities.length\\n  }\\n}];\"\n      }\n    },\n    {\n      \"name\": \"AI Lead Analysis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en lead scoring-expert. Analysera beteendedata och ge:\\n1. Lead grade (A/B/C/D)\\n2. K√∂psannolikhet (0-100%)\\n3. Rekommenderad action\\n4. B√§sta tid att kontakta\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Lead data:\\nEngagement score: {{$json.engagement_score}}\\nRecent activity: {{$json.recent_activity}}\\nActivity count: {{$json.activity_count}}\\n\\nAktiviteter senaste 30 dagar:\\n{{$node['Get Lead History'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Update CRM\",\n      \"type\": \"n8n-nodes-base.hubspot\",\n      \"parameters\": {\n        \"operation\": \"update\",\n        \"resource\": \"contact\",\n        \"contactId\": \"={{$json.lead_id}}\",\n        \"additionalFields\": {\n          \"lead_score\": \"={{$json.engagement_score}}\",\n          \"lead_grade\": \"={{$json.grade}}\",\n          \"purchase_probability\": \"={{$json.purchase_probability}}\"\n        }\n      }\n    },\n    {\n      \"name\": \"Route Hot Leads\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"string\": [{ \"value1\": \"={{$json.grade}}\", \"value2\": \"A\" }]\n        }\n      }\n    },\n    {\n      \"name\": \"Create Sales Task\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"resource\": \"task\",\n        \"name\": \"üî• HOT LEAD: Contact {{$json.lead_name}}\",\n        \"priority\": 1,\n        \"dueDate\": \"={{$json.best_contact_time}}\"\n      }\n    },\n    {\n      \"name\": \"Notify Sales Rep\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#sales-alerts\",\n        \"text\": \"üéØ *Grade A Lead Alert!*\\n\\n*Lead:* {{$json.lead_name}}\\n*Score:* {{$json.engagement_score}}\\n*Purchase probability:* {{$json.purchase_probability}}%\\n*Recommended action:* {{$json.recommended_action}}\\n*Best contact time:* {{$json.best_contact_time}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Hur skapar jag en custom Ollama model med specifikt beteende?", "output": "## Skapa Custom Ollama Model med Modelfile\n\n### 1. Grundl√§ggande Modelfile-struktur\n\n```dockerfile\n# modelfiles/Modelfile.my-assistant\n\n# Basmodell\nFROM qwen2.5-coder:14b\n\n# System prompt - definierar beteende\nSYSTEM \"\"\"\nDu √§r en svensk AI-assistent specialiserad p√• teknisk dokumentation.\n\n## Dina regler:\n1. Svara ALLTID p√• svenska\n2. Ge konkreta kodexempel\n3. F√∂rklara WHY, inte bara HOW\n4. FR√ÖGA ALDRIG - leverera direkt\n\n## Format:\n- Anv√§nd markdown\n- Kod i code blocks med spr√•k\n- Bullet points f√∂r listor\n\"\"\"\n\n# Parametrar\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER num_ctx 8192\nPARAMETER stop <|im_end|>\nPARAMETER stop <|endoftext|>\n\n# Template (optional - f√∂r chat format)\nTEMPLATE \"\"\"<|im_start|>system\n{{ .System }}<|im_end|>\n<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n```\n\n### 2. Skapa modellen\n\n```bash\n# Skapa fr√•n Modelfile\nollama create my-assistant -f modelfiles/Modelfile.my-assistant\n\n# Verifiera\nollama list | grep my-assistant\n\n# Testa\nollama run my-assistant \"F√∂rklara async/await i Python\"\n```\n\n### 3. Specialiserade exempel\n\n**Code Reviewer:**\n```dockerfile\nFROM qwen2.5-coder:14b\n\nSYSTEM \"\"\"\nDu √§r en senior code reviewer. F√∂r varje kod du granskar:\n\n## Struktur:\n1. **SAMMANFATTNING** - Kort bed√∂mning\n2. **BUGGAR** - Kritiska problem\n3. **F√ñRB√ÑTTRINGAR** - Suggestions\n4. **S√ÑKERHET** - Security issues\n5. **VERDICT** - APPROVED ‚úì eller NEEDS CHANGES ‚úó\n\n## Regler:\n- Var konstruktiv\n- Ge konkreta fix-f√∂rslag\n- Prioritera: Buggar > S√§kerhet > Performance > Style\n\"\"\"\n```\n\n**Planner:**\n```dockerfile\nFROM devstral:latest\n\nSYSTEM \"\"\"\nDu √§r en teknisk planerare. F√∂r varje uppgift:\n\n## Output format:\n### IMPLEMENTATION STEG\n1. [Steg] - [Tid]\n2. ...\n\n### FILER ATT √ÑNDRA\n- fil.py - [beskrivning]\n\n### POTENTIELLA UTMANINGAR\n- [Utmaning] - [L√∂sning]\n\n## Regler:\n- Bryt ner i 30-min steg\n- Var specifik med filnamn\n- Identifiera beroenden\n\"\"\"\n```\n\n### 4. Uppdatera befintlig modell\n\n```bash\n# Redigera Modelfile\nnano modelfiles/Modelfile.my-assistant\n\n# Skapa om (√∂verskriver)\nollama create my-assistant -f modelfiles/Modelfile.my-assistant\n\n# Testa igen\nollama run my-assistant \"Test prompt\"\n```\n\n### 5. Backup och dela\n\n```bash\n# Exportera Modelfile\nollama show my-assistant --modelfile > backup/Modelfile.my-assistant\n\n# Kopiera till annan maskin\nscp modelfiles/Modelfile.my-assistant user@server:~/modelfiles/\nssh user@server \"ollama create my-assistant -f ~/modelfiles/Modelfile.my-assistant\"\n```\n\n### 6. Tips f√∂r bra system prompts\n\n- **Var specifik** - Vaga instruktioner ger vaga svar\n- **Anv√§nd exempel** - Visa exakt format du vill ha\n- **Negativa instruktioner** - \"G√ñR INTE X\" funkar bra\n- **Rollspel** - \"Du √§r en...\" hj√§lper modellen\n- **Strukturera output** - Definiera rubriker och format"}
{"instruction": "Skapa en n8n workflow f√∂r self-healing infrastructure monitoring", "output": "```json\n{\n  \"name\": \"Self-Healing Infrastructure Monitor\",\n  \"nodes\": [\n    {\n      \"name\": \"Every 5 Minutes\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": {\n        \"rule\": { \"interval\": [{ \"field\": \"minutes\", \"minutesInterval\": 5 }] }\n      }\n    },\n    {\n      \"name\": \"Health Checks\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const services = [\\n  { name: 'ollama', url: 'http://localhost:11434/api/tags', restartCmd: 'systemctl restart ollama' },\\n  { name: 'backend', url: 'http://localhost:8000/health', restartCmd: 'systemctl restart ai-vibe-backend' },\\n  { name: 'frontend', url: 'http://localhost:5173', restartCmd: 'cd /app/frontend && npm run dev' }\\n];\\n\\nconst results = [];\\nfor (const service of services) {\\n  try {\\n    const response = await fetch(service.url, { timeout: 5000 });\\n    results.push({ ...service, healthy: response.ok, status: response.status });\\n  } catch (e) {\\n    results.push({ ...service, healthy: false, error: e.message });\\n  }\\n}\\n\\nreturn results.map(r => ({ json: r }));\"\n      }\n    },\n    {\n      \"name\": \"Filter Unhealthy\",\n      \"type\": \"n8n-nodes-base.filter\",\n      \"parameters\": {\n        \"conditions\": {\n          \"boolean\": [{ \"value1\": \"={{$json.healthy}}\", \"value2\": false }]\n        }\n      }\n    },\n    {\n      \"name\": \"AI Diagnosis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en DevOps-expert. Analysera felande service och ge:\\n1. Trolig orsak\\n2. Recommended fix\\n3. Risk level (low/medium/high)\\n4. Should auto-restart (true/false)\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Service: {{$json.name}}\\nError: {{$json.error}}\\nRestart command: {{$json.restartCmd}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Should Auto-Fix?\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"boolean\": [{ \"value1\": \"={{$json.should_auto_restart}}\", \"value2\": true }]\n        }\n      }\n    },\n    {\n      \"name\": \"Execute Fix\",\n      \"type\": \"n8n-nodes-base.executeCommand\",\n      \"parameters\": {\n        \"command\": \"={{$json.restartCmd}}\"\n      }\n    },\n    {\n      \"name\": \"Verify Fix\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"={{$json.url}}\",\n        \"options\": { \"timeout\": 10000 }\n      }\n    },\n    {\n      \"name\": \"Log Result\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"insert\",\n        \"table\": \"healing_events\",\n        \"columns\": \"service,issue,fix_applied,success,timestamp\"\n      }\n    },\n    {\n      \"name\": \"Alert if Failed\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#alerts\",\n        \"text\": \"üö® *Self-Healing Failed*\\n\\nService: {{$json.name}}\\nIssue: {{$json.error}}\\nDiagnosis: {{$json.diagnosis}}\\n\\n‚ö†Ô∏è Manual intervention required!\"\n      }\n    },\n    {\n      \"name\": \"Escalate High Risk\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#on-call\",\n        \"text\": \"üî¥ *HIGH RISK ISSUE*\\n\\nService: {{$json.name}}\\nRisk: HIGH\\nDiagnosis: {{$json.diagnosis}}\\n\\nAuto-restart disabled - awaiting manual approval.\\n\\nReact with ‚úÖ to approve restart.\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. K√∂r health checks var 5:e minut\\n2. Identifierar unhealthy services\\n3. Anv√§nder AI f√∂r att diagnostisera problem\\n4. Auto-fixar low/medium risk issues\\n5. Eskalerar high risk till on-call\\n6. Loggar alla healing events"}
{"instruction": "Implementera en Python service f√∂r semantic caching av LLM responses", "output": "```python\nimport hashlib\nimport json\nimport asyncio\nfrom typing import Optional, Dict, Any, List\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n@dataclass\nclass CacheEntry:\n    prompt_hash: str\n    prompt: str\n    response: str\n    embedding: List[float]\n    created_at: datetime\n    hits: int = 0\n    \n    @property\n    def age_hours(self) -> float:\n        return (datetime.now() - self.created_at).total_seconds() / 3600\n\nclass SemanticCache:\n    \"\"\"\n    Semantic cache f√∂r LLM responses.\n    Anv√§nder embeddings f√∂r att hitta semantiskt liknande prompts.\n    \"\"\"\n    \n    def __init__(\n        self,\n        embedding_model: str = \"nomic-embed-text\",\n        similarity_threshold: float = 0.92,\n        max_entries: int = 1000,\n        ttl_hours: float = 24.0\n    ):\n        self.embedding_model = embedding_model\n        self.similarity_threshold = similarity_threshold\n        self.max_entries = max_entries\n        self.ttl_hours = ttl_hours\n        self.cache: Dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n    \n    async def get_embedding(self, text: str) -> List[float]:\n        \"\"\"H√§mta embedding via Ollama.\"\"\"\n        import httpx\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                \"http://localhost:11434/api/embeddings\",\n                json={\"model\": self.embedding_model, \"prompt\": text}\n            )\n            return response.json()[\"embedding\"]\n    \n    def cosine_similarity(self, a: List[float], b: List[float]) -> float:\n        \"\"\"Ber√§kna cosine similarity mellan tv√• vektorer.\"\"\"\n        a_np = np.array(a)\n        b_np = np.array(b)\n        return float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np)))\n    \n    async def get(self, prompt: str) -> Optional[str]:\n        \"\"\"S√∂k efter cached response f√∂r semantiskt liknande prompt.\"\"\"\n        async with self._lock:\n            # Rensa expired entries\n            self._cleanup_expired()\n            \n            # H√§mta embedding f√∂r query\n            query_embedding = await self.get_embedding(prompt)\n            \n            # S√∂k efter match\n            best_match: Optional[CacheEntry] = None\n            best_similarity = 0.0\n            \n            for entry in self.cache.values():\n                similarity = self.cosine_similarity(query_embedding, entry.embedding)\n                if similarity > best_similarity and similarity >= self.similarity_threshold:\n                    best_similarity = similarity\n                    best_match = entry\n            \n            if best_match:\n                best_match.hits += 1\n                return best_match.response\n            \n            return None\n    \n    async def set(self, prompt: str, response: str):\n        \"\"\"Cacha en prompt-response pair.\"\"\"\n        async with self._lock:\n            # Evict if at capacity\n            if len(self.cache) >= self.max_entries:\n                self._evict_lru()\n            \n            # H√§mta embedding\n            embedding = await self.get_embedding(prompt)\n            \n            # Skapa hash f√∂r exakt lookup\n            prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]\n            \n            self.cache[prompt_hash] = CacheEntry(\n                prompt_hash=prompt_hash,\n                prompt=prompt,\n                response=response,\n                embedding=embedding,\n                created_at=datetime.now()\n            )\n    \n    def _cleanup_expired(self):\n        \"\"\"Ta bort expired entries.\"\"\"\n        expired = [\n            key for key, entry in self.cache.items()\n            if entry.age_hours > self.ttl_hours\n        ]\n        for key in expired:\n            del self.cache[key]\n    \n    def _evict_lru(self):\n        \"\"\"Evict least recently used entry.\"\"\"\n        if not self.cache:\n            return\n        \n        # Sortera p√• hits (ascending) och age (descending)\n        lru_key = min(\n            self.cache.keys(),\n            key=lambda k: (self.cache[k].hits, -self.cache[k].age_hours)\n        )\n        del self.cache[lru_key]\n    \n    @property\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Cache statistics.\"\"\"\n        total_hits = sum(e.hits for e in self.cache.values())\n        return {\n            \"entries\": len(self.cache),\n            \"total_hits\": total_hits,\n            \"avg_age_hours\": np.mean([e.age_hours for e in self.cache.values()]) if self.cache else 0\n        }\n\n\n# Anv√§ndning:\ncache = SemanticCache(similarity_threshold=0.92)\n\nasync def chat_with_cache(prompt: str) -> str:\n    # Check cache\n    cached = await cache.get(prompt)\n    if cached:\n        return cached\n    \n    # Generate response\n    response = await ollama.chat(prompt=prompt)\n    \n    # Cache it\n    await cache.set(prompt, response)\n    \n    return response\n```"}
{"instruction": "Skapa en React hook f√∂r optimistic updates med rollback", "output": "```typescript\nimport { useState, useCallback, useRef } from 'react';\n\ninterface OptimisticState<T> {\n  data: T;\n  isUpdating: boolean;\n  error: Error | null;\n}\n\ninterface UseOptimisticUpdateOptions<T> {\n  onSuccess?: (newData: T) => void;\n  onError?: (error: Error, rollbackData: T) => void;\n  retryCount?: number;\n  retryDelay?: number;\n}\n\nexport function useOptimisticUpdate<T>(\n  initialData: T,\n  options: UseOptimisticUpdateOptions<T> = {}\n) {\n  const { onSuccess, onError, retryCount = 0, retryDelay = 1000 } = options;\n  \n  const [state, setState] = useState<OptimisticState<T>>({\n    data: initialData,\n    isUpdating: false,\n    error: null,\n  });\n  \n  const rollbackRef = useRef<T>(initialData);\n  const retryCountRef = useRef(0);\n\n  const update = useCallback(\n    async (\n      optimisticData: T | ((prev: T) => T),\n      asyncOperation: () => Promise<T>\n    ) => {\n      // Store rollback point\n      rollbackRef.current = state.data;\n      retryCountRef.current = 0;\n\n      // Apply optimistic update immediately\n      const newData = typeof optimisticData === 'function'\n        ? (optimisticData as (prev: T) => T)(state.data)\n        : optimisticData;\n\n      setState({\n        data: newData,\n        isUpdating: true,\n        error: null,\n      });\n\n      const attemptOperation = async (): Promise<void> => {\n        try {\n          // Perform actual async operation\n          const serverData = await asyncOperation();\n          \n          // Update with server-confirmed data\n          setState({\n            data: serverData,\n            isUpdating: false,\n            error: null,\n          });\n          \n          onSuccess?.(serverData);\n        } catch (error) {\n          const err = error instanceof Error ? error : new Error(String(error));\n          \n          // Retry logic\n          if (retryCountRef.current < retryCount) {\n            retryCountRef.current++;\n            await new Promise(resolve => setTimeout(resolve, retryDelay));\n            return attemptOperation();\n          }\n          \n          // Rollback on failure\n          setState({\n            data: rollbackRef.current,\n            isUpdating: false,\n            error: err,\n          });\n          \n          onError?.(err, rollbackRef.current);\n        }\n      };\n\n      await attemptOperation();\n    },\n    [state.data, onSuccess, onError, retryCount, retryDelay]\n  );\n\n  const reset = useCallback(() => {\n    setState({\n      data: rollbackRef.current,\n      isUpdating: false,\n      error: null,\n    });\n  }, []);\n\n  return {\n    data: state.data,\n    isUpdating: state.isUpdating,\n    error: state.error,\n    update,\n    reset,\n  };\n}\n\n// Anv√§ndning:\nfunction TodoList() {\n  const {\n    data: todos,\n    isUpdating,\n    error,\n    update,\n  } = useOptimisticUpdate<Todo[]>([], {\n    onError: (error) => toast.error(`Failed: ${error.message}`),\n    retryCount: 2,\n  });\n\n  const addTodo = async (text: string) => {\n    const optimisticTodo: Todo = {\n      id: `temp-${Date.now()}`,\n      text,\n      completed: false,\n    };\n\n    await update(\n      // Optimistic update\n      (prev) => [...prev, optimisticTodo],\n      // Actual API call\n      async () => {\n        const response = await fetch('/api/todos', {\n          method: 'POST',\n          body: JSON.stringify({ text }),\n        });\n        return response.json();\n      }\n    );\n  };\n\n  return (\n    <div className={isUpdating ? 'opacity-50' : ''}>\n      {todos.map(todo => (\n        <TodoItem key={todo.id} todo={todo} />\n      ))}\n      {error && <p className=\"text-red-500\">{error.message}</p>}\n    </div>\n  );\n}\n```"}
{"instruction": "Implementera en Python circuit breaker pattern f√∂r external API calls", "output": "```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom typing import Callable, TypeVar, Generic, Optional\nfrom dataclasses import dataclass, field\nfrom functools import wraps\n\nT = TypeVar('T')\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject calls\n    HALF_OPEN = \"half_open\"  # Testing if recovered\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5\n    success_threshold: int = 2\n    timeout_seconds: float = 30.0\n    half_open_max_calls: int = 3\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    half_open_calls: int = 0\n\nclass CircuitBreakerError(Exception):\n    \"\"\"Raised when circuit is open.\"\"\"\n    pass\n\nclass CircuitBreaker:\n    \"\"\"\n    Circuit breaker for protecting external service calls.\n    \n    States:\n    - CLOSED: Normal operation, tracking failures\n    - OPEN: Service unhealthy, rejecting calls\n    - HALF_OPEN: Testing if service recovered\n    \"\"\"\n    \n    def __init__(self, name: str, config: CircuitBreakerConfig = None):\n        self.name = name\n        self.config = config or CircuitBreakerConfig()\n        self.state = CircuitState.CLOSED\n        self.stats = CircuitStats()\n        self._lock = asyncio.Lock()\n    \n    async def call(self, func: Callable[..., T], *args, **kwargs) -> T:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        async with self._lock:\n            self._check_state_transition()\n            \n            if self.state == CircuitState.OPEN:\n                raise CircuitBreakerError(\n                    f\"Circuit '{self.name}' is OPEN. \"\n                    f\"Retry after {self._time_until_retry():.1f}s\"\n                )\n            \n            if self.state == CircuitState.HALF_OPEN:\n                if self.stats.half_open_calls >= self.config.half_open_max_calls:\n                    raise CircuitBreakerError(\n                        f\"Circuit '{self.name}' is HALF_OPEN, max test calls reached\"\n                    )\n                self.stats.half_open_calls += 1\n        \n        try:\n            result = await func(*args, **kwargs)\n            await self._on_success()\n            return result\n        except Exception as e:\n            await self._on_failure()\n            raise\n    \n    async def _on_success(self):\n        async with self._lock:\n            self.stats.successes += 1\n            \n            if self.state == CircuitState.HALF_OPEN:\n                if self.stats.successes >= self.config.success_threshold:\n                    self._close()\n            elif self.state == CircuitState.CLOSED:\n                # Reset failure count on success\n                self.stats.failures = 0\n    \n    async def _on_failure(self):\n        async with self._lock:\n            self.stats.failures += 1\n            self.stats.last_failure_time = time.monotonic()\n            \n            if self.state == CircuitState.HALF_OPEN:\n                self._open()\n            elif self.state == CircuitState.CLOSED:\n                if self.stats.failures >= self.config.failure_threshold:\n                    self._open()\n    \n    def _check_state_transition(self):\n        \"\"\"Check if we should transition from OPEN to HALF_OPEN.\"\"\"\n        if self.state == CircuitState.OPEN:\n            if self._time_until_retry() <= 0:\n                self._half_open()\n    \n    def _time_until_retry(self) -> float:\n        if self.stats.last_failure_time is None:\n            return 0\n        elapsed = time.monotonic() - self.stats.last_failure_time\n        return max(0, self.config.timeout_seconds - elapsed)\n    \n    def _open(self):\n        self.state = CircuitState.OPEN\n        self.stats.half_open_calls = 0\n        print(f\"Circuit '{self.name}' OPENED after {self.stats.failures} failures\")\n    \n    def _close(self):\n        self.state = CircuitState.CLOSED\n        self.stats = CircuitStats()\n        print(f\"Circuit '{self.name}' CLOSED - service recovered\")\n    \n    def _half_open(self):\n        self.state = CircuitState.HALF_OPEN\n        self.stats.successes = 0\n        self.stats.half_open_calls = 0\n        print(f\"Circuit '{self.name}' HALF_OPEN - testing recovery\")\n\n\n# Decorator version\ndef circuit_breaker(name: str, config: CircuitBreakerConfig = None):\n    cb = CircuitBreaker(name, config)\n    \n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await cb.call(func, *args, **kwargs)\n        wrapper.circuit = cb  # Expose for monitoring\n        return wrapper\n    return decorator\n\n\n# Anv√§ndning:\n@circuit_breaker(\"ollama-api\", CircuitBreakerConfig(\n    failure_threshold=3,\n    timeout_seconds=60.0\n))\nasync def call_ollama(prompt: str) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:11434/api/generate\",\n            json={\"model\": \"qwen2.5-coder:14b\", \"prompt\": prompt},\n            timeout=30.0\n        )\n        return response.json()[\"response\"]\n\n# Check circuit status\nprint(call_ollama.circuit.state)  # CircuitState.CLOSED\n```"}
{"instruction": "Skapa en n8n workflow f√∂r intelligent content repurposing across platforms", "output": "```json\n{\n  \"name\": \"AI Content Repurposing Engine\",\n  \"nodes\": [\n    {\n      \"name\": \"New Blog Post Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": {\n        \"path\": \"new-content\",\n        \"httpMethod\": \"POST\"\n      }\n    },\n    {\n      \"name\": \"Analyze Content\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Analysera inneh√•llet och extrahera:\\n1. Huvudbudskap (1 mening)\\n2. Nyckelpo√§nger (3-5 bullets)\\n3. M√•lgrupp\\n4. Ton (professional/casual/technical)\\n5. Hashtags (5 relevanta)\\n6. Hook (attention-grabbing opener)\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Titel: {{$json.title}}\\n\\nInneh√•ll:\\n{{$json.content}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate Twitter Thread\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa en Twitter-tr√•d (5-7 tweets) fr√•n inneh√•llet.\\n\\nRegler:\\n- Tweet 1: Hook med emoji\\n- Tweet 2-6: Nyckelpo√§nger (max 280 tecken/tweet)\\n- Tweet 7: Call-to-action + l√§nk\\n\\nFormat: JSON array med tweets\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Nyckelpo√§nger: {{$json.key_points}}\\nHook: {{$json.hook}}\\nHashtags: {{$json.hashtags}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate LinkedIn Post\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa ett LinkedIn-inl√§gg fr√•n inneh√•llet.\\n\\nStruktur:\\n1. Hook (f√∂rsta raden, attention-grabbing)\\n2. Problem/kontext (2-3 meningar)\\n3. L√∂sning/insight (huvudinneh√•ll)\\n4. Key takeaways (bullets)\\n5. Call-to-action\\n6. Hashtags\\n\\nTon: Professional men personlig\\nL√§ngd: 1000-1500 tecken\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analys: {{$node['Analyze Content'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate Newsletter Snippet\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa ett newsletter-snippet.\\n\\nStruktur:\\n- Subject line (max 50 tecken, nyfikenhetsskapande)\\n- Preview text (max 100 tecken)\\n- Intro paragraph (personlig ton)\\n- 3 bullet points\\n- Read more CTA\\n\\nFormat: JSON\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analys: {{$node['Analyze Content'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Post to Twitter\",\n      \"type\": \"n8n-nodes-base.twitter\",\n      \"parameters\": {\n        \"operation\": \"createThread\",\n        \"tweets\": \"={{$json.tweets}}\"\n      }\n    },\n    {\n      \"name\": \"Post to LinkedIn\",\n      \"type\": \"n8n-nodes-base.linkedIn\",\n      \"parameters\": {\n        \"operation\": \"share\",\n        \"text\": \"={{$json.linkedin_post}}\"\n      }\n    },\n    {\n      \"name\": \"Save to Notion\",\n      \"type\": \"n8n-nodes-base.notion\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"database\": \"Content Library\",\n        \"properties\": {\n          \"Title\": \"={{$json.title}}\",\n          \"Twitter\": \"={{$json.twitter_url}}\",\n          \"LinkedIn\": \"={{$json.linkedin_url}}\",\n          \"Newsletter\": \"={{$json.newsletter_snippet}}\"\n        }\n      }\n    },\n    {\n      \"name\": \"Notify Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#content\",\n        \"text\": \"üì¢ *Content Repurposed!*\\n\\n*Original:* {{$json.title}}\\n\\n‚úÖ Twitter thread posted\\n‚úÖ LinkedIn post shared\\n‚úÖ Newsletter snippet saved\\n\\nView in Notion: {{$json.notion_url}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. Tar emot nytt blogginl√§gg via webhook\\n2. Analyserar inneh√•llet med AI\\n3. Genererar parallellt: Twitter-tr√•d, LinkedIn-post, Newsletter-snippet\\n4. Publicerar automatiskt till respektive plattform\\n5. Sparar allt i Notion content library\\n6. Notifierar teamet p√• Slack"}
{"instruction": "Implementera health check endpoint med detaljerad systemstatus", "output": "```python\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional\nfrom enum import Enum\nimport asyncio\nimport httpx\nimport psutil\nimport time\nfrom datetime import datetime\n\nclass HealthStatus(str, Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\nclass ComponentHealth(BaseModel):\n    name: str\n    status: HealthStatus\n    latency_ms: Optional[float] = None\n    message: Optional[str] = None\n    details: Optional[Dict] = None\n\nclass SystemHealth(BaseModel):\n    status: HealthStatus\n    timestamp: str\n    uptime_seconds: float\n    components: List[ComponentHealth]\n    system: Dict\n\napp = FastAPI()\nstart_time = time.time()\n\nasync def check_ollama() -> ComponentHealth:\n    \"\"\"Check Ollama API health.\"\"\"\n    start = time.perf_counter()\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                \"http://localhost:11434/api/tags\",\n                timeout=5.0\n            )\n            latency = (time.perf_counter() - start) * 1000\n            \n            if response.status_code == 200:\n                models = response.json().get(\"models\", [])\n                return ComponentHealth(\n                    name=\"ollama\",\n                    status=HealthStatus.HEALTHY,\n                    latency_ms=round(latency, 2),\n                    details={\"loaded_models\": len(models)}\n                )\n            else:\n                return ComponentHealth(\n                    name=\"ollama\",\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Status code: {response.status_code}\"\n                )\n    except Exception as e:\n        return ComponentHealth(\n            name=\"ollama\",\n            status=HealthStatus.UNHEALTHY,\n            message=str(e)\n        )\n\nasync def check_gpu() -> ComponentHealth:\n    \"\"\"Check GPU health and memory.\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total,utilization.gpu,temperature.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True, timeout=5\n        )\n        \n        if result.returncode == 0:\n            parts = result.stdout.strip().split(\", \")\n            mem_used, mem_total = int(parts[0]), int(parts[1])\n            mem_percent = (mem_used / mem_total) * 100\n            \n            status = HealthStatus.HEALTHY\n            if mem_percent > 90:\n                status = HealthStatus.DEGRADED\n            if mem_percent > 98:\n                status = HealthStatus.UNHEALTHY\n            \n            return ComponentHealth(\n                name=\"gpu\",\n                status=status,\n                details={\n                    \"memory_used_mb\": mem_used,\n                    \"memory_total_mb\": mem_total,\n                    \"memory_percent\": round(mem_percent, 1),\n                    \"utilization_percent\": int(parts[2]),\n                    \"temperature_c\": int(parts[3])\n                }\n            )\n        else:\n            return ComponentHealth(\n                name=\"gpu\",\n                status=HealthStatus.UNHEALTHY,\n                message=\"nvidia-smi failed\"\n            )\n    except Exception as e:\n        return ComponentHealth(\n            name=\"gpu\",\n            status=HealthStatus.UNHEALTHY,\n            message=str(e)\n        )\n\nasync def check_disk() -> ComponentHealth:\n    \"\"\"Check disk space.\"\"\"\n    disk = psutil.disk_usage(\"/\")\n    percent_used = disk.percent\n    \n    status = HealthStatus.HEALTHY\n    if percent_used > 85:\n        status = HealthStatus.DEGRADED\n    if percent_used > 95:\n        status = HealthStatus.UNHEALTHY\n    \n    return ComponentHealth(\n        name=\"disk\",\n        status=status,\n        details={\n            \"total_gb\": round(disk.total / (1024**3), 1),\n            \"used_gb\": round(disk.used / (1024**3), 1),\n            \"free_gb\": round(disk.free / (1024**3), 1),\n            \"percent_used\": percent_used\n        }\n    )\n\n@app.get(\"/health\", response_model=SystemHealth)\nasync def health_check(response: Response):\n    \"\"\"Comprehensive health check endpoint.\"\"\"\n    \n    # Run all checks in parallel\n    components = await asyncio.gather(\n        check_ollama(),\n        check_gpu(),\n        check_disk()\n    )\n    \n    # Determine overall status\n    statuses = [c.status for c in components]\n    if HealthStatus.UNHEALTHY in statuses:\n        overall = HealthStatus.UNHEALTHY\n        response.status_code = 503\n    elif HealthStatus.DEGRADED in statuses:\n        overall = HealthStatus.DEGRADED\n        response.status_code = 200\n    else:\n        overall = HealthStatus.HEALTHY\n        response.status_code = 200\n    \n    # System metrics\n    system = {\n        \"cpu_percent\": psutil.cpu_percent(),\n        \"memory_percent\": psutil.virtual_memory().percent,\n        \"load_average\": list(psutil.getloadavg())\n    }\n    \n    return SystemHealth(\n        status=overall,\n        timestamp=datetime.utcnow().isoformat() + \"Z\",\n        uptime_seconds=round(time.time() - start_time, 2),\n        components=list(components),\n        system=system\n    )\n\n# Liveness probe (simple)\n@app.get(\"/health/live\")\nasync def liveness():\n    return {\"status\": \"alive\"}\n\n# Readiness probe (checks dependencies)\n@app.get(\"/health/ready\")\nasync def readiness(response: Response):\n    ollama = await check_ollama()\n    if ollama.status == HealthStatus.UNHEALTHY:\n        response.status_code = 503\n        return {\"status\": \"not ready\", \"reason\": \"ollama unavailable\"}\n    return {\"status\": \"ready\"}\n```"}
{"instruction": "Skapa en TypeScript utility f√∂r type-safe environment variables", "output": "```typescript\nimport { z } from 'zod';\n\n// Define environment schema\nconst envSchema = z.object({\n  // Server\n  NODE_ENV: z.enum(['development', 'production', 'test']).default('development'),\n  PORT: z.string().transform(Number).default('3000'),\n  HOST: z.string().default('0.0.0.0'),\n  \n  // Database\n  DATABASE_URL: z.string().url(),\n  DATABASE_POOL_SIZE: z.string().transform(Number).default('10'),\n  \n  // Ollama\n  OLLAMA_BASE_URL: z.string().url().default('http://localhost:11434'),\n  OLLAMA_MODEL: z.string().default('qwen2.5-coder:14b'),\n  OLLAMA_TIMEOUT_MS: z.string().transform(Number).default('60000'),\n  \n  // Auth (optional)\n  JWT_SECRET: z.string().min(32).optional(),\n  JWT_EXPIRES_IN: z.string().default('7d'),\n  \n  // Feature flags\n  ENABLE_STREAMING: z.string().transform(v => v === 'true').default('true'),\n  ENABLE_CACHE: z.string().transform(v => v === 'true').default('true'),\n  \n  // Debug\n  LOG_LEVEL: z.enum(['debug', 'info', 'warn', 'error']).default('info'),\n  DEBUG: z.string().transform(v => v === 'true').default('false'),\n});\n\ntype Env = z.infer<typeof envSchema>;\n\nclass EnvironmentManager {\n  private env: Env | null = null;\n  private validated = false;\n\n  /**\n   * Validate and parse environment variables.\n   * Call this at application startup.\n   */\n  validate(): Env {\n    if (this.validated && this.env) {\n      return this.env;\n    }\n\n    const result = envSchema.safeParse(process.env);\n\n    if (!result.success) {\n      const errors = result.error.issues\n        .map(issue => `  - ${issue.path.join('.')}: ${issue.message}`)\n        .join('\\n');\n      \n      console.error('‚ùå Environment validation failed:\\n' + errors);\n      \n      // In production, exit immediately\n      if (process.env.NODE_ENV === 'production') {\n        process.exit(1);\n      }\n      \n      throw new Error('Invalid environment configuration');\n    }\n\n    this.env = result.data;\n    this.validated = true;\n\n    // Log in development\n    if (this.env.NODE_ENV === 'development') {\n      console.log('‚úÖ Environment validated:', {\n        NODE_ENV: this.env.NODE_ENV,\n        PORT: this.env.PORT,\n        OLLAMA_MODEL: this.env.OLLAMA_MODEL,\n        LOG_LEVEL: this.env.LOG_LEVEL,\n      });\n    }\n\n    return this.env;\n  }\n\n  /**\n   * Get validated environment.\n   * Throws if validate() hasn't been called.\n   */\n  get(): Env {\n    if (!this.validated || !this.env) {\n      throw new Error(\n        'Environment not validated. Call env.validate() at startup.'\n      );\n    }\n    return this.env;\n  }\n\n  /**\n   * Check if running in production.\n   */\n  isProduction(): boolean {\n    return this.get().NODE_ENV === 'production';\n  }\n\n  /**\n   * Check if running in development.\n   */\n  isDevelopment(): boolean {\n    return this.get().NODE_ENV === 'development';\n  }\n\n  /**\n   * Get a safe subset for client-side (no secrets).\n   */\n  getPublicEnv() {\n    const env = this.get();\n    return {\n      NODE_ENV: env.NODE_ENV,\n      ENABLE_STREAMING: env.ENABLE_STREAMING,\n    };\n  }\n}\n\n// Singleton instance\nexport const env = new EnvironmentManager();\n\n// Type helper for components\nexport type { Env };\n\n// Usage:\n// 1. At app startup:\n//    import { env } from './env';\n//    env.validate();\n//\n// 2. Anywhere else:\n//    const config = env.get();\n//    console.log(config.OLLAMA_MODEL); // Type-safe!\n//\n// 3. In tests:\n//    process.env.DATABASE_URL = 'postgresql://test:test@localhost/test';\n//    env.validate();\n```"}
{"instruction": "Implementera en Python-funktion: Get cached settings instance", "output": "```python\ndef get_settings() -> Settings:\n    \"\"\"Get cached settings instance\"\"\"\n    return Settings()\n```\n\nDenna funktion finns i `app/config.py`."}
{"instruction": "Implementera en Python-klass: Application settings with environment variable support", "output": "```python\nclass Settings(BaseSettings):\n    \"\"\"Application settings with environment variable support\"\"\"\n\n    # Application\n    app_name: str = \"Simons AI Backend\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n\n    # Server\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n\n    # Ollama (local)\n    ollama_base_url: str = \"http://localhost:11434\"\n    ollama_timeout_seconds: int = 120\n\n    # Gemini (cloud - FAST mode)\n    gemini_api_key: Optional[str] = None\n    gemini_model: str = \"gemini-2.0-flash-exp\"\n\n    # Hybrid Orchestrator\n    default_mode: str = \"auto\"  # auto, fast, deep\n\n    # CORS - explicit origins needed when allow_credentials=True\n    cors_origins: list[str] = [\n        \"http://localhost:5173\",\n        \"http://127.0.0.1:5173\",\n        \"http://192.168.86.32:5173\",\n        \"http://localhost:8000\",\n        \"http://127.0.0.1:8000\",\n        \"http://192.168.86.32:8000\",\n    ]\n    cors_allow_credentials: bool = True\n\n    # Logging\n    log_level: str = \"INFO\"\n    log_json: bool = False\n    log_file: Optional[str] = None\n\n    # WebSocket\n    ws_heartbeat_interval: int = 30  # seconds\n    ws_max_message_size: int = 65536  # bytes\n\n    # Model defaults\n    default_profile: str = \"chill\"\n    warmup_on_startup: bool = False\n    warmup_profile: str = \"chill\"\n\n    class Config:\n        env_prefix = \"LLM_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n```\n\nKlassen finns i `app/config.py`."}
{"instruction": "Skapa en Python-klass f√∂r config", "output": "```python\n    class Config:\n        env_prefix = \"LLM_\"\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n```\n\nKlassen finns i `app/config.py`."}
{"instruction": "Implementera en Python-funktion: Application lifespan manager", "output": "```python\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    Application lifespan manager.\n    Handles startup and shutdown events.\n    \"\"\"\n    # Startup\n    logger.info(\"=\" * 60)\n    logger.info(f\"  {settings.app_name} v{settings.app_version}\")\n    logger.info(\"=\" * 60)\n\n    # Check Ollama connection\n    ollama_ok = await ollama_client.is_connected()\n    if ollama_ok:\n        logger.info(\"Ollama connection: OK\")\n        models = await ollama_client.list_models()\n        logger.info(f\"Available models: {', '.join(models) if models else 'None'}\")\n    else:\n        logger.warning(\"Ollama connection: FAILED - Start with 'ollama serve'\")\n\n    # Check GPU\n    gpu_ok = await gpu_monitor.is_gpu_available()\n    if gpu_ok:\n        stats = await gpu_monitor.get_stats()\n        logger.info(f\"GPU: {stats.name}\")\n        logger.info(f\"VRAM: {stats.vram_used_gb:.1f}/{stats.vram_total_gb:.1f} GB\")\n    else:\n        logger.warning(\"GPU monitoring: FAILED - nvidia-smi not available\")\n\n    # Optional warmup\n    if settings.warmup_on_startup and ollama_ok:\n        from .models.Backend_Agent_Prompts import get_profile\n        profile = get_profile(settings.warmup_profile)\n        logger.info(f\"Warming up model: {profile.model}\")\n        await ollama_client.warmup_model(profile)\n\n    logger.info(f\"Server starting on http://{settings.host}:{settings.port}\")\n    logger.info(\"=\" * 60)\n\n    yield\n\n    # Shutdown\n    logger.info(\"Shutting down...\")\n    stop_gpu_broadcast()  # Stop GPU telemetry broadcast\n    stop_status_pulse()   # Stop status pulse\n    await ollama_client.close()\n```\n\nDenna funktion finns i `app/main.py`."}
{"instruction": "Implementera en Python-funktion: Root endpoint - basic info", "output": "```python\nasync def root():\n    \"\"\"Root endpoint - basic info\"\"\"\n    return {\n        \"name\": settings.app_name,\n        \"version\": settings.app_version,\n        \"docs\": \"/docs\",\n        \"health\": \"/api/health\",\n        \"profiles\": \"/api/profiles\",\n        \"gpu\": \"/api/gpu/stats\",\n        \"websocket\": \"ws://localhost:8000/api/chat\",\n        \"cascade\": \"ws://localhost:8000/api/cascade\",\n    }\n```\n\nDenna funktion finns i `app/main.py`."}
{"instruction": "Implementera en Python-funktion: Configure logging for the application", "output": "```python\ndef setup_logging(\n    level: str = \"INFO\",\n    json_output: bool = False,\n    log_file: Optional[str] = None\n) -> None:\n    \"\"\"\n    Configure logging for the application.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR)\n        json_output: Use JSON format instead of colored console\n        log_file: Optional file path for logging\n    \"\"\"\n    root_logger = logging.getLogger()\n    root_logger.setLevel(getattr(logging, level.upper()))\n\n    # Clear existing handlers\n    root_logger.handlers.clear()\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    if json_output:\n        console_handler.setFormatter(JSONFormatter())\n    else:\n        console_handler.setFormatter(ColoredFormatter(\n            fmt=\"%(timestamp)s ‚îÇ %(levelname)-8s ‚îÇ %(module_name)-15s ‚îÇ %(message)s\"\n        ))\n\n    root_logger.addHandler(console_handler)\n\n    # File handler (if specified)\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(JSONFormatter())\n        root_logger.addHandler(file_handler)\n\n    # Reduce noise from third-party libraries\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n    logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-funktion: Get a logger instance with the given name", "output": "```python\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"\n    Get a logger instance with the given name.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som format", "output": "```python\n    def format(self, record: logging.LogRecord) -> str:\n        # Add color to level name\n        level_color = self.COLORS.get(record.levelname, \"\")\n        record.levelname = f\"{level_color}{record.levelname}{self.RESET}\"\n\n        # Add timestamp\n        record.timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Format module name\n        record.module_name = record.name.split(\".\")[-1]\n\n        return super().format(record)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som debug", "output": "```python\n    def debug(self, msg: str, **kwargs) -> None:\n        self._log(logging.DEBUG, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som info", "output": "```python\n    def info(self, msg: str, **kwargs) -> None:\n        self._log(logging.INFO, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som warning", "output": "```python\n    def warning(self, msg: str, **kwargs) -> None:\n        self._log(logging.WARNING, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Skapa en Python-funktion som error", "output": "```python\n    def error(self, msg: str, **kwargs) -> None:\n        self._log(logging.ERROR, msg, **kwargs)\n```\n\nDenna funktion finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: Custom formatter with colors for console output", "output": "```python\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Custom formatter with colors for console output\"\"\"\n\n    # ANSI color codes\n    COLORS = {\n        \"DEBUG\": \"\\033[36m\",    # Cyan\n        \"INFO\": \"\\033[32m\",     # Green\n        \"WARNING\": \"\\033[33m\",  # Yellow\n        \"ERROR\": \"\\033[31m\",    # Red\n        \"CRITICAL\": \"\\033[35m\", # Magenta\n    }\n    RESET = \"\\033[0m\"\n    BOLD = \"\\033[1m\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        # Add color to level name\n        level_color = self.COLORS.get(record.levelname, \"\")\n        record.levelname = f\"{level_color}{record.levelname}{self.RESET}\"\n\n        # Add timestamp\n        record.timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Format module name\n        record.module_name = record.name.split(\".\")[-1]\n\n        return super().format(record)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: JSON formatter for production logging", "output": "```python\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for production logging\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        import json\n\n        log_data = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n        }\n\n        # Add extra fields if present\n        if hasattr(record, \"request_id\"):\n            log_data[\"request_id\"] = record.request_id\n        if hasattr(record, \"profile\"):\n            log_data[\"profile\"] = record.profile\n        if hasattr(record, \"model\"):\n            log_data[\"model\"] = record.model\n\n        # Add exception info if present\n        if record.exc_info:\n            log_data[\"exception\"] = self.formatException(record.exc_info)\n\n        return json.dumps(log_data)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-klass: Context manager for request-scoped logging", "output": "```python\nclass RequestLogger:\n    \"\"\"Context manager for request-scoped logging\"\"\"\n\n    def __init__(self, logger: logging.Logger, request_id: str, **extra):\n        self.logger = logger\n        self.request_id = request_id\n        self.extra = extra\n\n    def _log(self, level: int, msg: str, **kwargs) -> None:\n        extra = {\"request_id\": self.request_id, **self.extra, **kwargs}\n        self.logger.log(level, f\"[{self.request_id[:8]}] {msg}\", extra=extra)\n\n    def debug(self, msg: str, **kwargs) -> None:\n        self._log(logging.DEBUG, msg, **kwargs)\n\n    def info(self, msg: str, **kwargs) -> None:\n        self._log(logging.INFO, msg, **kwargs)\n\n    def warning(self, msg: str, **kwargs) -> None:\n        self._log(logging.WARNING, msg, **kwargs)\n\n    def error(self, msg: str, **kwargs) -> None:\n        self._log(logging.ERROR, msg, **kwargs)\n```\n\nKlassen finns i `app/utils/logging.py`."}
{"instruction": "Implementera en Python-funktion: Get profile by ID with fallback to QWEN-SONNET", "output": "```python\ndef get_profile(profile_id: str) -> ModelProfile:\n    \"\"\"Get profile by ID with fallback to QWEN-SONNET.\"\"\"\n    try:\n        return PROFILES[ProfileId(profile_id.lower())]\n    except (ValueError, KeyError):\n        return PROFILES[ProfileId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get list of all available profiles", "output": "```python\ndef get_all_profiles() -> list[ModelProfile]:\n    \"\"\"Get list of all available profiles.\"\"\"\n    return list(PROFILES.values())\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Find profile by Ollama model name", "output": "```python\ndef get_profile_by_model(model_name: str) -> Optional[ModelProfile]:\n    \"\"\"Find profile by Ollama model name.\"\"\"\n    for profile in PROFILES.values():\n        if profile.model == model_name:\n            return profile\n    return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get the default profile", "output": "```python\ndef get_default_profile() -> ModelProfile:\n    \"\"\"Get the default profile.\"\"\"\n    return PROFILES[ProfileId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Calculate total VRAM needed for all profiles loaded", "output": "```python\ndef calculate_total_vram() -> float:\n    \"\"\"Calculate total VRAM needed for all profiles loaded.\"\"\"\n    return sum(p.estimated_vram_gb for p in PROFILES.values())\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Skapa en Python-funktion som active profile", "output": "```python\n    def active_profile(self) -> Optional[ModelProfile]:\n        if self._active_profile:\n            return PROFILES.get(self._active_profile)\n        return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Resolve a profile ID to a ProfileId enum, or None if not found", "output": "```python\n    def _resolve_profile_id(self, profile_id) -> Optional[ProfileId]:\n        \"\"\"Resolve a profile ID to a ProfileId enum, or None if not found.\"\"\"\n        if isinstance(profile_id, ProfileId):\n            return profile_id\n        try:\n            return ProfileId(str(profile_id).lower())\n        except ValueError:\n            return None\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Mark a profile as loaded in VRAM", "output": "```python\n    def set_loaded(self, profile_id, loaded: bool = True) -> None:\n        \"\"\"Mark a profile as loaded in VRAM.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loaded = loaded\n        self._status[pid].is_loading = False\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get status for a specific profile", "output": "```python\n    def get_status(self, profile_id: ProfileId) -> ProfileStatus:\n        \"\"\"Get status for a specific profile.\"\"\"\n        return self._status[profile_id]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get status for all profiles", "output": "```python\n    def get_all_status(self) -> dict[ProfileId, ProfileStatus]:\n        \"\"\"Get status for all profiles.\"\"\"\n        return self._status.copy()\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-funktion: Get list of currently loaded profiles", "output": "```python\n    def get_loaded_profiles(self) -> list[ProfileId]:\n        \"\"\"Get list of currently loaded profiles.\"\"\"\n        return [pid for pid, status in self._status.items() if status.is_loaded]\n```\n\nDenna funktion finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Available profile identifiers", "output": "```python\nclass ProfileId(str, Enum):\n    \"\"\"Available profile identifiers\"\"\"\n    QWEN_SONNET = \"qwen-sonnet\"          # Daily Coder\n    DEVSTRAL_OPENHANDS = \"devstral-openhands\"  # Wave Primary\n    QWEN_OPENHANDS = \"qwen-openhands\"    # Wave Backup\n    # Legacy aliases\n    GPT_OSS = \"gpt-oss\"\n    DEVSTRAL = \"devstral\"\n    QWEN = \"qwen\"\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Configuration for an AI model profile (UI metadata only)", "output": "```python\nclass ModelProfile:\n    \"\"\"Configuration for an AI model profile (UI metadata only)\"\"\"\n    id: ProfileId\n    name: str\n    display_name: str\n    description: str\n    model: str  # Ollama model name\n    temperature: float\n    max_tokens: int\n    context_length: int\n    estimated_vram_gb: float\n    top_p: float = 0.9\n    repeat_penalty: float = 1.1\n    icon: str = \"robot\"\n    color: str = \"#FF4FD8\"\n    strengths: list[str] = None\n\n    def __post_init__(self):\n        if self.strengths is None:\n            self.strengths = []\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Runtime status for a profile", "output": "```python\nclass ProfileStatus:\n    \"\"\"Runtime status for a profile\"\"\"\n    profile_id: ProfileId\n    is_loaded: bool = False\n    is_loading: bool = False\n    last_used: Optional[str] = None\n    load_time_ms: Optional[int] = None\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: Manages profile state for dual-model setup", "output": "```python\nclass ProfileManager:\n    \"\"\"Manages profile state for dual-model setup.\"\"\"\n\n    def __init__(self):\n        self._status: dict[ProfileId, ProfileStatus] = {\n            pid: ProfileStatus(profile_id=pid)\n            for pid in ProfileId\n        }\n        self._active_profile: Optional[ProfileId] = None\n\n    @property\n    def active_profile(self) -> Optional[ModelProfile]:\n        if self._active_profile:\n            return PROFILES.get(self._active_profile)\n        return None\n\n    def _resolve_profile_id(self, profile_id) -> Optional[ProfileId]:\n        \"\"\"Resolve a profile ID to a ProfileId enum, or None if not found.\"\"\"\n        if isinstance(profile_id, ProfileId):\n            return profile_id\n        try:\n            return ProfileId(str(profile_id).lower())\n        except ValueError:\n            return None\n\n    def set_loaded(self, profile_id, loaded: bool = True) -> None:\n        \"\"\"Mark a profile as loaded in VRAM.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loaded = loaded\n        self._status[pid].is_loading = False\n\n    def set_active(self, profile_id) -> None:\n        \"\"\"Mark a profile as currently active (being used).\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._active_profile = pid\n        self._status[pid].is_loaded = True\n        self._status[pid].is_loading = False\n\n    def set_loading(self, profile_id) -> None:\n        \"\"\"Mark a profile as currently loading.\"\"\"\n        pid = self._resolve_profile_id(profile_id)\n        if pid is None:\n            return\n        self._status[pid].is_loading = True\n\n    def get_status(self, profile_id: ProfileId) -> ProfileStatus:\n        \"\"\"Get status for a specific profile.\"\"\"\n        return self._status[profile_id]\n\n    def get_all_status(self) -> dict[ProfileId, ProfileStatus]:\n        \"\"\"Get status for all profiles.\"\"\"\n        return self._status.copy()\n\n    def get_loaded_profiles(self) -> list[ProfileId]:\n        \"\"\"Get list of currently loaded profiles.\"\"\"\n        return [pid for pid, status in self._status.items() if status.is_loaded]\n```\n\nKlassen finns i `app/models/Backend_Agent_Prompts.py`."}
{"instruction": "Implementera en Python-klass: WebSocket message types", "output": "```python\nclass WSMessageType(str, Enum):\n    \"\"\"WebSocket message types\"\"\"\n    # Client -> Server\n    CHAT = \"chat\"\n    PING = \"ping\"\n    SWITCH_PROFILE = \"switch_profile\"\n\n    # Server -> Client\n    START = \"start\"\n    TOKEN = \"token\"\n    DONE = \"done\"\n    ERROR = \"error\"\n    WARMUP = \"warmup\"\n    PONG = \"pong\"\n    PROFILE_CHANGED = \"profile_changed\"\n    GPU_UPDATE = \"gpu_update\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single chat message", "output": "```python\nclass ChatMessage(BaseModel):\n    \"\"\"Single chat message\"\"\"\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Chat request from client", "output": "```python\nclass ChatRequest(BaseModel):\n    \"\"\"Chat request from client\"\"\"\n    profile: str = Field(default=\"qwen\", description=\"Profile ID: qwen or gemma\")\n    mode: str = Field(default=\"auto\", description=\"Inference mode: auto, fast, deep\")\n    messages: list[ChatMessage] = Field(default_factory=list)\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"profile\": \"qwen\",\n                \"mode\": \"auto\",\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"F√∂rklara async/await i Python\"}\n                ],\n                \"request_id\": \"550e8400-e29b-41d4-a716-446655440000\"\n            }\n        }\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Statistics for completed chat response", "output": "```python\nclass ChatStats(BaseModel):\n    \"\"\"Statistics for completed chat response\"\"\"\n    tokens_generated: int\n    tokens_per_second: float\n    total_duration_ms: int\n    prompt_eval_count: Optional[int] = None\n    prompt_eval_duration_ms: Optional[int] = None\n    model: str\n    profile: str\n    provider: str = \"ollama\"  # ollama or gemini\n    mode: str = \"auto\"  # auto, fast, deep\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single token in streaming response", "output": "```python\nclass TokenChunk(BaseModel):\n    \"\"\"Single token in streaming response\"\"\"\n    type: Literal[\"token\"] = \"token\"\n    content: str\n    request_id: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Complete chat response (non-streaming)", "output": "```python\nclass ChatResponse(BaseModel):\n    \"\"\"Complete chat response (non-streaming)\"\"\"\n    type: Literal[\"done\"] = \"done\"\n    request_id: str\n    content: str\n    stats: ChatStats\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Sent when chat generation starts", "output": "```python\nclass WSStartMessage(BaseModel):\n    \"\"\"Sent when chat generation starts\"\"\"\n    type: Literal[\"start\"] = \"start\"\n    request_id: str\n    profile: str\n    model: str\n    display_name: str  # e.g., \"QWEN_NEXUS\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Single streamed token", "output": "```python\nclass WSTokenMessage(BaseModel):\n    \"\"\"Single streamed token\"\"\"\n    type: Literal[\"token\"] = \"token\"\n    content: str\n    request_id: str\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Sent when generation completes", "output": "```python\nclass WSDoneMessage(BaseModel):\n    \"\"\"Sent when generation completes\"\"\"\n    type: Literal[\"done\"] = \"done\"\n    request_id: str\n    stats: ChatStats\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Error message", "output": "```python\nclass WSErrorMessage(BaseModel):\n    \"\"\"Error message\"\"\"\n    type: Literal[\"error\"] = \"error\"\n    request_id: str\n    code: str\n    message: str\n    retry_after_ms: Optional[int] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Model loading/warmup status", "output": "```python\nclass WSWarmupMessage(BaseModel):\n    \"\"\"Model loading/warmup status\"\"\"\n    type: Literal[\"warmup\"] = \"warmup\"\n    request_id: str\n    status: Literal[\"loading\", \"ready\", \"unloading\"]\n    model: str\n    progress_percent: Optional[int] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Generic WebSocket message wrapper", "output": "```python\nclass WSMessage(BaseModel):\n    \"\"\"Generic WebSocket message wrapper\"\"\"\n    type: WSMessageType\n    request_id: Optional[str] = None\n    data: Optional[dict] = None\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Profile information for API response", "output": "```python\nclass ProfileInfo(BaseModel):\n    \"\"\"Profile information for API response\"\"\"\n    id: str\n    name: str\n    display_name: str\n    description: str\n    model: str\n    estimated_vram_gb: float\n    icon: str\n    color: str\n    strengths: list[str] = []\n    is_active: bool = False\n    is_loading: bool = False\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Response for GET /api/profiles", "output": "```python\nclass ProfilesResponse(BaseModel):\n    \"\"\"Response for GET /api/profiles\"\"\"\n    profiles: list[ProfileInfo]\n    active_profile: Optional[str] = None\n    default_profile: str = \"chill\"\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: GPU statistics from nvidia-smi", "output": "```python\nclass GPUStats(BaseModel):\n    \"\"\"GPU statistics from nvidia-smi\"\"\"\n    name: str = \"NVIDIA GeForce RTX 4070\"\n    vram_total_gb: float = 12.0\n    vram_used_gb: float = 0.0\n    vram_free_gb: float = 12.0\n    vram_percent: float = 0.0\n    temperature_c: int = 0\n    power_draw_w: int = 0\n    power_limit_w: int = 200\n    gpu_util_percent: int = 0\n    memory_util_percent: int = 0\n    fan_speed_percent: Optional[int] = None\n    is_available: bool = True\n    driver_version: Optional[str] = None\n    cuda_version: Optional[str] = None\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"name\": \"NVIDIA GeForce RTX 4070\",\n                \"vram_total_gb\": 12.0,\n                \"vram_used_gb\": 10.2,\n                \"vram_free_gb\": 1.8,\n                \"vram_percent\": 85.0,\n                \"temperature_c\": 58,\n                \"power_draw_w\": 185,\n                \"power_limit_w\": 200,\n                \"gpu_util_percent\": 95,\n                \"memory_util_percent\": 85,\n                \"is_available\": True\n            }\n        }\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: Ollama service status", "output": "```python\nclass OllamaStatus(BaseModel):\n    \"\"\"Ollama service status\"\"\"\n    connected: bool = False\n    version: Optional[str] = None\n    models_available: list[str] = Field(default_factory=list)\n    models_loaded: list[str] = Field(default_factory=list)\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-klass: System information for header display", "output": "```python\nclass SystemInfo(BaseModel):\n    \"\"\"System information for header display\"\"\"\n    ram_total_gb: float\n    ram_used_gb: float\n    cpu_percent: float\n    cpu_freq_ghz: Optional[float] = None\n    uptime_seconds: int\n```\n\nKlassen finns i `app/models/schemas.py`."}
{"instruction": "Implementera en Python-funktion: Extract JSON tool call from QWEN's response text", "output": "```python\ndef extract_tool_call(text: str) -> Optional[dict]:\n    \"\"\"\n    Extract JSON tool call from QWEN's response text.\n\n    QWEN will include tool calls like:\n        {\"tool\": \"docker_ps\"}\n        {\"tool\": \"docker_logs\", \"container\": \"nginx\", \"lines\": 50}\n\n    Returns:\n        dict with tool name and params, or None if no tool call found\n    \"\"\"\n    match = TOOL_JSON_PATTERN.search(text)\n    if not match:\n        return None\n\n    try:\n        data = json.loads(match.group())\n        if \"tool\" in data:\n            return data\n    except json.JSONDecodeError:\n        pass\n\n    return None\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Returns last 3 Q&A exchanges for dashboard display", "output": "```python\ndef get_active_context() -> list[dict]:\n    \"\"\"Returns last 3 Q&A exchanges for dashboard display\"\"\"\n    return [ex.to_dict() for ex in reversed(_mobile_context)]\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Add a completed Q&A exchange to the active context buffer", "output": "```python\ndef add_mobile_exchange(\n    request_id: str,\n    question: str,\n    answer: str,\n    profile: str,\n    stats: dict = None\n) -> None:\n    \"\"\"Add a completed Q&A exchange to the active context buffer\"\"\"\n    exchange = MobileExchange(\n        request_id=request_id,\n        question=question,\n        answer=answer,\n        profile=profile,\n        timestamp=datetime.utcnow().isoformat() + \"Z\",\n        stats=stats or {},\n    )\n    _mobile_context.append(exchange)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast mobile activity to all connected dashboard clients", "output": "```python\nasync def emit_mobile_activity(\n    event_type: str,\n    request_id: str,\n    data: dict\n) -> None:\n    \"\"\"\n    Broadcast mobile activity to all connected dashboard clients.\n\n    Event types:\n    - mobile_request_start: Mobile request initiated\n    - mobile_generation: Token generation progress\n    - mobile_response: Complete Q&A response\n    - mobile_shell: Shell command executed\n    \"\"\"\n    packet = {\n        \"type\": \"mobile_activity\",\n        \"event\": event_type,\n        \"request_id\": request_id,\n        \"source\": \"mobile\",\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n        **data\n    }\n    await manager.broadcast(packet)\n    logger.debug(f\"Mobile activity broadcast: {event_type} ({request_id})\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast War Room events to all dashboard clients for real-time sync", "output": "```python\nasync def emit_war_room_event(\n    event_type: str,\n    data: dict\n) -> None:\n    \"\"\"\n    Broadcast War Room events to all dashboard clients for real-time sync.\n\n    Event types:\n    - start: Mobile/WebSocket request initiated (shows user prompt)\n    - stream: Batched tokens (100ms intervals for smooth display)\n    - end: Response complete (shows stats)\n\n    Used by KioskDashboard to display mobile sessions in real-time.\n    \"\"\"\n    packet = {\n        \"type\": \"war_room_event\",\n        \"event\": event_type,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n        **data\n    }\n    await manager.broadcast(packet)\n    logger.debug(f\"War Room broadcast: {event_type}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Emit a system log event to all connected clients", "output": "```python\nasync def emit_system_log(\n    event: str,\n    message: str,\n    level: str = \"info\",\n    agent_id: str | None = None\n) -> None:\n    \"\"\"\n    Emit a system log event to all connected clients.\n\n    Args:\n        event: Event type (e.g., 'model_load', 'generation_start', 'error')\n        message: Human-readable log message\n        level: Log level ('info', 'warn', 'error', 'success')\n        agent_id: Optional agent that triggered this log\n    \"\"\"\n    global _system_logs\n\n    log_entry = {\n        \"type\": \"system_log\",\n        \"event\": event,\n        \"message\": message,\n        \"level\": level,\n        \"agent_id\": agent_id,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n\n    # Add to buffer (circular)\n    _system_logs.append(log_entry)\n    if len(_system_logs) > MAX_LOG_BUFFER:\n        _system_logs = _system_logs[-MAX_LOG_BUFFER:]\n\n    # Broadcast to all connections\n    await manager.broadcast(log_entry)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Get recent system logs for new connections", "output": "```python\ndef get_recent_logs(count: int = 20) -> list[dict]:\n    \"\"\"Get recent system logs for new connections\"\"\"\n    return _system_logs[-count:]\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Background task that broadcasts GPU stats to all connected clients", "output": "```python\nasync def gpu_telemetry_loop():\n    \"\"\"\n    Background task that broadcasts GPU stats to all connected clients.\n    Runs every GPU_BROADCAST_INTERVAL seconds.\n\n    Sends status_update packets with:\n    - GPU VRAM usage, temperature, utilization\n    - Number of active models in Ollama\n    \"\"\"\n    logger.info(\"GPU telemetry broadcast started\")\n\n    while True:\n        try:\n            await asyncio.sleep(GPU_BROADCAST_INTERVAL)\n\n            # Skip if no connections\n            if not manager.active_connections:\n                continue\n\n            # Fetch GPU stats\n            gpu_stats = await gpu_monitor.get_stats()\n\n            # Fetch active models count\n            running_models = await ollama_client.list_running_models()\n            active_models_count = len(running_models)\n\n            # Build status_update packet (matches frontend expectations)\n            status_packet = {\n                \"type\": \"status_update\",\n                \"gpu\": {\n                    \"vram_used_gb\": round(gpu_stats.vram_used_gb, 2),\n                    \"vram_total_gb\": round(gpu_stats.vram_total_gb, 2),\n                    \"vram_percent\": round(gpu_stats.vram_percent, 1),\n                    \"temperature_c\": gpu_stats.temperature_c,\n                    \"utilization_percent\": gpu_stats.gpu_util_percent,\n                    \"power_draw_w\": gpu_stats.power_draw_w,\n                    \"power_limit_w\": gpu_stats.power_limit_w,\n                    \"name\": gpu_stats.name,\n                    \"is_available\": gpu_stats.is_available,\n                },\n                \"active_models_count\": active_models_count,\n                \"active_models\": [m.get(\"name\", \"\") for m in running_models],\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n            # Broadcast to all connections\n            await manager.broadcast(status_packet)\n\n        except asyncio.CancelledError:\n            logger.info(\"GPU telemetry broadcast stopped\")\n            break\n        except Exception as e:\n            logger.error(f\"GPU telemetry error: {e}\")\n            # Continue loop despite errors\n            await asyncio.sleep(GPU_BROADCAST_INTERVAL)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Start the GPU telemetry background task", "output": "```python\ndef start_gpu_broadcast():\n    \"\"\"Start the GPU telemetry background task\"\"\"\n    global _gpu_broadcast_task\n    if _gpu_broadcast_task is None or _gpu_broadcast_task.done():\n        _gpu_broadcast_task = asyncio.create_task(gpu_telemetry_loop())\n        logger.info(\"GPU telemetry broadcast task created\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Stop the GPU telemetry background task", "output": "```python\ndef stop_gpu_broadcast():\n    \"\"\"Stop the GPU telemetry background task\"\"\"\n    global _gpu_broadcast_task\n    if _gpu_broadcast_task and not _gpu_broadcast_task.done():\n        _gpu_broadcast_task.cancel()\n        logger.info(\"GPU telemetry broadcast task cancelled\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Start the status pulse background task", "output": "```python\ndef start_status_pulse():\n    \"\"\"Start the status pulse background task\"\"\"\n    global _status_pulse_task\n    if _status_pulse_task is None or _status_pulse_task.done():\n        _status_pulse_task = asyncio.create_task(status_pulse_loop())\n        logger.info(\"Status pulse task created\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Stop the status pulse background task", "output": "```python\ndef stop_status_pulse():\n    \"\"\"Stop the status pulse background task\"\"\"\n    global _status_pulse_task\n    if _status_pulse_task and not _status_pulse_task.done():\n        _status_pulse_task.cancel()\n        logger.info(\"Status pulse task cancelled\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send error message to client", "output": "```python\nasync def send_error(\n    websocket: WebSocket,\n    request_id: str,\n    code: str,\n    message: str,\n    retry_after_ms: Optional[int] = None\n) -> None:\n    \"\"\"Send error message to client\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.ERROR.value,\n        \"request_id\": request_id,\n        \"code\": code,\n        \"message\": message,\n        \"retry_after_ms\": retry_after_ms,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send model warmup status", "output": "```python\nasync def send_warmup(\n    websocket: WebSocket,\n    request_id: str,\n    model: str,\n    status: str,\n    progress: Optional[int] = None\n) -> None:\n    \"\"\"Send model warmup status\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.WARMUP.value,\n        \"request_id\": request_id,\n        \"model\": model,\n        \"status\": status,\n        \"progress_percent\": progress,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle incoming chat request with streaming response", "output": "```python\nasync def handle_chat_message(\n    websocket: WebSocket,\n    data: dict\n) -> None:\n    \"\"\"\n    Handle incoming chat request with streaming response.\n    Uses 4-Agent Orchestrator for profile-based routing.\n\n    Frontend sends profile: qwen/gemma/cloud/nerdy\n    Backend routes to correct model with appropriate system prompt.\n    Every packet includes agent_id for frontend panel highlighting.\n    \"\"\"\n    # Parse request\n    try:\n        request = ChatRequest(**data)\n    except Exception as e:\n        logger.error(f\"Invalid chat request: {e}\")\n        await send_error(\n            websocket,\n            data.get(\"request_id\", \"unknown\"),\n            \"INVALID_REQUEST\",\n            f\"Invalid request format: {str(e)}\"\n        )\n        return\n\n    # Get agent config for display info\n    agent = get_agent(request.profile)\n\n    req_logger = RequestLogger(logger, request.request_id, profile=request.profile)\n    req_logger.info(f\"Chat request - profile: {request.profile} ‚Üí agent: {agent.display_name}\")\n\n    # Build messages list\n    messages = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n\n    # Start timing for latency tracking\n    request_start_time = time.time()\n    first_token_sent = False\n    latency_ms = None\n\n    # Send start message with agent_id for highlighting\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.START.value,\n        \"request_id\": request.request_id,\n        \"profile\": request.profile,\n        \"agent_id\": agent.id.value,  # For panel highlighting\n        \"model\": agent.model,\n        \"sender\": \"agent\",\n    })\n\n    try:\n        # Stream response via 4-agent orchestrator\n        final_stats = None\n        token_count = 0\n        active_agent_id = agent.id.value\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request.request_id,\n            profile=request.profile,  # Route by profile, not mode\n            temperature=None,  # Use agent's default\n            max_tokens=4096\n        ):\n            active_agent_id = agent_id\n\n            if token:\n                token_count += 1\n\n                # Calculate latency on first token\n                if not first_token_sent:\n                    latency_ms = int((time.time() - request_start_time) * 1000)\n                    first_token_sent = True\n                    req_logger.info(f\"TTFT: {latency_ms}ms\")\n\n                # Send token with agent_id for highlighting\n                token_packet = {\n                    \"type\": WSMessageType.TOKEN.value,\n                    \"content\": token,\n                    \"request_id\": request.request_id,\n                    \"agent_id\": active_agent_id,  # For panel highlighting\n                    \"sender\": \"agent\",\n                    \"text\": token,  # Antigravity format compatibility\n                    \"is_finished\": False,\n                }\n\n                # Include latency only on first token\n                if latency_ms is not None:\n                    token_packet[\"latency_ms\"] = latency_ms\n                    latency_ms = None  # Only send once\n\n                await manager.send_json(websocket, token_packet)\n\n            if stats:\n                final_stats = stats\n\n        # Send completion with full stats and agent_id\n        done_packet = {\n            \"type\": WSMessageType.DONE.value,\n            \"request_id\": request.request_id,\n            \"agent_id\": final_stats.agent_id if final_stats else active_agent_id,\n            \"sender\": \"agent\",\n            \"text\": \"\",\n            \"is_finished\": True,\n            \"model\": final_stats.model if final_stats else agent.model,\n            \"provider\": final_stats.provider if final_stats else agent.provider.value,\n        }\n\n        if final_stats:\n            done_packet[\"stats\"] = {\n                \"tokens_generated\": final_stats.tokens_generated,\n                \"tokens_per_second\": round(final_stats.tokens_per_second, 1),\n                \"total_duration_ms\": final_stats.total_duration_ms,\n                \"prompt_tokens\": final_stats.prompt_tokens,\n                \"model\": final_stats.model,\n                \"provider\": final_stats.provider,\n                \"agent_id\": final_stats.agent_id,\n                \"profile\": request.profile,\n            }\n\n        await manager.send_json(websocket, done_packet)\n\n        req_logger.info(\n            f\"Completed via {active_agent_id}: {token_count} tokens \"\n            f\"in {final_stats.total_duration_ms if final_stats else 0}ms\"\n        )\n\n    except OllamaConnectionError as e:\n        req_logger.error(f\"Connection error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"PROVIDER_DISCONNECTED\",\n            str(e),\n            retry_after_ms=5000\n        )\n\n    except OllamaModelNotFoundError as e:\n        req_logger.error(f\"Model not found: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"MODEL_NOT_FOUND\",\n            str(e)\n        )\n\n    except XAIError as e:\n        req_logger.error(f\"xAI error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"XAI_ERROR\",\n            str(e)\n        )\n\n    except OllamaTimeoutError as e:\n        req_logger.error(f\"Timeout: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"TIMEOUT\",\n            str(e),\n            retry_after_ms=10000\n        )\n\n    except Exception as e:\n        req_logger.error(f\"Unexpected error: {e}\")\n        await send_error(\n            websocket,\n            request.request_id,\n            \"INTERNAL_ERROR\",\n            f\"An unexpected error occurred: {str(e)}\"\n        )\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle ping message", "output": "```python\nasync def handle_ping(websocket: WebSocket, data: dict) -> None:\n    \"\"\"Handle ping message\"\"\"\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.PONG.value,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle profile switch request", "output": "```python\nasync def handle_switch_profile(websocket: WebSocket, data: dict) -> None:\n    \"\"\"Handle profile switch request\"\"\"\n    profile_id = data.get(\"profile\", \"chill\")\n    profile = get_profile(profile_id)\n\n    # Optionally warmup the model\n    if data.get(\"warmup\", False):\n        await send_warmup(websocket, \"switch\", profile.model, \"loading\")\n        success = await ollama_client.warmup_model(profile)\n        status = \"ready\" if success else \"failed\"\n        await send_warmup(websocket, \"switch\", profile.model, status)\n\n    await manager.send_json(websocket, {\n        \"type\": WSMessageType.PROFILE_CHANGED.value,\n        \"profile\": profile.id.value,\n        \"display_name\": profile.display_name,\n        \"model\": profile.model,\n    })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Handle Antigravity/Robot Unicorn frontend protocol (legacy simple format)", "output": "```python\nasync def handle_antigravity_message(websocket: WebSocket, data: dict) -> None:\n    \"\"\"\n    Handle Antigravity/Robot Unicorn frontend protocol (legacy simple format).\n\n    Incoming format:\n        {\"text\": \"user message\", \"profile\": \"qwen\"|\"gemma\"|\"cloud\"|\"nerdy\"}\n\n    Outgoing format (streamed):\n        {\"sender\": \"agent\", \"text\": \"token...\", \"is_finished\": false, \"agent_id\": \"qwen\"}\n        {\"sender\": \"agent\", \"text\": \"\", \"is_finished\": true, \"stats\": {...}}\n\n    OPERATION VITAL SIGNS features:\n        - agent_id: Identifies which agent panel should highlight\n        - latency_ms: Time to first token (TTFT), sent with first token\n    \"\"\"\n    text = data.get(\"text\", \"\")\n    profile = data.get(\"profile\", \"qwen\")\n\n    if not text:\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": \"Error: No text provided\",\n            \"is_finished\": True,\n            \"error\": True\n        })\n        return\n\n    request_id = str(uuid.uuid4())\n    agent = get_agent(profile)\n    logger.info(f\"[{request_id}] Antigravity request - profile: {profile} ‚Üí {agent.display_name}\")\n\n    # Emit system log for terminal\n    await emit_system_log(\n        \"request_start\",\n        f\"[{agent.display_name}] Processing request...\",\n        \"info\",\n        agent.id.value\n    )\n\n    # Build messages list (simple format - just the user message)\n    messages = [{\"role\": \"user\", \"content\": text}]\n\n    # Start timing for latency tracking\n    request_start_time = time.time()\n    first_token_sent = False\n\n    # WAR ROOM: Broadcast request start to all dashboards\n    await emit_war_room_event(\"start\", {\n        \"agent\": profile,\n        \"user_prompt\": text[:100],\n        \"request_id\": request_id,\n    })\n\n    try:\n        # Stream response via 4-agent orchestrator\n        full_response = []\n        final_stats = None\n        active_agent_id = agent.id.value\n        latency_ms = None\n\n        # WAR ROOM: 100ms token batching for smooth dashboard display\n        war_room_buffer = []\n        last_war_room_flush = time.time()\n        WAR_ROOM_FLUSH_INTERVAL = 0.1  # 100ms\n        WAR_ROOM_BUFFER_MAX = 50  # chars\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request_id,\n            profile=profile\n        ):\n            active_agent_id = agent_id\n\n            if token:\n                full_response.append(token)\n\n                # Calculate latency on first token\n                if not first_token_sent:\n                    latency_ms = int((time.time() - request_start_time) * 1000)\n                    first_token_sent = True\n                    logger.info(f\"[{request_id}] TTFT: {latency_ms}ms via {agent_id}\")\n\n                    # Emit system log for TTFT\n                    await emit_system_log(\n                        \"generation_start\",\n                        f\"[{agent.display_name}] Streaming response (TTFT: {latency_ms}ms)\",\n                        \"success\",\n                        agent_id\n                    )\n\n                # Stream each token to frontend with agent_id and latency\n                token_packet = {\n                    \"sender\": \"agent\",\n                    \"text\": token,\n                    \"is_finished\": False,\n                    \"model\": agent.model,\n                    \"provider\": agent.provider.value,\n                    \"agent_id\": active_agent_id,  # For dynamic highlighting\n                }\n\n                # Include latency only on first token\n                if latency_ms is not None:\n                    token_packet[\"latency_ms\"] = latency_ms\n                    latency_ms = None  # Only send once\n\n                await manager.send_json(websocket, token_packet)\n\n                # WAR ROOM: Add token to buffer\n                war_room_buffer.append(token)\n                buffer_text = \"\".join(war_room_buffer)\n                now = time.time()\n\n                # Flush if 100ms elapsed OR buffer > 50 chars\n                if (now - last_war_room_flush >= WAR_ROOM_FLUSH_INTERVAL) or len(buffer_text) > WAR_ROOM_BUFFER_MAX:\n                    await emit_war_room_event(\"stream\", {\n                        \"tokens\": buffer_text,\n                        \"request_id\": request_id,\n                    })\n                    war_room_buffer = []\n                    last_war_room_flush = now\n\n            if stats:\n                final_stats = stats\n\n        # WAR ROOM: Flush any remaining tokens in buffer\n        if war_room_buffer:\n            await emit_war_room_event(\"stream\", {\n                \"tokens\": \"\".join(war_room_buffer),\n                \"request_id\": request_id,\n            })\n\n        # WAR ROOM: Broadcast completion with stats\n        duration_ms = int((time.time() - request_start_time) * 1000)\n        await emit_war_room_event(\"end\", {\n            \"request_id\": request_id,\n            \"stats\": {\n                \"tokens\": len(full_response),\n                \"duration_ms\": duration_ms,\n            }\n        })\n\n        # =================================================================\n        # TOOL CALL DETECTION - Check if QWEN wants to run a tool\n        # =================================================================\n        full_text = \"\".join(full_response)\n        tool_call = extract_tool_call(full_text)\n\n        if tool_call and profile == \"qwen\":\n            tool_name = tool_call.get(\"tool\")\n            params = {k: v for k, v in tool_call.items() if k != \"tool\"}\n\n            tool_def = TOOLS.get(tool_name)\n\n            if tool_def:\n                logger.info(f\"[{request_id}] Tool call detected: {tool_name} ({tool_def.safety.value})\")\n\n                await emit_system_log(\n                    \"tool_call\",\n                    f\"[QWEN] Verktygsanrop: {tool_name}\",\n                    \"info\",\n                    \"qwen\"\n                )\n\n                if tool_def.safety == SafetyLevel.SAFE:\n                    # Execute SAFE tool directly\n                    result = await system_tools.execute(tool_name, params)\n\n                    # Send tool result to frontend\n                    await manager.send_json(websocket, {\n                        \"sender\": \"system\",\n                        \"tool_result\": {\n                            \"tool\": tool_name,\n                            \"success\": result.success,\n                            \"output\": result.output[:2000] if result.output else \"\",\n                            \"error\": result.error,\n                        },\n                        \"is_finished\": False\n                    })\n\n                    await emit_system_log(\n                        \"tool_executed\",\n                        f\"[SYSTEM] {tool_name}: {'OK' if result.success else 'FEL'}\",\n                        \"success\" if result.success else \"error\",\n                        \"qwen\"\n                    )\n\n                    # Let QWEN interpret the result\n                    follow_up_messages = [\n                        {\"role\": \"user\", \"content\": text},\n                        {\"role\": \"assistant\", \"content\": full_text},\n                        {\"role\": \"user\", \"content\": f\"Verktygsresultat f√∂r {tool_name}:\\n```\\n{result.output[:2000] if result.output else result.error}\\n```\\nTolka resultatet och ge en sammanfattning.\"}\n                    ]\n\n                    # Stream QWEN's interpretation\n                    async for token, stats, agent_id in orchestrator.chat_stream(\n                        messages=follow_up_messages,\n                        request_id=request_id + \"-followup\",\n                        profile=\"qwen\"\n                    ):\n                        if token:\n                            await manager.send_json(websocket, {\n                                \"sender\": \"agent\",\n                                \"text\": token,\n                                \"is_finished\": False,\n                                \"agent_id\": \"qwen\",\n                            })\n                        if stats:\n                            final_stats = stats\n\n                else:\n                    # DANGEROUS tool - require confirmation\n                    _pending_tool_confirmations[request_id] = {\n                        \"tool\": tool_name,\n                        \"params\": params,\n                        \"original_question\": text,\n                        \"original_response\": full_text,\n                        \"websocket\": websocket,\n                    }\n\n                    await manager.send_json(websocket, {\n                        \"sender\": \"system\",\n                        \"confirmation_required\": {\n                            \"request_id\": request_id,\n                            \"tool\": tool_name,\n                            \"params\": params,\n                            \"description\": tool_def.description,\n                            \"message\": f\"‚ö†Ô∏è QWEN vill k√∂ra: {tool_name}({params}). Godk√§nn?\"\n                        },\n                        \"is_finished\": False\n                    })\n\n                    await emit_system_log(\n                        \"tool_confirmation\",\n                        f\"[V√ÑNTANDE] {tool_name} kr√§ver bekr√§ftelse\",\n                        \"warn\",\n                        \"qwen\"\n                    )\n\n                    # Don't send final done message yet - wait for confirmation\n                    return\n\n        # Send completion message with full stats\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": \"\",\n            \"is_finished\": True,\n            \"model\": final_stats.model if final_stats else agent.model,\n            \"provider\": final_stats.provider if final_stats else agent.provider.value,\n            \"agent_id\": final_stats.agent_id if final_stats else active_agent_id,\n            \"stats\": {\n                \"tokens\": final_stats.tokens_generated if final_stats else 0,\n                \"speed\": round(final_stats.tokens_per_second, 1) if final_stats else 0,\n                \"duration_ms\": final_stats.total_duration_ms if final_stats else 0,\n                \"provider\": final_stats.provider if final_stats else agent.provider.value,\n                \"model\": final_stats.model if final_stats else agent.model,\n                \"agent_id\": final_stats.agent_id if final_stats else active_agent_id\n            }\n        })\n\n        logger.info(f\"[{request_id}] Antigravity complete via {active_agent_id}\")\n\n        # Emit completion log\n        tokens = final_stats.tokens_generated if final_stats else 0\n        speed = round(final_stats.tokens_per_second, 1) if final_stats else 0\n        await emit_system_log(\n            \"generation_complete\",\n            f\"[{agent.display_name}] Complete: {tokens} tokens @ {speed} tok/s\",\n            \"success\",\n            active_agent_id\n        )\n\n    except Exception as e:\n        logger.error(f\"[{request_id}] Antigravity error: {e}\")\n\n        # Emit error log\n        await emit_system_log(\n            \"error\",\n            f\"[ERROR] {str(e)[:50]}\",\n            \"error\",\n            agent.id.value if agent else None\n        )\n        await manager.send_json(websocket, {\n            \"sender\": \"agent\",\n            \"text\": f\"Error: {str(e)}\",\n            \"is_finished\": True,\n            \"error\": True\n        })\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Main WebSocket endpoint for chat communication", "output": "```python\nasync def websocket_endpoint(websocket: WebSocket) -> None:\n    \"\"\"\n    Main WebSocket endpoint for chat communication.\n\n    Supports two protocols:\n    1. Legacy: {\"type\": \"chat\", \"messages\": [...], \"mode\": \"auto\"}\n    2. Antigravity: {\"text\": \"message\", \"mode\": \"auto\"}\n\n    Server responds with streaming tokens and status updates.\n\n    OPERATION VITAL SIGNS:\n    - Starts GPU telemetry broadcast when first client connects\n    - Broadcasts status_update packets every 2 seconds with GPU stats\n    \"\"\"\n    await manager.connect(websocket)\n\n    # Start GPU telemetry broadcast if not already running\n    start_gpu_broadcast()\n\n    # Start status pulse for Kiosk dashboard\n    start_status_pulse()\n\n    try:\n        while True:\n            # Receive message\n            try:\n                raw_data = await websocket.receive_text()\n                data = json.loads(raw_data)\n            except json.JSONDecodeError:\n                logger.warning(\"Received invalid JSON\")\n                continue\n\n            # Detect protocol format\n            if \"text\" in data and \"type\" not in data:\n                # Antigravity protocol: {\"text\": \"...\", \"mode\": \"...\"}\n                await handle_antigravity_message(websocket, data)\n            else:\n                # Legacy protocol with \"type\" field\n                message_type = data.get(\"type\", \"chat\")\n\n                if message_type == \"chat\":\n                    await handle_chat_message(websocket, data)\n                elif message_type == \"ping\":\n                    await handle_ping(websocket, data)\n                elif message_type == \"switch_profile\":\n                    await handle_switch_profile(websocket, data)\n                else:\n                    logger.warning(f\"Unknown message type: {message_type}\")\n\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        manager.disconnect(websocket)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: WebSocket endpoint f√∂r Multi-Agent Cascade (Planner ‚Üí Coder ‚Üí Reviewer)", "output": "```python\nasync def cascade_websocket_endpoint(websocket: WebSocket) -> None:\n    \"\"\"\n    WebSocket endpoint f√∂r Multi-Agent Cascade (Planner ‚Üí Coder ‚Üí Reviewer).\n\n    Protocol:\n        Send: {\"task\": \"Skriv en REST API...\", \"skip_review\": false}\n        Receive: {\"phase\": \"planning|coding|reviewing|complete\", \"agent_id\": \"...\", \"token\": \"...\"}\n    \"\"\"\n    from ..services.cascade_orchestrator import cascade_orchestrator, CascadePhase\n\n    await manager.connect(websocket)\n    logger.info(\"Cascade WebSocket connected\")\n\n    try:\n        while True:\n            raw_data = await websocket.receive_text()\n            data = json.loads(raw_data)\n\n            task = data.get(\"task\", \"\")\n            skip_review = data.get(\"skip_review\", False)\n\n            if not task:\n                await manager.send_json(websocket, {\n                    \"phase\": \"error\",\n                    \"error\": \"No task provided\"\n                })\n                continue\n\n            request_id = str(uuid.uuid4())\n            logger.info(f\"[{request_id}] Cascade started: {task[:100]}...\")\n\n            # Stream cascade phases\n            async for result in cascade_orchestrator.execute_cascade(\n                task=task,\n                request_id=request_id,\n                skip_review=skip_review\n            ):\n                packet = {\n                    \"phase\": result.phase.value,\n                    \"agent_id\": result.agent_id,\n                    \"token\": result.content,\n                }\n\n                if result.stats:\n                    packet[\"stats\"] = {\n                        \"tokens\": result.stats.tokens_generated,\n                        \"tokens_per_sec\": result.stats.tokens_per_second,\n                        \"duration_ms\": result.stats.total_duration_ms,\n                    }\n\n                await manager.send_json(websocket, packet)\n\n            logger.info(f\"[{request_id}] Cascade complete\")\n\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n    except Exception as e:\n        logger.error(f\"Cascade WebSocket error: {e}\")\n        manager.disconnect(websocket)\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en Python-funktion som to dict", "output": "```python\n    def to_dict(self) -> dict:\n        return {\n            \"request_id\": self.request_id,\n            \"question\": self.question[:100] + \"...\" if len(self.question) > 100 else self.question,\n            \"answer\": self.answer[:200] + \"...\" if len(self.answer) > 200 else self.answer,\n            \"profile\": self.profile,\n            \"timestamp\": self.timestamp,\n            \"stats\": self.stats,\n        }\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en async Python-funktion som connect", "output": "```python\n    async def connect(self, websocket: WebSocket) -> None:\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        logger.info(f\"WebSocket connected. Active: {len(self.active_connections)}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Skapa en Python-funktion som disconnect", "output": "```python\n    def disconnect(self, websocket: WebSocket) -> None:\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n        logger.info(f\"WebSocket disconnected. Active: {len(self.active_connections)}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Send JSON message to a specific connection", "output": "```python\n    async def send_json(self, websocket: WebSocket, data: dict) -> None:\n        \"\"\"Send JSON message to a specific connection\"\"\"\n        try:\n            await websocket.send_json(data)\n        except Exception as e:\n            logger.error(f\"Failed to send message: {e}\")\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Broadcast message to all connections", "output": "```python\n    async def broadcast(self, data: dict) -> None:\n        \"\"\"Broadcast message to all connections\"\"\"\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(data)\n            except Exception:\n                pass\n```\n\nDenna funktion finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-klass: Represents a mobile Q&A exchange for the Active Context buffer", "output": "```python\nclass MobileExchange:\n    \"\"\"Represents a mobile Q&A exchange for the Active Context buffer\"\"\"\n    request_id: str\n    question: str\n    answer: str\n    profile: str\n    timestamp: str\n    stats: dict = field(default_factory=dict)\n\n    def to_dict(self) -> dict:\n        return {\n            \"request_id\": self.request_id,\n            \"question\": self.question[:100] + \"...\" if len(self.question) > 100 else self.question,\n            \"answer\": self.answer[:200] + \"...\" if len(self.answer) > 200 else self.answer,\n            \"profile\": self.profile,\n            \"timestamp\": self.timestamp,\n            \"stats\": self.stats,\n        }\n```\n\nKlassen finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-klass: Manages WebSocket connections", "output": "```python\nclass ConnectionManager:\n    \"\"\"Manages WebSocket connections\"\"\"\n\n    def __init__(self):\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket) -> None:\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        logger.info(f\"WebSocket connected. Active: {len(self.active_connections)}\")\n\n    def disconnect(self, websocket: WebSocket) -> None:\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n        logger.info(f\"WebSocket disconnected. Active: {len(self.active_connections)}\")\n\n    async def send_json(self, websocket: WebSocket, data: dict) -> None:\n        \"\"\"Send JSON message to a specific connection\"\"\"\n        try:\n            await websocket.send_json(data)\n        except Exception as e:\n            logger.error(f\"Failed to send message: {e}\")\n\n    async def broadcast(self, data: dict) -> None:\n        \"\"\"Broadcast message to all connections\"\"\"\n        for connection in self.active_connections:\n            try:\n                await connection.send_json(data)\n            except Exception:\n                pass\n```\n\nKlassen finns i `app/api/Backend_Chat_Stream.py`."}
{"instruction": "Implementera en Python-funktion: Health check endpoint", "output": "```python\nasync def health_check(\n    ollama: OllamaClient = Depends(get_ollama_client),\n    gpu: GPUMonitor = Depends(get_gpu_monitor),\n) -> HealthResponse:\n    \"\"\"\n    Health check endpoint.\n    Returns system status and component health.\n    \"\"\"\n    # Check Ollama\n    ollama_connected = await ollama.is_connected()\n    ollama_version = await ollama.get_version() if ollama_connected else None\n    models_available = await ollama.list_models() if ollama_connected else []\n    running_models = await ollama.list_running_models() if ollama_connected else []\n\n    # Check GPU\n    gpu_available = await gpu.is_gpu_available()\n\n    # Check required models (QWEN for local inference)\n    qwen_model = PROFILES[ProfileId.QWEN].model\n    qwen_available = qwen_model in models_available or any(qwen_model in m for m in models_available)\n\n    # Get orchestrator status for xAI\n    orch_status = await orchestrator.get_status()\n    xai_ok = orch_status.get(\"providers\", {}).get(\"xai\", {}).get(\"connected\", False)\n\n    # Determine overall status\n    checks = {\n        \"ollama\": ollama_connected,\n        \"gpu\": gpu_available,\n        \"qwen_model\": qwen_available,\n        \"xai\": xai_ok,\n    }\n\n    # Healthy if at least one provider works\n    if ollama_connected and qwen_available:\n        status = \"healthy\"\n    elif xai_ok:\n        status = \"degraded\"  # Cloud-only mode\n    else:\n        status = \"unhealthy\"\n\n    return HealthResponse(\n        status=status,\n        version=settings.app_version,\n        timestamp=datetime.utcnow(),\n        ollama=OllamaStatus(\n            connected=ollama_connected,\n            version=ollama_version,\n            models_available=models_available,\n            models_loaded=[m.get(\"name\", \"\") for m in running_models],\n        ),\n        gpu_available=gpu_available,\n        checks=checks,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: List available AI profiles (THINK/CHILL)", "output": "```python\nasync def get_profiles() -> ProfilesResponse:\n    \"\"\"\n    List available AI profiles (THINK/CHILL).\n    Returns profile configurations and current status.\n    \"\"\"\n    profiles = []\n    active_profile = profile_manager.active_profile\n\n    for profile in get_all_profiles():\n        status = profile_manager.get_status(profile.id)\n        profiles.append(ProfileInfo(\n            id=profile.id.value,\n            name=profile.name,\n            display_name=profile.display_name,\n            description=profile.description,\n            model=profile.model,\n            estimated_vram_gb=profile.estimated_vram_gb,\n            icon=profile.icon,\n            color=profile.color,\n            strengths=profile.strengths or [],\n            is_active=status.is_loaded,\n            is_loading=status.is_loading,\n        ))\n\n    return ProfilesResponse(\n        profiles=profiles,\n        active_profile=active_profile.id.value if active_profile else None,\n        default_profile=settings.default_profile,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get a specific profile by ID", "output": "```python\nasync def get_profile_by_id(profile_id: str) -> ProfileInfo:\n    \"\"\"\n    Get a specific profile by ID.\n    \"\"\"\n    try:\n        pid = ProfileId(profile_id.lower())\n        profile = PROFILES[pid]\n    except (ValueError, KeyError):\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Profile '{profile_id}' not found. Available: qwen, gemma\"\n        )\n\n    status = profile_manager.get_status(profile.id)\n\n    return ProfileInfo(\n        id=profile.id.value,\n        name=profile.name,\n        display_name=profile.display_name,\n        description=profile.description,\n        model=profile.model,\n        estimated_vram_gb=profile.estimated_vram_gb,\n        icon=profile.icon,\n        color=profile.color,\n        strengths=profile.strengths or [],\n        is_active=status.is_loaded,\n        is_loading=status.is_loading,\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Pre-load a model into GPU memory", "output": "```python\nasync def warmup_profile(\n    profile_id: str,\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> dict:\n    \"\"\"\n    Pre-load a model into GPU memory.\n    This reduces first-response latency.\n    \"\"\"\n    try:\n        pid = ProfileId(profile_id.lower())\n        profile = PROFILES[pid]\n    except (ValueError, KeyError):\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Profile '{profile_id}' not found\"\n        )\n\n    logger.info(f\"Warming up profile: {profile_id}\")\n    success = await ollama.warmup_model(profile)\n\n    if not success:\n        raise HTTPException(\n            status_code=503,\n            detail=f\"Failed to warm up model {profile.model}. Is Ollama running?\"\n        )\n\n    return {\n        \"status\": \"ready\",\n        \"profile\": profile_id,\n        \"model\": profile.model,\n        \"message\": f\"Model {profile.display_name} is now loaded and ready\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get current GPU statistics", "output": "```python\nasync def get_gpu_stats(\n    gpu: GPUMonitor = Depends(get_gpu_monitor),\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> GPUStatsResponse:\n    \"\"\"\n    Get current GPU statistics.\n    Used by frontend GPU monitor component.\n    \"\"\"\n    stats = await gpu.get_stats()\n    running = await ollama.list_running_models()\n\n    return GPUStatsResponse(\n        gpu=stats,\n        timestamp=datetime.utcnow(),\n        ollama_models_loaded=[m.get(\"name\", \"\") for m in running],\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: List all available Ollama models", "output": "```python\nasync def list_models(\n    ollama: OllamaClient = Depends(get_ollama_client),\n) -> dict:\n    \"\"\"\n    List all available Ollama models.\n    \"\"\"\n    if not await ollama.is_connected():\n        raise HTTPException(\n            status_code=503,\n            detail=\"Ollama not available. Start with: ollama serve\"\n        )\n\n    models = await ollama.list_models()\n    running = await ollama.list_running_models()\n\n    return {\n        \"available\": models,\n        \"loaded\": [m.get(\"name\", \"\") for m in running],\n        \"required\": {\n            \"qwen\": PROFILES[ProfileId.QWEN].model,\n            \"gemma\": PROFILES[ProfileId.GEMMA].model,\n        }\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get status of Hybrid Orchestrator providers", "output": "```python\nasync def get_orchestrator_status() -> dict:\n    \"\"\"\n    Get status of Hybrid Orchestrator providers.\n\n    Returns status of:\n    - xAI Grok (cloud) - for FAST mode\n    - Ollama (local) - for DEEP mode\n    \"\"\"\n    status = await orchestrator.get_status()\n    return {\n        \"providers\": status,\n        \"modes\": {\n            \"auto\": \"Intelligent routing based on query\",\n            \"fast\": \"Grok API (instant, cloud)\",\n            \"deep\": \"Qwen 14B (max quality, local GPU)\",\n        },\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get project structure context for frontend display", "output": "```python\nasync def get_project_context(refresh: bool = False) -> dict:\n    \"\"\"\n    Get project structure context for frontend display.\n\n    Args:\n        refresh: Force refresh of cached context\n    \"\"\"\n    context = intelligence.get_project_context(force_refresh=refresh)\n    return {\n        \"root_path\": context.root_path,\n        \"file_tree\": context.file_tree,\n        \"file_count\": context.file_count,\n        \"directory_count\": context.directory_count,\n        \"languages\": context.languages_detected,\n        \"key_files\": context.key_files,\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Validate code syntax (currently Python only)", "output": "```python\nasync def validate_code(body: dict) -> dict:\n    \"\"\"\n    Validate code syntax (currently Python only).\n\n    Body:\n        code: str - Code to validate\n        language: str - Language (default: python)\n    \"\"\"\n    code = body.get(\"code\", \"\")\n    language = body.get(\"language\", \"python\").lower()\n\n    if language != \"python\":\n        return {\n            \"valid\": None,\n            \"supported\": False,\n            \"message\": f\"Validation not supported for {language}. Only Python is supported.\"\n        }\n\n    from ..services.intelligence import validate_python_syntax, build_correction_prompt\n\n    result = validate_python_syntax(code)\n\n    response = {\n        \"valid\": result.is_valid,\n        \"supported\": True,\n        \"language\": result.language\n    }\n\n    if not result.is_valid:\n        response[\"error\"] = {\n            \"message\": result.error_message,\n            \"line\": result.error_line\n        }\n        response[\"correction_prompt\"] = build_correction_prompt(result)\n\n    return response\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Extract and validate JSON from text", "output": "```python\nasync def extract_json(body: dict) -> dict:\n    \"\"\"\n    Extract and validate JSON from text.\n\n    Body:\n        text: str - Text containing JSON\n        required_fields: list[str] - Optional required field names\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    required_fields = body.get(\"required_fields\", [])\n\n    from ..services.intelligence import extract_json_from_response, validate_json_against_schema\n\n    success, data, raw = extract_json_from_response(text)\n\n    response = {\n        \"found\": success,\n        \"data\": data if success else None,\n        \"raw\": raw if success else None\n    }\n\n    if success and required_fields:\n        valid, missing = validate_json_against_schema(data, required_fields)\n        response[\"fields_valid\"] = valid\n        if not valid:\n            response[\"missing_fields\"] = missing\n\n    if not success:\n        response[\"error\"] = data  # Contains error message\n\n    return response\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Warm up the system by pre-loading the default model", "output": "```python\nasync def warmup_system() -> dict:\n    \"\"\"\n    Warm up the system by pre-loading the default model.\n    This reduces first-response latency.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating system warmup\")\n        await emit_system_log(\"info\", \"System warmup initiated\")\n\n        result = await warmup_model()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", f\"System warmup completed: {result.get('message', 'Model loaded')}\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Model warmed up successfully\"),\n                \"model\": result.get(\"model\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"System warmup failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to warm up model\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"System warmup error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart the backend service", "output": "```python\nasync def restart_backend_service() -> dict:\n    \"\"\"\n    Restart the backend service.\n    This will reload all configurations and services.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating backend restart\")\n        await emit_system_log(\"warning\", \"Backend restart initiated\")\n\n        result = await restart_backend()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", \"Backend restart completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Backend restart initiated\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Backend restart failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to restart backend\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Backend restart error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart the frontend service", "output": "```python\nasync def restart_frontend_service() -> dict:\n    \"\"\"\n    Restart the frontend service.\n    This will reload the Next.js development server.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating frontend restart\")\n        await emit_system_log(\"warning\", \"Frontend restart initiated\")\n\n        result = await restart_frontend()\n\n        if result.get(\"success\"):\n            await emit_system_log(\"success\", \"Frontend restart completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"Frontend restart initiated\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Frontend restart failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to restart frontend\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Frontend restart error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Unload all models from GPU memory", "output": "```python\nasync def unload_models() -> dict:\n    \"\"\"\n    Unload all models from GPU memory.\n    This frees up VRAM and allows for model switching.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Initiating model unload\")\n        await emit_system_log(\"info\", \"Unloading all models from GPU memory\")\n\n        result = await unload_all_models()\n\n        if result.get(\"success\"):\n            models_unloaded = result.get(\"models_unloaded\", 0)\n            await emit_system_log(\"success\", f\"Successfully unloaded {models_unloaded} model(s)\")\n            return {\n                \"success\": True,\n                \"message\": result.get(\"message\", \"All models unloaded\"),\n                \"models_unloaded\": models_unloaded,\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Model unload failed: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to unload models\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Model unload error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get today's system statistics", "output": "```python\nasync def get_stats_today() -> dict:\n    \"\"\"\n    Get today's system statistics.\n    Returns usage metrics, request counts, and performance data.\n    \"\"\"\n    try:\n        logger.info(\"Kiosk Dashboard: Fetching today's statistics\")\n\n        result = await get_today_stats()\n\n        if result.get(\"success\"):\n            stats = result.get(\"stats\", {})\n            return {\n                \"success\": True,\n                \"stats\": stats,\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        else:\n            await emit_system_log(\"error\", f\"Failed to fetch stats: {result.get('error', 'Unknown error')}\")\n            return {\n                \"success\": False,\n                \"error\": result.get(\"error\", \"Failed to retrieve statistics\"),\n                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n            }\n\n    except Exception as e:\n        error_msg = f\"Stats retrieval error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"error\", error_msg)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Voice command endpoint for mobile app", "output": "```python\nasync def voice_command(body: dict) -> dict:\n    \"\"\"\n    Voice command endpoint for mobile app.\n    Receives transcribed text and returns AI response.\n\n    WAR ROOM SYNC: Broadcasts mobile_activity events to all connected dashboards.\n\n    Request body:\n        text: str - User message\n        profile: str - Agent profile (\"qwen\", \"nerdy\", \"gemma\", default: \"qwen\")\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    profile = body.get(\"profile\", \"qwen\").lower()\n\n    if not text:\n        return {\"success\": False, \"error\": \"No text provided\"}\n\n    # Validate profile\n    valid_profiles = [\"qwen\", \"nerdy\", \"gemma\"]\n    if profile not in valid_profiles:\n        profile = \"qwen\"\n\n    try:\n        # Skapa meddelande\n        messages = [{\"role\": \"user\", \"content\": text}]\n        request_id = str(uuid.uuid4())[:8]\n\n        # WAR ROOM: Emit request start to dashboards\n        await emit_mobile_activity(\"mobile_request_start\", request_id, {\n            \"profile\": profile,\n            \"question_preview\": text[:50] + \"...\" if len(text) > 50 else text,\n        })\n\n        # Samla hela svaret fr√•n stream\n        full_response = \"\"\n        token_count = 0\n        last_emit_time = datetime.utcnow()\n        final_stats = None\n\n        async for token, stats, agent_id in orchestrator.chat_stream(\n            messages=messages,\n            request_id=request_id,\n            profile=profile,\n        ):\n            if token:\n                full_response += token\n                token_count += 1\n\n            if stats:\n                final_stats = stats\n\n            # WAR ROOM: Emit generation progress every 500ms\n            now = datetime.utcnow()\n            if (now - last_emit_time).total_seconds() >= 0.5:\n                tps = final_stats.tokens_per_second if final_stats else 0\n                await emit_mobile_activity(\"mobile_generation\", request_id, {\n                    \"tokens_generated\": token_count,\n                    \"tokens_per_second\": round(tps, 1),\n                })\n                last_emit_time = now\n\n        # Build stats dict for response\n        stats_dict = {}\n        if final_stats:\n            stats_dict = {\n                \"tokens_generated\": final_stats.tokens_generated,\n                \"tokens_per_second\": round(final_stats.tokens_per_second, 1),\n                \"total_duration_ms\": final_stats.total_duration_ms,\n            }\n\n        # WAR ROOM: Add to active context buffer\n        add_mobile_exchange(\n            request_id=request_id,\n            question=text,\n            answer=full_response,\n            profile=profile,\n            stats=stats_dict,\n        )\n\n        # WAR ROOM: Emit complete response with active context\n        await emit_mobile_activity(\"mobile_response\", request_id, {\n            \"question\": text,\n            \"answer\": full_response[:300] + \"...\" if len(full_response) > 300 else full_response,\n            \"stats\": stats_dict,\n            \"active_context\": get_active_context(),\n        })\n\n        return {\n            \"success\": True,\n            \"response\": full_response or \"Inget svar\"\n        }\n\n    except Exception as e:\n        logger.error(f\"Voice command error: {e}\")\n        # WAR ROOM: Emit error\n        await emit_mobile_activity(\"mobile_error\", request_id, {\n            \"error\": str(e)[:100],\n        })\n        return {\n            \"success\": False,\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: SSE streaming endpoint for mobile app", "output": "```python\nasync def voice_command_stream(body: dict) -> StreamingResponse:\n    \"\"\"\n    SSE streaming endpoint for mobile app.\n    Receives transcribed text and streams AI response token by token.\n\n    Request body:\n        text: str - User message\n        profile: str - Agent profile (\"qwen\" or \"nerdy\", default: \"qwen\")\n        history: list - Optional conversation history\n\n    Response: Server-Sent Events stream\n        - Each token: data: {\"token\": \"...\"}\\n\\n\n        - On completion: data: {\"done\": true, \"agent_id\": \"qwen\"}\\n\\n\n        - On error: data: {\"error\": \"message\"}\\n\\n\n    \"\"\"\n    text = body.get(\"text\", \"\")\n    profile = body.get(\"profile\", \"qwen\").lower()\n    history = body.get(\"history\", [])\n\n    if not text:\n        # Return immediate error response\n        async def error_stream() -> AsyncGenerator[str, None]:\n            error_data = {\"error\": \"No text provided\"}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n        return StreamingResponse(\n            error_stream(),\n            media_type=\"text/event-stream\",\n            headers={\n                \"Cache-Control\": \"no-cache\",\n                \"Connection\": \"keep-alive\",\n                \"X-Accel-Buffering\": \"no\"\n            }\n        )\n\n    # Validate profile\n    valid_profiles = [\"qwen\", \"nerdy\", \"gemma\"]\n    if profile not in valid_profiles:\n        profile = \"qwen\"\n        logger.warning(f\"Invalid profile requested, defaulting to qwen. Valid: {valid_profiles}\")\n\n    async def event_stream() -> AsyncGenerator[str, None]:\n        \"\"\"Generate SSE stream from orchestrator\"\"\"\n        try:\n            # Build messages array with history\n            messages = []\n\n            # Add history if provided\n            if history and isinstance(history, list):\n                for msg in history:\n                    if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n                        messages.append({\n                            \"role\": msg[\"role\"],\n                            \"content\": msg[\"content\"]\n                        })\n\n            # Add current user message\n            messages.append({\"role\": \"user\", \"content\": text})\n\n            request_id = str(uuid.uuid4())[:8]\n            logger.info(f\"SSE stream started: request_id={request_id}, profile={profile}, messages={len(messages)}\")\n\n            # Stream tokens from orchestrator\n            agent_id = profile\n            async for token, stats, returned_agent_id in orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n            ):\n                if returned_agent_id:\n                    agent_id = returned_agent_id\n\n                if token:\n                    token_data = {\"token\": token}\n                    yield f\"data: {json.dumps(token_data)}\\n\\n\"\n\n            # Send completion event\n            completion_data = {\n                \"done\": True,\n                \"agent_id\": agent_id\n            }\n            yield f\"data: {json.dumps(completion_data)}\\n\\n\"\n\n            logger.info(f\"SSE stream completed: request_id={request_id}, agent_id={agent_id}\")\n\n        except Exception as e:\n            logger.error(f\"SSE stream error: {e}\", exc_info=True)\n            error_data = {\"error\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get complete system health statistics", "output": "```python\nasync def get_system_health() -> dict:\n    \"\"\"\n    Get complete system health statistics.\n    Returns CPU, RAM, Disk, GPU stats for the admin dashboard.\n    \"\"\"\n    try:\n        stats = await system_monitor.get_stats()\n        return {\n            \"success\": True,\n            **stats.to_dict()\n        }\n    except Exception as e:\n        logger.error(f\"System health error: {e}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get minimal system health for frequent polling", "output": "```python\nasync def get_system_health_quick() -> dict:\n    \"\"\"\n    Get minimal system health for frequent polling.\n    Returns just percentages for CPU, RAM, Disk, GPU.\n    \"\"\"\n    try:\n        summary = await system_monitor.get_quick_summary()\n        return {\n            \"success\": True,\n            **summary,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n    except Exception as e:\n        logger.error(f\"Quick health error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Execute a shell command", "output": "```python\nasync def execute_shell_command(body: dict) -> dict:\n    \"\"\"\n    Execute a shell command.\n\n    WAR ROOM SYNC: Broadcasts mobile_shell event to dashboards.\n\n    Request body:\n        command: str - Command to execute (with or without $ prefix)\n        confirmed: bool - Whether dangerous commands are confirmed\n\n    Response:\n        success, stdout, stderr, exit_code, duration_ms\n    \"\"\"\n    command = body.get(\"command\", \"\")\n    confirmed = body.get(\"confirmed\", False)\n\n    if not command:\n        return {\n            \"success\": False,\n            \"error\": \"No command provided\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    try:\n        result = await shell_executor.execute(command, confirmed=confirmed)\n        result_dict = result.to_dict()\n\n        # WAR ROOM: Emit shell command to dashboards\n        shell_id = f\"shell_{datetime.utcnow().strftime('%H%M%S')}\"\n        await emit_mobile_activity(\"mobile_shell\", shell_id, {\n            \"command\": command[:80],\n            \"stdout\": result_dict.get(\"stdout\", \"\")[:500],\n            \"exit_code\": result_dict.get(\"exit_code\", -1),\n            \"success\": result_dict.get(\"success\", False),\n        })\n\n        return result_dict\n    except Exception as e:\n        logger.error(f\"Shell execute error: {e}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get list of quick commands for the UI", "output": "```python\nasync def get_shell_commands() -> dict:\n    \"\"\"\n    Get list of quick commands for the UI.\n    Returns predefined safe commands for easy access.\n    \"\"\"\n    try:\n        commands = await shell_executor.get_quick_commands()\n        return {\n            \"success\": True,\n            \"commands\": commands\n        }\n    except Exception as e:\n        logger.error(f\"Get commands error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Execute a QWEN SysAdmin tool", "output": "```python\nasync def execute_tool(body: dict) -> dict:\n    \"\"\"\n    Execute a QWEN SysAdmin tool.\n\n    This endpoint is used for:\n    1. Confirming and executing DANGEROUS tools after user approval\n    2. Directly executing tools from admin interface\n\n    Request body:\n        tool: str - Tool name (e.g., \"docker_restart\")\n        params: dict - Tool parameters\n        confirmed: bool - Whether user has confirmed (required for DANGEROUS tools)\n        request_id: str - Optional request ID for pending confirmations\n\n    Response:\n        success, output, error\n    \"\"\"\n    tool_name = body.get(\"tool\", \"\")\n    params = body.get(\"params\", {})\n    confirmed = body.get(\"confirmed\", False)\n    request_id = body.get(\"request_id\")\n\n    if not tool_name:\n        return {\n            \"success\": False,\n            \"error\": \"No tool specified\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # Check if tool exists\n    tool_def = TOOLS.get(tool_name)\n    if not tool_def:\n        return {\n            \"success\": False,\n            \"error\": f\"Unknown tool: {tool_name}. Use GET /api/tools for available tools.\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # Check safety level\n    if tool_def.safety == SafetyLevel.DANGEROUS and not confirmed:\n        return {\n            \"success\": False,\n            \"error\": f\"Tool '{tool_name}' is DANGEROUS and requires confirmation. Set confirmed=true.\",\n            \"requires_confirmation\": True,\n            \"tool\": tool_name,\n            \"description\": tool_def.description,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    # If this is a pending confirmation, remove it from pending\n    if request_id and request_id in _pending_tool_confirmations:\n        pending = _pending_tool_confirmations.pop(request_id)\n        # Use params from pending if not overridden\n        if not params:\n            params = pending.get(\"params\", {})\n\n    try:\n        await emit_system_log(\n            \"tool_execute\",\n            f\"[SYSTEM] Executing {tool_name}...\",\n            \"info\",\n            \"qwen\"\n        )\n\n        # Execute the tool\n        result = await system_tools.execute(tool_name, params)\n\n        # Log result\n        if result.success:\n            await emit_system_log(\n                \"tool_success\",\n                f\"[SYSTEM] {tool_name}: OK\",\n                \"success\",\n                \"qwen\"\n            )\n        else:\n            await emit_system_log(\n                \"tool_error\",\n                f\"[SYSTEM] {tool_name}: {result.error}\",\n                \"error\",\n                \"qwen\"\n            )\n\n        return {\n            \"success\": result.success,\n            \"tool\": tool_name,\n            \"output\": result.output,\n            \"error\": result.error,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    except Exception as e:\n        error_msg = f\"Tool execution error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        await emit_system_log(\"tool_error\", error_msg, \"error\", \"qwen\")\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Confirm and execute a pending DANGEROUS tool", "output": "```python\nasync def confirm_tool(request_id: str) -> dict:\n    \"\"\"\n    Confirm and execute a pending DANGEROUS tool.\n\n    This is called when user clicks \"Godk√§nn\" in the frontend after\n    QWEN requests a dangerous operation.\n\n    Path params:\n        request_id: The request_id from confirmation_required message\n    \"\"\"\n    if request_id not in _pending_tool_confirmations:\n        return {\n            \"success\": False,\n            \"error\": f\"No pending tool confirmation for request_id: {request_id}\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    pending = _pending_tool_confirmations.pop(request_id)\n    tool_name = pending[\"tool\"]\n    params = pending[\"params\"]\n\n    await emit_system_log(\n        \"tool_confirmed\",\n        f\"[USER] Confirmed: {tool_name}\",\n        \"warn\",\n        \"qwen\"\n    )\n\n    try:\n        result = await system_tools.execute(tool_name, params)\n\n        if result.success:\n            await emit_system_log(\n                \"tool_success\",\n                f\"[SYSTEM] {tool_name}: OK\",\n                \"success\",\n                \"qwen\"\n            )\n        else:\n            await emit_system_log(\n                \"tool_error\",\n                f\"[SYSTEM] {tool_name}: {result.error}\",\n                \"error\",\n                \"qwen\"\n            )\n\n        return {\n            \"success\": result.success,\n            \"tool\": tool_name,\n            \"output\": result.output,\n            \"error\": result.error,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    except Exception as e:\n        error_msg = f\"Tool execution error: {str(e)}\"\n        logger.error(error_msg, exc_info=True)\n        return {\n            \"success\": False,\n            \"error\": error_msg,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Cancel a pending DANGEROUS tool", "output": "```python\nasync def cancel_tool(request_id: str) -> dict:\n    \"\"\"\n    Cancel a pending DANGEROUS tool.\n\n    This is called when user clicks \"Avbryt\" in the frontend.\n    \"\"\"\n    if request_id not in _pending_tool_confirmations:\n        return {\n            \"success\": False,\n            \"error\": f\"No pending tool confirmation for request_id: {request_id}\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n\n    pending = _pending_tool_confirmations.pop(request_id)\n    tool_name = pending[\"tool\"]\n\n    await emit_system_log(\n        \"tool_cancelled\",\n        f\"[USER] Cancelled: {tool_name}\",\n        \"warn\",\n        \"qwen\"\n    )\n\n    return {\n        \"success\": True,\n        \"message\": f\"Tool {tool_name} cancelled\",\n        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n    }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Pull latest changes from git repository", "output": "```python\nasync def deploy_git_pull() -> dict:\n    \"\"\"\n    Pull latest changes from git repository.\n    Uses --ff-only to prevent merge conflicts.\n    \"\"\"\n    try:\n        await emit_system_log(\"info\", \"Git pull initiated from mobile app\")\n        result = await deploy_manager.git_pull()\n\n        if result.success:\n            await emit_system_log(\"success\", f\"Git pull completed: {result.output}\")\n        else:\n            await emit_system_log(\"error\", f\"Git pull failed: {result.error}\")\n\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Git pull error: {e}\")\n        return {\n            \"success\": False,\n            \"operation\": \"git_pull\",\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get current git status (modified files, etc", "output": "```python\nasync def deploy_git_status() -> dict:\n    \"\"\"Get current git status (modified files, etc.)\"\"\"\n    try:\n        result = await deploy_manager.git_status()\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Git status error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Restart a systemd service", "output": "```python\nasync def deploy_restart_service(service: str, body: dict = None) -> dict:\n    \"\"\"\n    Restart a systemd service.\n\n    Path params:\n        service: \"simons-ai-backend\", \"simons-ai-frontend\", or \"ollama\"\n    \"\"\"\n    try:\n        await emit_system_log(\"warning\", f\"Service restart initiated: {service}\")\n        result = await deploy_manager.restart_service(service)\n\n        if result.success:\n            await emit_system_log(\"success\", f\"Service {service} restarted\")\n        else:\n            await emit_system_log(\"error\", f\"Failed to restart {service}: {result.error}\")\n\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Restart service error: {e}\")\n        return {\n            \"success\": False,\n            \"operation\": f\"restart_{service}\",\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Get status of a systemd service", "output": "```python\nasync def deploy_service_status(service: str) -> dict:\n    \"\"\"\n    Get status of a systemd service.\n\n    Path params:\n        service: \"simons-ai-backend\", \"simons-ai-frontend\", or \"ollama\"\n    \"\"\"\n    try:\n        result = await deploy_manager.get_service_status(service)\n        return result.to_dict()\n    except Exception as e:\n        logger.error(f\"Service status error: {e}\")\n        return {\"success\": False, \"error\": str(e)}\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Full deployment: git pull + restart backend + restart frontend", "output": "```python\nasync def deploy_full() -> StreamingResponse:\n    \"\"\"\n    Full deployment: git pull + restart backend + restart frontend.\n\n    Returns Server-Sent Events with progress updates.\n    \"\"\"\n    async def event_stream() -> AsyncGenerator[str, None]:\n        try:\n            await emit_system_log(\"warning\", \"Full deploy initiated from mobile app\")\n\n            async for progress in deploy_manager.full_deploy():\n                data = {\n                    \"step\": progress.step,\n                    \"status\": progress.status,\n                    \"message\": progress.message,\n                    \"output\": progress.output,\n                }\n                yield f\"data: {json.dumps(data)}\\n\\n\"\n\n            await emit_system_log(\"success\", \"Full deploy completed\")\n\n        except Exception as e:\n            logger.error(f\"Full deploy error: {e}\")\n            error_data = {\"step\": \"error\", \"status\": \"failed\", \"message\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n\n    return StreamingResponse(\n        event_stream(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Serve the Android APK file with proper headers for download", "output": "```python\nasync def download_apk():\n    \"\"\"\n    Serve the Android APK file with proper headers for download.\n    Uses StreamingResponse to handle the 72MB file efficiently.\n    \"\"\"\n    apk_path = Path(\"/home/ai-server/simons-ai-v2-nerdy-fix.apk\")\n\n    if not apk_path.exists():\n        raise HTTPException(status_code=404, detail=\"APK file not found\")\n\n    file_size = os.path.getsize(apk_path)\n\n    async def file_iterator():\n        \"\"\"Stream the file in chunks to avoid loading it all into memory\"\"\"\n        chunk_size = 1024 * 1024  # 1MB chunks\n        with open(apk_path, \"rb\") as f:\n            while chunk := f.read(chunk_size):\n                yield chunk\n\n    return StreamingResponse(\n        file_iterator(),\n        media_type=\"application/vnd.android.package-archive\",\n        headers={\n            \"Content-Disposition\": f\"attachment; filename={apk_path.name}\",\n            \"Content-Length\": str(file_size),\n            \"Cache-Control\": \"no-cache\",\n        }\n    )\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Generate SSE stream from orchestrator", "output": "```python\n    async def event_stream() -> AsyncGenerator[str, None]:\n        \"\"\"Generate SSE stream from orchestrator\"\"\"\n        try:\n            # Build messages array with history\n            messages = []\n\n            # Add history if provided\n            if history and isinstance(history, list):\n                for msg in history:\n                    if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n                        messages.append({\n                            \"role\": msg[\"role\"],\n                            \"content\": msg[\"content\"]\n                        })\n\n            # Add current user message\n            messages.append({\"role\": \"user\", \"content\": text})\n\n            request_id = str(uuid.uuid4())[:8]\n            logger.info(f\"SSE stream started: request_id={request_id}, profile={profile}, messages={len(messages)}\")\n\n            # Stream tokens from orchestrator\n            agent_id = profile\n            async for token, stats, returned_agent_id in orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n            ):\n                if returned_agent_id:\n                    agent_id = returned_agent_id\n\n                if token:\n                    token_data = {\"token\": token}\n                    yield f\"data: {json.dumps(token_data)}\\n\\n\"\n\n            # Send completion event\n            completion_data = {\n                \"done\": True,\n                \"agent_id\": agent_id\n            }\n            yield f\"data: {json.dumps(completion_data)}\\n\\n\"\n\n            logger.info(f\"SSE stream completed: request_id={request_id}, agent_id={agent_id}\")\n\n        except Exception as e:\n            logger.error(f\"SSE stream error: {e}\", exc_info=True)\n            error_data = {\"error\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Skapa en async Python-funktion som event stream", "output": "```python\n    async def event_stream() -> AsyncGenerator[str, None]:\n        try:\n            await emit_system_log(\"warning\", \"Full deploy initiated from mobile app\")\n\n            async for progress in deploy_manager.full_deploy():\n                data = {\n                    \"step\": progress.step,\n                    \"status\": progress.status,\n                    \"message\": progress.message,\n                    \"output\": progress.output,\n                }\n                yield f\"data: {json.dumps(data)}\\n\\n\"\n\n            await emit_system_log(\"success\", \"Full deploy completed\")\n\n        except Exception as e:\n            logger.error(f\"Full deploy error: {e}\")\n            error_data = {\"step\": \"error\", \"status\": \"failed\", \"message\": str(e)}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Stream the file in chunks to avoid loading it all into memory", "output": "```python\n    async def file_iterator():\n        \"\"\"Stream the file in chunks to avoid loading it all into memory\"\"\"\n        chunk_size = 1024 * 1024  # 1MB chunks\n        with open(apk_path, \"rb\") as f:\n            while chunk := f.read(chunk_size):\n                yield chunk\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Skapa en async Python-funktion som error stream", "output": "```python\n        async def error_stream() -> AsyncGenerator[str, None]:\n            error_data = {\"error\": \"No text provided\"}\n            yield f\"data: {json.dumps(error_data)}\\n\\n\"\n```\n\nDenna funktion finns i `app/api/routes.py`."}
{"instruction": "Implementera en Python-funktion: Proxy till Claude Stats API p√• dev-maskinen", "output": "```python\nasync def get_claude_stats() -> dict:\n    \"\"\"\n    Proxy till Claude Stats API p√• dev-maskinen.\n    Returnerar tokens, kostnad, modeller och aktiv status.\n    \"\"\"\n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(f\"{DEV_MACHINE_URL}/api/all\")\n            \n            if response.status_code == 200:\n                return {\n                    \"success\": True,\n                    **response.json(),\n                    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n                }\n            else:\n                return {\n                    \"success\": False,\n                    \"error\": f\"API returned {response.status_code}\",\n                    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n                }\n                \n    except httpx.ConnectError:\n        return {\n            \"success\": False,\n            \"error\": \"Cannot connect to dev machine (192.168.86.27:8765)\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nDenna funktion finns i `app/api/claude_routes.py`."}
{"instruction": "Implementera en Python-funktion: Dependency injection helper", "output": "```python\nasync def get_shell_executor() -> ShellExecutor:\n    \"\"\"Dependency injection helper\"\"\"\n    return shell_executor\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Check if command requires confirmation", "output": "```python\n    def _is_dangerous(self, command: str) -> bool:\n        \"\"\"Check if command requires confirmation\"\"\"\n        cmd_lower = command.lower()\n        return any(pattern in cmd_lower for pattern in self.DANGEROUS_PATTERNS)\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Return list of quick commands for the UI", "output": "```python\n    async def get_quick_commands(self) -> list[dict]:\n        \"\"\"Return list of quick commands for the UI\"\"\"\n        return [\n            {\"id\": \"uptime\", \"command\": \"uptime\", \"description\": \"Visa systemupptid\", \"dangerous\": False},\n            {\"id\": \"free\", \"command\": \"free -h\", \"description\": \"Visa minnesanv√§ndning\", \"dangerous\": False},\n            {\"id\": \"df\", \"command\": \"df -h\", \"description\": \"Visa diskutrymme\", \"dangerous\": False},\n            {\"id\": \"ps\", \"command\": \"ps aux --sort=-%cpu | head -10\", \"description\": \"Topp 10 processer (CPU)\", \"dangerous\": False},\n            {\"id\": \"nvidia\", \"command\": \"nvidia-smi\", \"description\": \"GPU-status\", \"dangerous\": False},\n            {\"id\": \"ollama_list\", \"command\": \"ollama list\", \"description\": \"Visa Ollama-modeller\", \"dangerous\": False},\n            {\"id\": \"ollama_ps\", \"command\": \"ollama ps\", \"description\": \"Visa k√∂rande modeller\", \"dangerous\": False},\n            {\"id\": \"backend_logs\", \"command\": \"journalctl -u simons-ai-backend -n 30 --no-pager\", \"description\": \"Backend-loggar (30 rader)\", \"dangerous\": False},\n            {\"id\": \"frontend_logs\", \"command\": \"journalctl -u simons-ai-frontend -n 30 --no-pager\", \"description\": \"Frontend-loggar (30 rader)\", \"dangerous\": False},\n            {\"id\": \"restart_backend\", \"command\": \"sudo systemctl restart simons-ai-backend\", \"description\": \"Starta om backend\", \"dangerous\": True},\n            {\"id\": \"restart_frontend\", \"command\": \"sudo systemctl restart simons-ai-frontend\", \"description\": \"Starta om frontend\", \"dangerous\": True},\n        ]\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Number of commands executed this session", "output": "```python\n    def execution_count(self) -> int:\n        \"\"\"Number of commands executed this session\"\"\"\n        return self._execution_count\n```\n\nDenna funktion finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-klass: Result from shell command execution", "output": "```python\nclass ShellResult:\n    \"\"\"Result from shell command execution\"\"\"\n    success: bool\n    command: str\n    stdout: str\n    stderr: str\n    exit_code: int\n    duration_ms: int\n    requires_confirmation: bool = False\n    message: Optional[str] = None\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"command\": self.command,\n            \"stdout\": self.stdout,\n            \"stderr\": self.stderr,\n            \"exit_code\": self.exit_code,\n            \"duration_ms\": self.duration_ms,\n            \"requires_confirmation\": self.requires_confirmation,\n            \"message\": self.message,\n            \"timestamp\": self.timestamp,\n        }\n```\n\nKlassen finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-klass: Execute shell commands with safety checks", "output": "```python\nclass ShellExecutor:\n    \"\"\"\n    Execute shell commands with safety checks.\n\n    Security model: Trust LAN, require confirmation for dangerous commands.\n    This is NOT secure for public internet - only use on local network.\n    \"\"\"\n\n    # Patterns that require user confirmation before execution\n    DANGEROUS_PATTERNS = [\n        \"restart\",\n        \"stop\",\n        \"kill\",\n        \"rm \",\n        \"rm\\t\",\n        \"reboot\",\n        \"shutdown\",\n        \"systemctl\",\n        \"service \",\n        \"pkill\",\n        \"killall\",\n        \"dd \",\n        \"mkfs\",\n        \"> /\",  # redirect to root\n    ]\n\n    # Commands that are always blocked (even with confirmation)\n    BLOCKED_PATTERNS = [\n        \"rm -rf /\",\n        \"rm -rf /*\",\n        \":(){ :|:& };:\",  # fork bomb\n        \"> /dev/sda\",\n        \"mkfs /dev/sd\",\n    ]\n\n    def __init__(self):\n        self._execution_count = 0\n\n    def _is_dangerous(self, command: str) -> bool:\n        \"\"\"Check if command requires confirmation\"\"\"\n        cmd_lower = command.lower()\n        return any(pattern in cmd_lower for pattern in self.DANGEROUS_PATTERNS)\n\n    def _is_blocked(self, command: str) -> bool:\n        \"\"\"Check if command is always blocked\"\"\"\n        cmd_lower = command.lower().replace(\"  \", \" \")\n        return any(pattern in cmd_lower for pattern in self.BLOCKED_PATTERNS)\n\n    async def execute(\n        self,\n        command: str,\n        confirmed: bool = False,\n        timeout: int = DEFAULT_TIMEOUT\n    ) -> ShellResult:\n        \"\"\"\n        Execute a shell command.\n\n        Args:\n            command: Shell command to execute\n            confirmed: Whether user has confirmed dangerous commands\n            timeout: Timeout in seconds\n\n        Returns:\n            ShellResult with output and status\n        \"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n\n        # Strip the $ prefix if present\n        if command.startswith(\"$ \"):\n            command = command[2:]\n        elif command.startswith(\"$\"):\n            command = command[1:]\n\n        command = command.strip()\n\n        if not command:\n            return ShellResult(\n                success=False,\n                command=command,\n                stdout=\"\",\n                stderr=\"No command provided\",\n                exit_code=-1,\n                duration_ms=0,\n                message=\"Empty command\",\n                timestamp=timestamp,\n            )\n\n        # Check if blocked\n        if self._is_blocked(command):\n            logger.warning(f\"Blocked dangerous command: {command}\")\n            return ShellResult(\n                success=False,\n                command=command,\n                stdout=\"\",\n                stderr=\"This command is blocked for safety reasons\",\n                exit_code=-1,\n                duration_ms=0,\n                message=\"Command blocked\",\n                timestamp=timestamp,\n            )\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/shell_executor.py`."}
{"instruction": "Implementera en Python-funktion: Load environment variables from ", "output": "```python\ndef _load_env():\n    \"\"\"Load environment variables from .env and .env.local files\"\"\"\n    for env_file in [\".env\", \".env.local\"]:\n        env_path = Path(__file__).parent.parent.parent / env_file\n        if env_path.exists():\n            with open(env_path) as f:\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith(\"#\") and \"=\" in line:\n                        key, _, value = line.partition(\"=\")\n                        key = key.strip()\n                        value = value.strip().strip('\"').strip(\"'\")\n                        if key and key not in os.environ:\n                            os.environ[key] = value\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Skapa en Python-funktion som total duration ms", "output": "```python\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Skapa en Python-funktion som tokens per second", "output": "```python\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Claude UI URL is configured", "output": "```python\n    def is_configured(self) -> bool:\n        \"\"\"Check if Claude UI URL is configured\"\"\"\n        return bool(self.ui_url)\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Claude Code UI WebSocket is reachable", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Claude Code UI WebSocket is reachable\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Claude UI URL not configured\")\n            return False\n\n        try:\n            # Try to connect to WebSocket\n            ws_url = self.ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n            async with websockets.connect(\n                ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers={\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n            ) as ws:\n                return True\n        except Exception as e:\n            logger.warning(f\"Claude UI connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Extract the latest user message from conversation", "output": "```python\n    def _build_command_from_messages(self, messages: list[dict], system_prompt: Optional[str] = None) -> str:\n        \"\"\"Extract the latest user message from conversation\"\"\"\n        # Find the last user message\n        for msg in reversed(messages):\n            if msg.get(\"role\") == \"user\":\n                return msg.get(\"content\", \"\")\n        \n        # If no user message, use system prompt or empty\n        return system_prompt or \"\"\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Stream chat completion tokens from Claude CLI", "output": "```python\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[ClaudeStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Claude CLI.\n\n        Yields:\n            Tuple of (token_content, stats)\n            - During streaming: (token, None)\n            - Final yield: (\"\", ClaudeStats)\n        \"\"\"\n        if not self.is_configured:\n            raise ClaudeError(\"Claude CLI not found\")\n\n        stats = ClaudeStats(start_time=time.time())\n\n        # Extract command from messages\n        command = self._build_command_from_messages(messages, system_prompt)\n\n        # Build WebSocket message\n        ws_message = {\n            \"type\": \"claude-command\",\n            \"command\": command,\n            \"options\": {\n                \"model\": self.model,\n                \"sessionId\": request_id,  # Use request_id as session ID\n                \"projectPath\": os.getcwd()  # Use current working directory\n            }\n        }\n\n        logger.info(f\"[{request_id}] Starting Claude UI chat with {self.model}\")\n\n        try:\n            # Connect to WebSocket\n            headers = {}\n            if self.token:\n                headers[\"Authorization\"] = f\"Bearer {self.token}\"\n\n            async with websockets.connect(\n                self.ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers=headers\n            ) as ws:\n                # Send command\n                await ws.send(json.dumps(ws_message))\n\n                # Read streaming responses\n                async for message in ws:\n                    try:\n                        data = json.loads(message)\n                        \n                        # Handle different message types from Claude UI\n                        if data.get(\"type\") == \"claude-response\":\n                            # Content chunk\n                            content = data.get(\"data\", {}).get(\"content\", \"\")\n                            if content:\n                                if stats.first_token_time is None:\n                                    stats.first_token_time = time.time()\n                                stats.tokens_generated += len(content) // 4\n                                yield content, None\n                        \n                        elif data.get(\"type\") == \"result\":\n                            # Final result\n                            content = data.get(\"content\", \"\")\n                            if content:\n                                if stats.first_token_time is None:\n                                    stats.first_token_time = time.time()\n                                stats.tokens_generated += len(content) // 4\n                                yield content, None\n                            break\n                        \n                        elif data.get(\"type\") == \"error\":\n                            error_msg = data.get(\"error\", \"Unknown error\")\n                            logger.error(f\"Claude UI error: {error_msg}\")\n                            raise ClaudeError(f\"Claude UI error: {error_msg}\")\n\n                    except json.JSONDecodeError:\n                        # Try to treat as plain text\n                        if message:\n                            if stats.first_token_time is None:\n                                stats.first_token_time = time.time()\n                            stats.tokens_generated += len(message) // 4\n                            yield message, None\n\n        except asyncio.TimeoutError:\n            stats.end_time = time.time()\n            raise ClaudeTimeoutError(f\"Request timed out after {READ_TIMEOUT}s\")\n\n        except Exception as e:\n            stats.end_time = time.time()\n            raise ClaudeError(f\"Claude CLI error: {e}\") from e\n\n        # Final yield with stats\n        stats.end_time = time.time()\n        logger.info(\n            f\"[{request_id}] Claude completed: {stats.tokens_generated} tokens \"\n            f\"in {stats.total_duration_ms}ms ({stats.tokens_per_second:.1f} tok/s)\"\n        )\n        yield \"\", stats\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Close WebSocket connection", "output": "```python\n    async def close(self) -> None:\n        \"\"\"Close WebSocket connection\"\"\"\n        if self._ws:\n            await self._ws.close()\n            self._ws = None\n```\n\nDenna funktion finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Claude CLI response", "output": "```python\nclass ClaudeStats:\n    \"\"\"Statistics from Claude CLI response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Claude errors", "output": "```python\nclass ClaudeError(Exception):\n    \"\"\"Base exception for Claude errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Claude CLI failed", "output": "```python\nclass ClaudeConnectionError(ClaudeError):\n    \"\"\"Connection to Claude CLI failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Operation timed out", "output": "```python\nclass ClaudeTimeoutError(ClaudeError):\n    \"\"\"Operation timed out\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Claude CLI with streaming support", "output": "```python\nclass ClaudeClient:\n    \"\"\"\n    Async client for Claude CLI with streaming support.\n\n    Uses Claude Code CLI in --print mode with stream-json format\n    for real-time token streaming.\n    \"\"\"\n\n    def __init__(self, ui_url: str = CLAUDE_UI_URL, token: str = CLAUDE_UI_TOKEN, model: str = CLAUDE_MODEL):\n        self.ui_url = ui_url\n        self.ws_url = ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n        self.token = token\n        self.model = model\n        self._ws: Optional[WebSocketClientProtocol] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if Claude UI URL is configured\"\"\"\n        return bool(self.ui_url)\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Claude Code UI WebSocket is reachable\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Claude UI URL not configured\")\n            return False\n\n        try:\n            # Try to connect to WebSocket\n            ws_url = self.ui_url.replace(\"http://\", \"ws://\").replace(\"https://\", \"wss://\") + \"/ws\"\n            async with websockets.connect(\n                ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers={\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n            ) as ws:\n                return True\n        except Exception as e:\n            logger.warning(f\"Claude UI connection check failed: {e}\")\n            return False\n\n    def _build_command_from_messages(self, messages: list[dict], system_prompt: Optional[str] = None) -> str:\n        \"\"\"Extract the latest user message from conversation\"\"\"\n        # Find the last user message\n        for msg in reversed(messages):\n            if msg.get(\"role\") == \"user\":\n                return msg.get(\"content\", \"\")\n        \n        # If no user message, use system prompt or empty\n        return system_prompt or \"\"\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[ClaudeStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Claude CLI.\n\n        Yields:\n            Tuple of (token_content, stats)\n            - During streaming: (token, None)\n            - Final yield: (\"\", ClaudeStats)\n        \"\"\"\n        if not self.is_configured:\n            raise ClaudeError(\"Claude CLI not found\")\n\n        stats = ClaudeStats(start_time=time.time())\n\n        # Extract command from messages\n        command = self._build_command_from_messages(messages, system_prompt)\n\n        # Build WebSocket message\n        ws_message = {\n            \"type\": \"claude-command\",\n            \"command\": command,\n            \"options\": {\n                \"model\": self.model,\n                \"sessionId\": request_id,  # Use request_id as session ID\n                \"projectPath\": os.getcwd()  # Use current working directory\n            }\n        }\n\n        logger.info(f\"[{request_id}] Starting Claude UI chat with {self.model}\")\n\n        try:\n            # Connect to WebSocket\n            headers = {}\n            if self.token:\n                headers[\"Authorization\"] = f\"Bearer {self.token}\"\n\n            async with websockets.connect(\n                self.ws_url,\n                timeout=CONNECT_TIMEOUT,\n                extra_headers=headers\n            ) as ws:\n                # Send command\n                await ws.send(json.dumps(ws_message))\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/claude_client.py`."}
{"instruction": "Implementera en Python-funktion: Get agent config by ID, defaults to QWEN_SONNET", "output": "```python\ndef get_agent(agent_id: str) -> AgentConfig:\n    \"\"\"Get agent config by ID, defaults to QWEN_SONNET\"\"\"\n    try:\n        normalized = agent_id.lower().replace(\"_\", \"-\")\n        return AGENTS[AgentId(normalized)]\n    except (ValueError, KeyError):\n        legacy_map = {\n            \"qwen\": AgentId.QWEN_SONNET,\n            \"qwen_agent\": AgentId.DEVSTRAL_OPENHANDS,\n            \"architect\": AgentId.QWEN_SONNET,\n            \"coder\": AgentId.DEVSTRAL_OPENHANDS,\n            \"sven-gpt\": AgentId.QWEN_SONNET,\n            \"sven-coder\": AgentId.DEVSTRAL_OPENHANDS,\n            \"gpt-oss\": AgentId.QWEN_SONNET,\n            \"devstral\": AgentId.DEVSTRAL_OPENHANDS,\n        }\n        if agent_id.lower() in legacy_map:\n            return AGENTS[legacy_map[agent_id.lower()]]\n        logger.warning(f\"Unknown agent '{agent_id}', defaulting to QWEN_SONNET\")\n        return AGENTS[AgentId.QWEN_SONNET]\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som from ollama", "output": "```python\n    def from_ollama(cls, stats: StreamStats, model: str, agent_id: str = \"qwen-sonnet\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_eval_count,\n            provider=\"ollama\",\n            model=model,\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som from qwen agent", "output": "```python\n    def from_qwen_agent(cls, stats: QwenAgentStats, agent_id: str = \"devstral-openhands\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=0,\n            provider=\"qwen_agent\",\n            model=MODEL_WAVES_PRIMARY,\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-funktion: Get status of all agents", "output": "```python\n    async def get_status(self) -> dict:\n        \"\"\"Get status of all agents\"\"\"\n        ollama_ok = await self._ollama.is_connected()\n        nerdy_ok = await self._ollama_clients[\"nerdy\"].is_connected()\n\n        return {\n            \"agents\": {\n                \"qwen-sonnet\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_DAILY,\n                    \"display_name\": MODEL_DAILY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_DAILY_DESC\n                },\n                \"devstral-openhands\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_WAVES_PRIMARY,\n                    \"display_name\": MODEL_WAVES_PRIMARY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_WAVES_PRIMARY_DESC\n                },\n                \"qwen-openhands\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_WAVES_BACKUP,\n                    \"display_name\": MODEL_WAVES_BACKUP_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_WAVES_BACKUP_DESC\n                },\n                \"nerdy\": {\n                    \"available\": nerdy_ok,\n                    \"model\": \"qwen2.5:3b-instruct\",\n                    \"display_name\": \"NERDY\",\n                    \"provider\": \"ollama\",\n                    \"server\": \"192.168.86.27:11434\",\n                    \"description\": \"Legal/Compliance\"\n                }\n            },\n            \"providers\": {\n                \"ollama_local\": {\"connected\": ollama_ok, \"server\": \"localhost:11434\"},\n                \"ollama_nerdy\": {\"connected\": nerdy_ok, \"server\": \"192.168.86.27:11434\"},\n            }\n        }\n```\n\nDenna funktion finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Agent identifiers matching frontend expectations", "output": "```python\nclass AgentId(str, Enum):\n    \"\"\"Agent identifiers matching frontend expectations\"\"\"\n    QWEN_SONNET = \"qwen-sonnet\"\n    DEVSTRAL_OPENHANDS = \"devstral-openhands\"\n    QWEN_OPENHANDS = \"qwen-openhands\"\n    # Legacy aliases\n    GPT_OSS = \"gpt-oss\"\n    DEVSTRAL = \"devstral\"\n    QWEN = \"qwen\"\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Backend providers", "output": "```python\nclass Provider(str, Enum):\n    \"\"\"Backend providers\"\"\"\n    OLLAMA = \"ollama\"\n    QWEN_AGENT = \"qwen_agent\"\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Unified statistics across providers", "output": "```python\nclass UnifiedStats:\n    \"\"\"Unified statistics across providers\"\"\"\n    tokens_generated: int = 0\n    tokens_per_second: float = 0.0\n    total_duration_ms: int = 0\n    prompt_tokens: int = 0\n    provider: str = \"unknown\"\n    model: str = \"unknown\"\n    agent_id: str = \"qwen-sonnet\"\n\n    @classmethod\n    def from_ollama(cls, stats: StreamStats, model: str, agent_id: str = \"qwen-sonnet\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_eval_count,\n            provider=\"ollama\",\n            model=model,\n            agent_id=agent_id\n        )\n\n    @classmethod\n    def from_qwen_agent(cls, stats: QwenAgentStats, agent_id: str = \"devstral-openhands\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=0,\n            provider=\"qwen_agent\",\n            model=MODEL_WAVES_PRIMARY,\n            agent_id=agent_id\n        )\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Implementera en Python-klass: Routes requests to the appropriate AI agent", "output": "```python\nclass MultiAgentOrchestrator:\n    \"\"\"\n    Routes requests to the appropriate AI agent.\n    System prompts are in Modelfiles - we just forward messages.\n    \"\"\"\n\n    def __init__(self):\n        self._ollama = ollama_client\n        self._ollama_clients = {\n            \"qwen-sonnet\": ollama_client,\n            \"devstral-openhands\": ollama_client,\n            \"qwen-openhands\": ollama_client,\n            \"gpt-oss\": ollama_client,\n            \"devstral\": ollama_client,\n            \"nerdy\": OllamaClient(\"http://192.168.86.27:11434\"),\n        }\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        profile: str = \"qwen\",\n        temperature: Optional[float] = None,\n        max_tokens: int = 4096,\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str], None]:\n        \"\"\"\n        Stream chat completion to the specified agent.\n        NO system prompt injection - Modelfile has the prompt.\n        \"\"\"\n        agent = get_agent(profile)\n        temp = temperature if temperature is not None else agent.temperature\n\n        logger.info(\n            f\"[{request_id}] Routing to agent: {agent.display_name} \"\n            f\"(model={agent.model}, provider={agent.provider.value})\"\n        )\n\n        try:\n            if agent.provider == Provider.QWEN_AGENT:\n                async for token, stats in qwen_agent_provider.chat_stream(\n                    messages=messages,\n                    system_prompt=\"\",  # Empty - prompt in Modelfile\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_qwen_agent(stats, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n\n            else:  # OLLAMA (local models)\n                # NO system prompt injection - Modelfile has it\n                msgs_clean = [\n                    {\"role\": m.get(\"role\"), \"content\": m.get(\"content\")}\n                    for m in messages if m.get(\"role\") != \"system\"\n                ]\n\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 4096\n\n                profile_obj = SimpleProfile(agent.id.value, agent.model, temp, max_tokens)\n                agent_ollama = self._ollama_clients.get(agent.id.value, self._ollama)\n\n                async for token, stats in agent_ollama.chat_stream(\n                    profile=profile_obj,\n                    messages=msgs_clean,\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_ollama(stats, agent.model, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n\n        except OllamaError as e:\n            logger.error(f\"[{request_id}] Agent {agent.id.value} failed: {e}\")\n            raise\n\n    async def get_status(self) -> dict:\n        \"\"\"Get status of all agents\"\"\"\n        ollama_ok = await self._ollama.is_connected()\n        nerdy_ok = await self._ollama_clients[\"nerdy\"].is_connected()\n\n        return {\n            \"agents\": {\n                \"qwen-sonnet\": {\n                    \"available\": ollama_ok,\n                    \"model\": MODEL_DAILY,\n                    \"display_name\": MODEL_DAILY_NAME,\n                    \"provider\": \"ollama\",\n                    \"server\": \"localhost:11434\",\n                    \"description\": MODEL_DAILY_DESC\n                },\n                \"devstral-openhands\": {\n                    \"available\": ollama_ok,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-klass f√∂r simple profile", "output": "```python\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 4096\n```\n\nKlassen finns i `app/services/Backend_Fraga_Router.py`."}
{"instruction": "Skapa en Python-funktion som time to first token ms", "output": "```python\n    def time_to_first_token_ms(self) -> Optional[int]:\n        if self.first_token_time and self.start_time:\n            return int((self.first_token_time - self.start_time) * 1000)\n        return None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get or create HTTP client", "output": "```python\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get model-specific configuration options", "output": "```python\n    def _get_model_options(self, profile: ModelProfile) -> dict:\n        \"\"\"\n        Get model-specific configuration options.\n        Uses optimized configs for the SVEN dual-expert system.\n        \"\"\"\n        # QWEN-SONNET (Daily Coder)\n        if MODEL_DAILY in profile.model or \"qwen-sonnet\" in profile.model:\n            logger.info(f\"Using QWEN_SONNET_CONFIG for {profile.model} with {QWEN_SONNET_CONFIG['num_ctx']} context\")\n            return QWEN_SONNET_CONFIG.copy()\n\n        # DEVSTRAL-OPENHANDS (Waves Primary)\n        if MODEL_WAVES_PRIMARY in profile.model or \"devstral\" in profile.model:\n            logger.info(f\"Using DEVSTRAL_CONFIG for {profile.model} with {DEVSTRAL_CONFIG['num_ctx']} context\")\n            return DEVSTRAL_CONFIG.copy()\n\n        # QWEN-OPENHANDS (Waves Backup)\n        if MODEL_WAVES_BACKUP in profile.model or \"qwen-openhands\" in profile.model:\n            logger.info(f\"Using QWEN_OPENHANDS_CONFIG for {profile.model} with {QWEN_OPENHANDS_CONFIG['num_ctx']} context\")\n            return QWEN_OPENHANDS_CONFIG.copy()\n\n        # Fallback to profile defaults\n        return {\n            \"temperature\": profile.temperature,\n            \"top_p\": profile.top_p,\n            \"repeat_penalty\": profile.repeat_penalty,\n            \"num_predict\": profile.max_tokens,\n            \"num_ctx\": profile.context_length,\n        }\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Close the HTTP client", "output": "```python\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Ollama server is reachable", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Ollama server is reachable\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(\n                f\"{self.base_url}/api/tags\",\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Ollama connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Get Ollama version", "output": "```python\n    async def get_version(self) -> Optional[str]:\n        \"\"\"Get Ollama version\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/version\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"version\")\n        except Exception:\n            pass\n        return None\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: List available models (downloaded)", "output": "```python\n    async def list_models(self) -> list[str]:\n        \"\"\"List available models (downloaded)\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/tags\")\n            if response.status_code == 200:\n                data = response.json()\n                return [m[\"name\"] for m in data.get(\"models\", [])]\n        except Exception as e:\n            logger.error(f\"Failed to list models: {e}\")\n        return []\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: List currently loaded/running models", "output": "```python\n    async def list_running_models(self) -> list[dict]:\n        \"\"\"List currently loaded/running models\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/ps\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"models\", [])\n        except Exception as e:\n            logger.error(f\"Failed to list running models: {e}\")\n        return []\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if a specific model is downloaded", "output": "```python\n    async def is_model_available(self, model_name: str) -> bool:\n        \"\"\"Check if a specific model is downloaded\"\"\"\n        models = await self.list_models()\n        # Handle both full names and short names\n        return any(\n            model_name in m or m.startswith(model_name.split(\":\")[0])\n            for m in models\n        )\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if model is currently loaded in memory", "output": "```python\n    async def is_model_loaded(self, model_name: str) -> bool:\n        \"\"\"Check if model is currently loaded in memory\"\"\"\n        running = await self.list_running_models()\n        return any(model_name in m.get(\"name\", \"\") for m in running)\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Unload a specific model from VRAM by setting keep_alive=0", "output": "```python\n    async def unload_model(self, model_name: str) -> bool:\n        \"\"\"Unload a specific model from VRAM by setting keep_alive=0\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.post(\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": model_name,\n                    \"prompt\": \"\",\n                    \"keep_alive\": 0\n                },\n                timeout=10.0\n            )\n            logger.info(f\"Unloaded model: {model_name}\")\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Failed to unload {model_name}: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Warm up a model by sending a minimal request", "output": "```python\n    async def warmup_model(self, profile: ModelProfile) -> bool:\n        \"\"\"\n        Warm up a model by sending a minimal request.\n        This pre-loads the model into GPU memory.\n        \"\"\"\n        logger.info(f\"Warming up model: {profile.model}\")\n        profile_manager.set_loading(profile.id)\n\n        # Auto-unload andra modeller f√∂rst (RTX 4070 kan bara ha en)\n        unloaded = await self.unload_other_models(profile.model)\n        if unloaded:\n            logger.info(f\"Auto-unloaded models to free VRAM: {unloaded}\")\n\n        try:\n            client = await self._get_client()\n\n            # Send minimal generate request to load model\n            async with client.stream(\n                \"POST\",\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": profile.model,\n                    \"prompt\": \"Hi\",\n                    \"options\": {\"num_predict\": 1}\n                },\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=WARMUP_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            ) as response:\n                if response.status_code != 200:\n                    logger.error(f\"Warmup failed: {response.status_code}\")\n                    return False\n\n                # Consume stream to ensure model loads\n                async for _ in response.aiter_lines():\n                    pass\n\n            profile_manager.set_active(profile.id)\n            logger.info(f\"Model warmed up: {profile.model}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Warmup error for {profile.model}: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Non-streaming chat completion", "output": "```python\n    async def chat_complete(\n        self,\n        profile: ModelProfile,\n        messages: list[dict],\n        request_id: str\n    ) -> tuple[str, StreamStats]:\n        \"\"\"\n        Non-streaming chat completion.\n        Returns complete response and stats.\n        \"\"\"\n        full_response = []\n        stats = None\n\n        async for token, final_stats in self.chat_stream(profile, messages, request_id):\n            if token:\n                full_response.append(token)\n            if final_stats:\n                stats = final_stats\n\n        return \"\".join(full_response), stats\n```\n\nDenna funktion finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics collected during streaming", "output": "```python\nclass StreamStats:\n    \"\"\"Statistics collected during streaming\"\"\"\n    tokens_generated: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n    prompt_eval_count: int = 0\n    prompt_eval_duration_ns: int = 0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n\n    @property\n    def time_to_first_token_ms(self) -> Optional[int]:\n        if self.first_token_time and self.start_time:\n            return int((self.first_token_time - self.start_time) * 1000)\n        return None\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Ollama errors", "output": "```python\nclass OllamaError(Exception):\n    \"\"\"Base exception for Ollama errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Ollama failed", "output": "```python\nclass OllamaConnectionError(OllamaError):\n    \"\"\"Connection to Ollama failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Requested model not available", "output": "```python\nclass OllamaModelNotFoundError(OllamaError):\n    \"\"\"Requested model not available\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Ollama API with streaming support", "output": "```python\nclass OllamaClient:\n    \"\"\"\n    Async client for Ollama API with streaming support.\n\n    Handles:\n    - Streaming chat completions\n    - Model availability checking\n    - Model loading/unloading tracking\n    - Connection health checks\n    \"\"\"\n\n    def __init__(self, base_url: str = OLLAMA_BASE_URL):\n        self.base_url = base_url\n        self._client: Optional[httpx.AsyncClient] = None\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    def _get_model_options(self, profile: ModelProfile) -> dict:\n        \"\"\"\n        Get model-specific configuration options.\n        Uses optimized configs for the SVEN dual-expert system.\n        \"\"\"\n        # QWEN-SONNET (Daily Coder)\n        if MODEL_DAILY in profile.model or \"qwen-sonnet\" in profile.model:\n            logger.info(f\"Using QWEN_SONNET_CONFIG for {profile.model} with {QWEN_SONNET_CONFIG['num_ctx']} context\")\n            return QWEN_SONNET_CONFIG.copy()\n\n        # DEVSTRAL-OPENHANDS (Waves Primary)\n        if MODEL_WAVES_PRIMARY in profile.model or \"devstral\" in profile.model:\n            logger.info(f\"Using DEVSTRAL_CONFIG for {profile.model} with {DEVSTRAL_CONFIG['num_ctx']} context\")\n            return DEVSTRAL_CONFIG.copy()\n\n        # QWEN-OPENHANDS (Waves Backup)\n        if MODEL_WAVES_BACKUP in profile.model or \"qwen-openhands\" in profile.model:\n            logger.info(f\"Using QWEN_OPENHANDS_CONFIG for {profile.model} with {QWEN_OPENHANDS_CONFIG['num_ctx']} context\")\n            return QWEN_OPENHANDS_CONFIG.copy()\n\n        # Fallback to profile defaults\n        return {\n            \"temperature\": profile.temperature,\n            \"top_p\": profile.top_p,\n            \"repeat_penalty\": profile.repeat_penalty,\n            \"num_predict\": profile.max_tokens,\n            \"num_ctx\": profile.context_length,\n        }\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Ollama server is reachable\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(\n                f\"{self.base_url}/api/tags\",\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Ollama connection check failed: {e}\")\n            return False\n\n    async def get_version(self) -> Optional[str]:\n        \"\"\"Get Ollama version\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/version\")\n            if response.status_code == 200:\n                data = response.json()\n                return data.get(\"version\")\n        except Exception:\n            pass\n        return None\n\n    async def list_models(self) -> list[str]:\n        \"\"\"List available models (downloaded)\"\"\"\n        try:\n            client = await self._get_client()\n            response = await client.get(f\"{self.base_url}/api/tags\")\n            if response.status_code == 200:\n                data = response.json()\n                return [m[\"name\"] for m in data.get(\"models\", [])]\n        except Exception as e:\n            logger.error(f\"Failed to list models: {e}\")\n        return []\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/ollama_client.py`."}
{"instruction": "Implementera en Python-funktion: Convert to dictionary for JSON serialization", "output": "```python\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n        return asdict(self)\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get current system health statistics", "output": "```python\n    async def get_stats(self, force_refresh: bool = False) -> SystemHealth:\n        \"\"\"\n        Get current system health statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            SystemHealth with current system information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.cached_at).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.health\n\n            # Fetch fresh stats\n            health = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedHealth(\n                health=health,\n                cached_at=datetime.utcnow()\n            )\n\n            return health\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Fetch current system statistics", "output": "```python\n    async def _fetch_stats(self) -> SystemHealth:\n        \"\"\"Fetch current system statistics\"\"\"\n        try:\n            # CPU - use interval=None for non-blocking\n            cpu_percent = psutil.cpu_percent(interval=None)\n            cpu_count = psutil.cpu_count()\n\n            # Load average (Unix only)\n            try:\n                load_avg = os.getloadavg()\n            except (AttributeError, OSError):\n                load_avg = (0.0, 0.0, 0.0)\n\n            # RAM\n            ram = psutil.virtual_memory()\n\n            # Disk (root partition)\n            disk = psutil.disk_usage('/')\n\n            # Uptime\n            boot_time = psutil.boot_time()\n            uptime_seconds = int(time.time() - boot_time)\n            boot_time_str = datetime.fromtimestamp(boot_time).isoformat()\n\n            # GPU (from existing monitor)\n            gpu_stats = await gpu_monitor.get_stats()\n            gpu_dict = {\n                \"name\": gpu_stats.name,\n                \"vram_total_gb\": gpu_stats.vram_total_gb,\n                \"vram_used_gb\": gpu_stats.vram_used_gb,\n                \"vram_free_gb\": gpu_stats.vram_free_gb,\n                \"vram_percent\": gpu_stats.vram_percent,\n                \"temperature_c\": gpu_stats.temperature_c,\n                \"gpu_util_percent\": gpu_stats.gpu_util_percent,\n                \"power_draw_w\": gpu_stats.power_draw_w,\n                \"is_available\": gpu_stats.is_available,\n            }\n\n            return SystemHealth(\n                # CPU\n                cpu_percent=round(cpu_percent, 1),\n                cpu_count=cpu_count,\n                load_average=tuple(round(x, 2) for x in load_avg),\n\n                # RAM\n                ram_total_gb=round(ram.total / (1024**3), 2),\n                ram_used_gb=round(ram.used / (1024**3), 2),\n                ram_free_gb=round(ram.available / (1024**3), 2),\n                ram_percent=round(ram.percent, 1),\n\n                # Disk\n                disk_total_gb=round(disk.total / (1024**3), 2),\n                disk_used_gb=round(disk.used / (1024**3), 2),\n                disk_free_gb=round(disk.free / (1024**3), 2),\n                disk_percent=round((disk.used / disk.total) * 100, 1),\n\n                # System\n                uptime_seconds=uptime_seconds,\n                boot_time=boot_time_str,\n\n                # GPU\n                gpu=gpu_dict,\n\n                # Timestamp\n                timestamp=datetime.utcnow().isoformat() + \"Z\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch system stats: {e}\")\n            # Return minimal fallback\n            return SystemHealth(\n                cpu_percent=0.0,\n                cpu_count=0,\n                load_average=(0.0, 0.0, 0.0),\n                ram_total_gb=0.0,\n                ram_used_gb=0.0,\n                ram_free_gb=0.0,\n                ram_percent=0.0,\n                disk_total_gb=0.0,\n                disk_used_gb=0.0,\n                disk_free_gb=0.0,\n                disk_percent=0.0,\n                uptime_seconds=0,\n                boot_time=\"\",\n                gpu=None,\n                timestamp=datetime.utcnow().isoformat() + \"Z\"\n            )\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get minimal stats for WebSocket broadcast", "output": "```python\n    async def get_quick_summary(self) -> dict:\n        \"\"\"Get minimal stats for WebSocket broadcast\"\"\"\n        stats = await self.get_stats()\n        return {\n            \"cpu_percent\": stats.cpu_percent,\n            \"ram_percent\": stats.ram_percent,\n            \"disk_percent\": stats.disk_percent,\n            \"gpu_util_percent\": stats.gpu.get(\"gpu_util_percent\", 0) if stats.gpu else 0,\n            \"gpu_vram_percent\": stats.gpu.get(\"vram_percent\", 0) if stats.gpu else 0,\n        }\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Format uptime as human-readable string", "output": "```python\n    def format_uptime(self, seconds: int) -> str:\n        \"\"\"Format uptime as human-readable string\"\"\"\n        days = seconds // 86400\n        hours = (seconds % 86400) // 3600\n        minutes = (seconds % 3600) // 60\n\n        parts = []\n        if days > 0:\n            parts.append(f\"{days}d\")\n        if hours > 0:\n            parts.append(f\"{hours}h\")\n        if minutes > 0:\n            parts.append(f\"{minutes}m\")\n\n        return \" \".join(parts) if parts else \"< 1m\"\n```\n\nDenna funktion finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Complete system health statistics", "output": "```python\nclass SystemHealth:\n    \"\"\"Complete system health statistics\"\"\"\n    # CPU\n    cpu_percent: float\n    cpu_count: int\n    load_average: tuple[float, float, float]  # 1, 5, 15 min\n\n    # RAM\n    ram_total_gb: float\n    ram_used_gb: float\n    ram_free_gb: float\n    ram_percent: float\n\n    # Disk\n    disk_total_gb: float\n    disk_used_gb: float\n    disk_free_gb: float\n    disk_percent: float\n\n    # System\n    uptime_seconds: int\n    boot_time: str\n\n    # GPU (from gpu_monitor)\n    gpu: Optional[dict] = None\n\n    # Timestamp\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n        return asdict(self)\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Cached system health with timestamp", "output": "```python\nclass CachedHealth:\n    \"\"\"Cached system health with timestamp\"\"\"\n    health: SystemHealth\n    cached_at: datetime\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-klass: Monitors system resources: CPU, RAM, Disk", "output": "```python\nclass SystemMonitor:\n    \"\"\"\n    Monitors system resources: CPU, RAM, Disk.\n\n    Features:\n    - Async-friendly with caching\n    - Integrates with existing GPU monitor\n    - Low overhead polling\n    \"\"\"\n\n    def __init__(self):\n        self._cache: Optional[CachedHealth] = None\n        self._lock = asyncio.Lock()\n\n    async def get_stats(self, force_refresh: bool = False) -> SystemHealth:\n        \"\"\"\n        Get current system health statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            SystemHealth with current system information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.cached_at).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.health\n\n            # Fetch fresh stats\n            health = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedHealth(\n                health=health,\n                cached_at=datetime.utcnow()\n            )\n\n            return health\n\n    async def _fetch_stats(self) -> SystemHealth:\n        \"\"\"Fetch current system statistics\"\"\"\n        try:\n            # CPU - use interval=None for non-blocking\n            cpu_percent = psutil.cpu_percent(interval=None)\n            cpu_count = psutil.cpu_count()\n\n            # Load average (Unix only)\n            try:\n                load_avg = os.getloadavg()\n            except (AttributeError, OSError):\n                load_avg = (0.0, 0.0, 0.0)\n\n            # RAM\n            ram = psutil.virtual_memory()\n\n            # Disk (root partition)\n            disk = psutil.disk_usage('/')\n\n            # Uptime\n            boot_time = psutil.boot_time()\n            uptime_seconds = int(time.time() - boot_time)\n            boot_time_str = datetime.fromtimestamp(boot_time).isoformat()\n\n            # GPU (from existing monitor)\n            gpu_stats = await gpu_monitor.get_stats()\n            gpu_dict = {\n                \"name\": gpu_stats.name,\n                \"vram_total_gb\": gpu_stats.vram_total_gb,\n                \"vram_used_gb\": gpu_stats.vram_used_gb,\n                \"vram_free_gb\": gpu_stats.vram_free_gb,\n                \"vram_percent\": gpu_stats.vram_percent,\n                \"temperature_c\": gpu_stats.temperature_c,\n                \"gpu_util_percent\": gpu_stats.gpu_util_percent,\n                \"power_draw_w\": gpu_stats.power_draw_w,\n                \"is_available\": gpu_stats.is_available,\n            }\n\n            return SystemHealth(\n                # CPU\n                cpu_percent=round(cpu_percent, 1),\n                cpu_count=cpu_count,\n                load_average=tuple(round(x, 2) for x in load_avg),\n\n                # RAM\n                ram_total_gb=round(ram.total / (1024**3), 2),\n                ram_used_gb=round(ram.used / (1024**3), 2),\n                ram_free_gb=round(ram.available / (1024**3), 2),\n                ram_percent=round(ram.percent, 1),\n\n                # Disk\n                disk_total_gb=round(disk.total / (1024**3), 2),\n                disk_used_gb=round(disk.used / (1024**3), 2),\n                disk_free_gb=round(disk.free / (1024**3), 2),\n                disk_percent=round((disk.used / disk.total) * 100, 1),\n\n                # System\n                uptime_seconds=uptime_seconds,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/system_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Analysera planner output f√∂r reward signals", "output": "```python\ndef analyze_plan(content: str) -> Dict[str, float]:\n    \"\"\"Analysera planner output f√∂r reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter tydliga steg\n    step_patterns = [\n        r'##\\s*IMPLEMENTATION\\s*STEG',\n        r'##\\s*STEG',\n        r'\\d+\\.\\s+\\[?Steg',\n        r'1\\.\\s+\\w+',\n    ]\n    signals['has_clear_steps'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in step_patterns\n    ) else 0.0\n\n    # Kolla efter fillista\n    file_patterns = [\n        r'##\\s*FILER',\n        r'- \\[?\\w+\\.(py|ts|js|tsx|jsx)\\]?',\n        r'`\\w+\\.\\w+`',\n    ]\n    signals['has_file_list'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in file_patterns\n    ) else 0.0\n\n    # Kolla efter utmaningar\n    challenge_patterns = [\n        r'##\\s*(POTENTIELLA\\s*)?UTMANINGAR',\n        r'##\\s*RISKER',\n        r'OBS:',\n        r'Utmaning:',\n    ]\n    signals['has_challenges'] = 1.0 if any(\n        re.search(p, content, re.IGNORECASE) for p in challenge_patterns\n    ) else 0.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Analysera coder output f√∂r reward signals", "output": "```python\ndef analyze_code(content: str) -> Dict[str, float]:\n    \"\"\"Analysera coder output f√∂r reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter kodblock\n    code_blocks = re.findall(r'```[\\w]*\\n[\\s\\S]*?\\n```', content)\n    signals['has_code_blocks'] = min(1.0, len(code_blocks) * 0.25)\n\n    # Kolla om koden ser komplett ut\n    completeness_signals = [\n        r'def\\s+\\w+\\s*\\(',  # Funktioner\n        r'class\\s+\\w+',     # Klasser\n        r'return\\s+',       # Return statements\n        r'if\\s+__name__',   # Main guard\n    ]\n    completeness_score = sum(\n        1 for p in completeness_signals if re.search(p, content)\n    ) / len(completeness_signals)\n    signals['code_is_complete'] = completeness_score\n\n    # Kolla efter imports\n    signals['has_imports'] = 1.0 if re.search(\n        r'(import\\s+\\w+|from\\s+\\w+\\s+import)', content\n    ) else 0.0\n\n    # Negativ: placeholders\n    placeholder_patterns = [\n        r'#\\s*\\.\\.\\.',\n        r'//\\s*\\.\\.\\.',\n        r'pass\\s*#\\s*TODO',\n        r'\\.\\.\\.\\s*resten',\n        r'# Add your code here',\n    ]\n    has_placeholders = any(\n        re.search(p, content, re.IGNORECASE) for p in placeholder_patterns\n    )\n    signals['no_placeholders'] = 0.0 if has_placeholders else 1.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Analysera reviewer output f√∂r reward signals", "output": "```python\ndef analyze_review(content: str) -> Dict[str, float]:\n    \"\"\"Analysera reviewer output f√∂r reward signals\"\"\"\n    signals = {}\n\n    # Kolla efter bug-sektion\n    signals['has_bug_section'] = 1.0 if re.search(\n        r'##\\s*(BUGGAR|BUGS|FEL)', content, re.IGNORECASE\n    ) else 0.0\n\n    # Kolla efter verdict\n    signals['has_verdict'] = 1.0 if re.search(\n        r'##\\s*VERDICT|APPROVED|NEEDS\\s*CHANGES', content, re.IGNORECASE\n    ) else 0.0\n\n    # Kolla om approved\n    if re.search(r'APPROVED\\s*[‚úì‚úî]', content, re.IGNORECASE):\n        signals['is_approved'] = 1.0\n    elif re.search(r'NEEDS\\s*CHANGES\\s*[‚úó‚úò]', content, re.IGNORECASE):\n        signals['is_approved'] = 0.3  # Partial credit f√∂r att hitta problem\n    else:\n        signals['is_approved'] = 0.0\n\n    return signals\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r en tr√§ningsepisode med flera tasks", "output": "```python\nasync def run_training_episode(\n    agent: CascadeLitAgent,\n    tasks: List[Dict[str, Any]],\n    episode_id: str\n) -> List[float]:\n    \"\"\"\n    K√∂r en tr√§ningsepisode med flera tasks.\n\n    Args:\n        agent: CascadeLitAgent instance\n        tasks: Lista med training tasks\n        episode_id: Episode identifier\n\n    Returns:\n        Lista med rewards f√∂r varje task\n    \"\"\"\n    rewards = []\n\n    for i, task in enumerate(tasks):\n        rollout_id = f\"{episode_id}-task{i}\"\n        logger.info(f\"Running task {i+1}/{len(tasks)}: {rollout_id}\")\n\n        reward = await agent.training_rollout_async(\n            task=task,\n            rollout_id=rollout_id,\n            resources=None\n        )\n        rewards.append(reward)\n\n        logger.info(f\"Task {i+1} reward: {reward:.2f}\")\n\n    avg_reward = sum(rewards) / len(rewards) if rewards else 0\n    logger.info(f\"Episode {episode_id} complete - avg reward: {avg_reward:.2f}\")\n\n    return rewards\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Ladda training tasks fr√•n JSONL fil", "output": "```python\ndef load_training_tasks(path: str) -> List[Dict[str, Any]]:\n    \"\"\"Ladda training tasks fr√•n JSONL fil\"\"\"\n    import json\n\n    tasks = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                data = json.loads(line)\n                # Konvertera till task-format\n                tasks.append({\n                    \"question\": data.get(\"instruction\", \"\"),\n                    \"expected_answer\": data.get(\"output\", \"\"),\n                })\n\n    return tasks\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Ber√§kna total reward (-1 till +3 skala)", "output": "```python\n    def total(self) -> float:\n        \"\"\"Ber√§kna total reward (-1 till +3 skala)\"\"\"\n        weights = {\n            # Planning (max 0.6)\n            'has_clear_steps': 0.2,\n            'has_file_list': 0.2,\n            'has_challenges': 0.2,\n\n            # Coding (max 1.2)\n            'has_code_blocks': 0.3,\n            'code_is_complete': 0.4,\n            'has_imports': 0.2,\n            'no_placeholders': 0.3,\n\n            # Review (max 0.6)\n            'has_bug_section': 0.2,\n            'has_verdict': 0.2,\n            'is_approved': 0.2,\n\n            # Completion (max 0.6)\n            'task_completed': 0.6,\n        }\n\n        total = sum(\n            getattr(self, key) * weight\n            for key, weight in weights.items()\n        )\n\n        return min(3.0, max(-1.0, total))\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Huvudtr√§ningsloop - k√∂r cascade och ber√§kna reward", "output": "```python\n    async def training_rollout_async(\n        self,\n        task: Dict[str, Any],\n        rollout_id: str,\n        resources: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Huvudtr√§ningsloop - k√∂r cascade och ber√§kna reward.\n\n        Args:\n            task: Training task med \"question\" och optional \"expected_answer\"\n            rollout_id: Unik ID f√∂r denna rollout\n            resources: Tillg√§ngliga resurser (ignoreras f√∂r nu)\n\n        Returns:\n            reward: Float mellan -1 och +3\n        \"\"\"\n        logger.info(f\"[{rollout_id}] Starting training rollout\")\n\n        # Extrahera task\n        question = task.get(\"question\", task.get(\"task\", \"\"))\n        expected = task.get(\"expected_answer\", task.get(\"expected\", \"\"))\n        skip_review = task.get(\"skip_review\", False)\n\n        if not question:\n            logger.error(f\"[{rollout_id}] Empty question in task\")\n            return -1.0\n\n        # Samla output fr√•n cascade\n        plan_content = \"\"\n        code_content = \"\"\n        review_content = \"\"\n        completed = False\n\n        try:\n            async for result in self.cascade.execute_cascade(\n                task=question,\n                request_id=rollout_id,\n                skip_review=skip_review\n            ):\n                if result.phase == CascadePhase.PLANNING:\n                    plan_content += result.content\n                elif result.phase == CascadePhase.CODING:\n                    code_content += result.content\n                elif result.phase == CascadePhase.REVIEWING:\n                    review_content += result.content\n                elif result.phase == CascadePhase.COMPLETE:\n                    completed = True\n\n        except Exception as e:\n            logger.error(f\"[{rollout_id}] Cascade failed: {e}\")\n            return -1.0\n\n        # Ber√§kna reward signals\n        signals = RewardSignals()\n\n        # Planning signals\n        if plan_content:\n            plan_signals = analyze_plan(plan_content)\n            signals.has_clear_steps = plan_signals.get('has_clear_steps', 0)\n            signals.has_file_list = plan_signals.get('has_file_list', 0)\n            signals.has_challenges = plan_signals.get('has_challenges', 0)\n\n        # Coding signals\n        if code_content:\n            code_signals = analyze_code(code_content)\n            signals.has_code_blocks = code_signals.get('has_code_blocks', 0)\n            signals.code_is_complete = code_signals.get('code_is_complete', 0)\n            signals.has_imports = code_signals.get('has_imports', 0)\n            signals.no_placeholders = code_signals.get('no_placeholders', 0)\n\n        # Review signals\n        if review_content:\n            review_signals = analyze_review(review_content)\n            signals.has_bug_section = review_signals.get('has_bug_section', 0)\n            signals.has_verdict = review_signals.get('has_verdict', 0)\n            signals.is_approved = review_signals.get('is_approved', 0)\n\n        # Completion bonus\n        signals.task_completed = 1.0 if completed else 0.0\n\n        # Extra reward f√∂r match mot expected (om tillg√§ngligt)\n        if expected and code_content:\n            # Simple keyword matching\n            expected_keywords = set(re.findall(r'\\w{4,}', expected.lower()))\n            code_keywords = set(re.findall(r'\\w{4,}', code_content.lower()))\n            if expected_keywords:\n                overlap = len(expected_keywords & code_keywords) / len(expected_keywords)\n                signals.task_completed += overlap * 0.5\n\n        # Ber√§kna total reward\n        reward = signals.total()\n\n        logger.info(\n            f\"[{rollout_id}] Rollout complete - \"\n            f\"reward={reward:.2f}, \"\n            f\"plan={len(plan_content)}, code={len(code_content)}, \"\n            f\"review={len(review_content)}\"\n        )\n\n        return reward\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Inference mode (ej tr√§ning) - returnera cascade output", "output": "```python\n    async def inference_async(\n        self,\n        task: Dict[str, Any],\n        request_id: str\n    ) -> Dict[str, str]:\n        \"\"\"\n        Inference mode (ej tr√§ning) - returnera cascade output.\n\n        Args:\n            task: Task att k√∂ra\n            request_id: Request ID\n\n        Returns:\n            {\"plan\": \"...\", \"code\": \"...\", \"review\": \"...\"}\n        \"\"\"\n        return await self.cascade.quick_cascade(\n            task=task.get(\"question\", task.get(\"task\", \"\")),\n            request_id=request_id\n        )\n```\n\nDenna funktion finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-klass: Reward signals fr√•n en cascade-k√∂rning", "output": "```python\nclass RewardSignals:\n    \"\"\"Reward signals fr√•n en cascade-k√∂rning\"\"\"\n    # Planning rewards\n    has_clear_steps: float = 0.0\n    has_file_list: float = 0.0\n    has_challenges: float = 0.0\n\n    # Coding rewards\n    has_code_blocks: float = 0.0\n    code_is_complete: float = 0.0\n    has_imports: float = 0.0\n    no_placeholders: float = 0.0\n\n    # Review rewards\n    has_bug_section: float = 0.0\n    has_verdict: float = 0.0\n    is_approved: float = 0.0\n\n    # Overall\n    task_completed: float = 0.0\n\n    def total(self) -> float:\n        \"\"\"Ber√§kna total reward (-1 till +3 skala)\"\"\"\n        weights = {\n            # Planning (max 0.6)\n            'has_clear_steps': 0.2,\n            'has_file_list': 0.2,\n            'has_challenges': 0.2,\n\n            # Coding (max 1.2)\n            'has_code_blocks': 0.3,\n            'code_is_complete': 0.4,\n            'has_imports': 0.2,\n            'no_placeholders': 0.3,\n\n            # Review (max 0.6)\n            'has_bug_section': 0.2,\n            'has_verdict': 0.2,\n            'is_approved': 0.2,\n\n            # Completion (max 0.6)\n            'task_completed': 0.6,\n        }\n\n        total = sum(\n            getattr(self, key) * weight\n            for key, weight in weights.items()\n        )\n\n        return min(3.0, max(-1.0, total))\n```\n\nKlassen finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-klass: LitAgent wrapper f√∂r Cascade Orchestrator", "output": "```python\nclass CascadeLitAgent(LitAgent):\n    \"\"\"\n    LitAgent wrapper f√∂r Cascade Orchestrator.\n\n    M√∂jligg√∂r RL-tr√§ning av multi-agent cascade genom att:\n    1. K√∂ra cascade p√• training tasks\n    2. Samla output fr√•n varje fas\n    3. Ber√§kna reward baserat p√• output-kvalitet\n    4. Returnera reward f√∂r GRPO/PPO training\n\n    Anv√§ndning:\n        agent = CascadeLitAgent()\n        await agent.training_rollout_async(task, rollout_id, resources)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.cascade = cascade_orchestrator\n        self.ollama = ollama_client\n        logger.info(\"CascadeLitAgent initialized\")\n\n    async def training_rollout_async(\n        self,\n        task: Dict[str, Any],\n        rollout_id: str,\n        resources: Optional[Dict] = None\n    ) -> float:\n        \"\"\"\n        Huvudtr√§ningsloop - k√∂r cascade och ber√§kna reward.\n\n        Args:\n            task: Training task med \"question\" och optional \"expected_answer\"\n            rollout_id: Unik ID f√∂r denna rollout\n            resources: Tillg√§ngliga resurser (ignoreras f√∂r nu)\n\n        Returns:\n            reward: Float mellan -1 och +3\n        \"\"\"\n        logger.info(f\"[{rollout_id}] Starting training rollout\")\n\n        # Extrahera task\n        question = task.get(\"question\", task.get(\"task\", \"\"))\n        expected = task.get(\"expected_answer\", task.get(\"expected\", \"\"))\n        skip_review = task.get(\"skip_review\", False)\n\n        if not question:\n            logger.error(f\"[{rollout_id}] Empty question in task\")\n            return -1.0\n\n        # Samla output fr√•n cascade\n        plan_content = \"\"\n        code_content = \"\"\n        review_content = \"\"\n        completed = False\n\n        try:\n            async for result in self.cascade.execute_cascade(\n                task=question,\n                request_id=rollout_id,\n                skip_review=skip_review\n            ):\n                if result.phase == CascadePhase.PLANNING:\n                    plan_content += result.content\n                elif result.phase == CascadePhase.CODING:\n                    code_content += result.content\n                elif result.phase == CascadePhase.REVIEWING:\n                    review_content += result.content\n                elif result.phase == CascadePhase.COMPLETE:\n                    completed = True\n\n        except Exception as e:\n            logger.error(f\"[{rollout_id}] Cascade failed: {e}\")\n            return -1.0\n\n        # Ber√§kna reward signals\n        signals = RewardSignals()\n\n        # Planning signals\n        if plan_content:\n            plan_signals = analyze_plan(plan_content)\n            signals.has_clear_steps = plan_signals.get('has_clear_steps', 0)\n            signals.has_file_list = plan_signals.get('has_file_list', 0)\n            signals.has_challenges = plan_signals.get('has_challenges', 0)\n\n        # Coding signals\n        if code_content:\n            code_signals = analyze_code(code_content)\n            signals.has_code_blocks = code_signals.get('has_code_blocks', 0)\n            signals.code_is_complete = code_signals.get('code_is_complete', 0)\n            signals.has_imports = code_signals.get('has_imports', 0)\n            signals.no_placeholders = code_signals.get('no_placeholders', 0)\n\n        # Review signals\n        if review_content:\n            review_signals = analyze_review(review_content)\n            signals.has_bug_section = review_signals.get('has_bug_section', 0)\n            signals.has_verdict = review_signals.get('has_verdict', 0)\n            signals.is_approved = review_signals.get('is_approved', 0)\n\n        # Completion bonus\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/cascade_lit_agent.py`."}
{"instruction": "Implementera en Python-funktion: Get Docker sandbox instance", "output": "```python\ndef get_docker_sandbox() -> DockerSandbox:\n    \"\"\"Get Docker sandbox instance\"\"\"\n    return docker_sandbox\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Create a fresh Assistant instance", "output": "```python\n    def _create_assistant(self, system_prompt: str) -> Assistant:\n        \"\"\"Create a fresh Assistant instance\"\"\"\n        return Assistant(\n            function_list=self.function_list,\n            llm=self.llm_cfg,\n            name='QWEN AGENT',\n            description='Elite coder med Code Interpreter',\n            system_message=system_prompt\n        )\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Stream response fr√•n Qwen-Agent med Code Interpreter support", "output": "```python\n    async def chat_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        system_prompt: str,\n        request_id: str\n    ) -> AsyncGenerator[tuple[str, Optional[QwenAgentStats]], None]:\n        \"\"\"\n        Stream response fr√•n Qwen-Agent med Code Interpreter support.\n\n        Args:\n            messages: Lista med {\"role\": \"user/assistant\", \"content\": \"...\"}\n            system_prompt: System message f√∂r agenten\n            request_id: Unik request ID f√∂r logging\n\n        Yields:\n            (token_text, None) f√∂r streaming\n            (\"\", QwenAgentStats) som sista yield med statistik\n        \"\"\"\n        logger.info(f\"[{request_id}] QwenAgentProvider: Starting chat with Code Interpreter\")\n\n        start_time = time.time()\n        token_count = 0\n        tool_calls = 0\n        code_executed = False\n        last_content = \"\"\n\n        try:\n            assistant = self._create_assistant(system_prompt)\n\n            # Konvertera messages till qwen-agent format\n            qwen_messages = []\n            for msg in messages:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue  # System message hanteras separat\n                qwen_messages.append({\"role\": role, \"content\": content})\n\n            # K√∂r qwen-agent i thread pool (assistant.run() √§r synkron)\n            loop = asyncio.get_event_loop()\n\n            def run_agent():\n                \"\"\"Synkron k√∂rning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n\n            # K√∂r i bakgrunden och streama resultat\n            all_responses = await loop.run_in_executor(_executor, run_agent)\n\n            # Processa alla responses\n            for response_list in all_responses:\n                if not response_list:\n                    continue\n\n                last_msg = response_list[-1]\n\n                # Kolla om det finns tool calls\n                if last_msg.get(\"function_call\"):\n                    tool_calls += 1\n                    code_executed = True\n                    logger.debug(f\"[{request_id}] Tool call detected: {last_msg.get('function_call', {}).get('name', 'unknown')}\")\n\n                # Extrahera content\n                content = last_msg.get(\"content\", \"\")\n                if isinstance(content, str) and content and content != last_content:\n                    # Streama ny content\n                    new_content = content[len(last_content):]\n                    if new_content:\n                        token_count += len(new_content.split())\n                        yield new_content, None\n                    last_content = content\n\n            # Ber√§kna statistik\n            duration_ms = int((time.time() - start_time) * 1000)\n            tokens_per_sec = token_count / (duration_ms / 1000) if duration_ms > 0 else 0\n\n            logger.info(\n                f\"[{request_id}] QwenAgentProvider: Completed - \"\n                f\"tokens={token_count}, tool_calls={tool_calls}, \"\n                f\"duration={duration_ms}ms, speed={tokens_per_sec:.1f} t/s\"\n            )\n\n            # Yield final stats\n            yield \"\", QwenAgentStats(\n                tokens_generated=token_count,\n                tokens_per_second=tokens_per_sec,\n                total_duration_ms=duration_ms,\n                code_executed=code_executed,\n                tool_calls=tool_calls\n            )\n\n        except Exception as e:\n            logger.error(f\"[{request_id}] QwenAgentProvider error: {e}\")\n            # Yield error message\n            yield f\"\\n\\n[ERROR] Code Interpreter failed: {str(e)}\", None\n\n            # Yield stats anyway\n            duration_ms = int((time.time() - start_time) * 1000)\n            yield \"\", QwenAgentStats(\n                tokens_generated=token_count,\n                tokens_per_second=0,\n                total_duration_ms=duration_ms,\n                code_executed=code_executed,\n                tool_calls=tool_calls\n            )\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r Python-kod i isolerad Docker-container", "output": "```python\n    async def execute(self, code: str) -> Dict[str, Any]:\n        \"\"\"\n        K√∂r Python-kod i isolerad Docker-container.\n\n        Args:\n            code: Python-kod att k√∂ra\n\n        Returns:\n            {\"success\": bool, \"output\": str, \"error\": str, \"duration_ms\": int}\n        \"\"\"\n        import tempfile\n        import time\n\n        start_time = time.time()\n\n        try:\n            # Skriv kod till tempor√§r fil\n            with tempfile.NamedTemporaryFile(\n                mode='w',\n                suffix='.py',\n                delete=False\n            ) as f:\n                f.write(code)\n                script_path = f.name\n\n            logger.info(f\"DockerSandbox: Executing code in {self.image}\")\n\n            # K√∂r i Docker med s√§kerhetsbegr√§nsningar\n            proc = await asyncio.wait_for(\n                asyncio.create_subprocess_exec(\n                    \"docker\", \"run\", \"--rm\",\n                    \"--network=none\",              # Ingen internet\n                    f\"--memory={self.memory_limit}\",  # RAM-gr√§ns\n                    f\"--cpus={self.cpu_limit}\",    # CPU-gr√§ns\n                    \"--read-only\",                 # Read-only filesystem\n                    \"--tmpfs=/tmp:size=64m\",       # Tillf√§llig skrivbar /tmp\n                    \"-v\", f\"{script_path}:/sandbox/script.py:ro\",\n                    self.image,\n                    \"python\", \"/sandbox/script.py\",\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE\n                ),\n                timeout=self.timeout\n            )\n\n            stdout, stderr = await proc.communicate()\n            duration_ms = int((time.time() - start_time) * 1000)\n\n            # Rensa temp-fil\n            import os\n            os.unlink(script_path)\n\n            if proc.returncode == 0:\n                logger.info(f\"DockerSandbox: Success ({duration_ms}ms)\")\n                return {\n                    \"success\": True,\n                    \"output\": stdout.decode()[:10000],  # Max 10KB output\n                    \"error\": \"\",\n                    \"duration_ms\": duration_ms\n                }\n            else:\n                logger.warning(f\"DockerSandbox: Failed with code {proc.returncode}\")\n                return {\n                    \"success\": False,\n                    \"output\": stdout.decode()[:5000],\n                    \"error\": stderr.decode()[:5000],\n                    \"duration_ms\": duration_ms\n                }\n\n        except asyncio.TimeoutError:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Timeout after {self.timeout}s\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": f\"Timeout efter {self.timeout} sekunder\",\n                \"duration_ms\": duration_ms\n            }\n        except Exception as e:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Error - {e}\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": str(e),\n                \"duration_ms\": duration_ms\n            }\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Kolla om Docker och sandbox-image finns", "output": "```python\n    async def is_available(self) -> bool:\n        \"\"\"Kolla om Docker och sandbox-image finns\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"docker\", \"image\", \"inspect\", self.image,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            return proc.returncode == 0\n        except Exception:\n            return False\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Synkron k√∂rning av qwen-agent", "output": "```python\n            def run_agent():\n                \"\"\"Synkron k√∂rning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n```\n\nDenna funktion finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Qwen-Agent execution", "output": "```python\nclass QwenAgentStats:\n    \"\"\"Statistics from Qwen-Agent execution\"\"\"\n    tokens_generated: int = 0\n    tokens_per_second: float = 0.0\n    total_duration_ms: int = 0\n    code_executed: bool = False\n    tool_calls: int = 0\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: Provider f√∂r Qwen-Agent med Code Interpreter, MCP och Web Search", "output": "```python\nclass QwenAgentProvider:\n    \"\"\"\n    Provider f√∂r Qwen-Agent med Code Interpreter, MCP och Web Search.\n\n    Anv√§nder Ollama som backend och ger tillg√•ng till:\n    - code_interpreter: K√∂r Python-kod direkt\n    - web_search: S√∂k p√• webben\n    - file tools: L√§s/skriv filer\n    - MCP servers: Externa verktyg (filesystem, etc.)\n    \"\"\"\n\n    def __init__(self):\n        self.llm_cfg = {\n            'model': 'devstral:24b',  # Devstral 24B - Code Interpreter\n            'model_server': 'http://localhost:11434/v1',\n            'api_key': 'ollama',\n            'generate_cfg': {\n                'max_input_tokens': 8192,\n                'max_retries': 3,\n            }\n        }\n\n        # Initialize MCP Manager if available\n        self.mcp_manager = None\n        if MCP_AVAILABLE:\n            try:\n                self.mcp_manager = MCPManager()\n                logger.info(\"MCPManager initialized successfully\")\n            except Exception as e:\n                logger.warning(f\"Failed to initialize MCPManager: {e}\")\n\n        # Build function list:\n        # 1. Code Interpreter (built-in)\n        # 2. Web Search\n        # 3. File Tools (custom)\n        self.function_list = [\n            'code_interpreter',\n            WebSearch(),\n        ] + FILE_TOOLS\n\n        logger.info(f\"QwenAgentProvider initialized with {len(self.function_list)} tools\")\n\n    def _create_assistant(self, system_prompt: str) -> Assistant:\n        \"\"\"Create a fresh Assistant instance\"\"\"\n        return Assistant(\n            function_list=self.function_list,\n            llm=self.llm_cfg,\n            name='QWEN AGENT',\n            description='Elite coder med Code Interpreter',\n            system_message=system_prompt\n        )\n\n    async def chat_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        system_prompt: str,\n        request_id: str\n    ) -> AsyncGenerator[tuple[str, Optional[QwenAgentStats]], None]:\n        \"\"\"\n        Stream response fr√•n Qwen-Agent med Code Interpreter support.\n\n        Args:\n            messages: Lista med {\"role\": \"user/assistant\", \"content\": \"...\"}\n            system_prompt: System message f√∂r agenten\n            request_id: Unik request ID f√∂r logging\n\n        Yields:\n            (token_text, None) f√∂r streaming\n            (\"\", QwenAgentStats) som sista yield med statistik\n        \"\"\"\n        logger.info(f\"[{request_id}] QwenAgentProvider: Starting chat with Code Interpreter\")\n\n        start_time = time.time()\n        token_count = 0\n        tool_calls = 0\n        code_executed = False\n        last_content = \"\"\n\n        try:\n            assistant = self._create_assistant(system_prompt)\n\n            # Konvertera messages till qwen-agent format\n            qwen_messages = []\n            for msg in messages:\n                role = msg.get(\"role\", \"user\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue  # System message hanteras separat\n                qwen_messages.append({\"role\": role, \"content\": content})\n\n            # K√∂r qwen-agent i thread pool (assistant.run() √§r synkron)\n            loop = asyncio.get_event_loop()\n\n            def run_agent():\n                \"\"\"Synkron k√∂rning av qwen-agent\"\"\"\n                results = []\n                for response in assistant.run(messages=qwen_messages):\n                    if response:\n                        results.append(response)\n                return results\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-klass: S√§ker kodexekvering i Docker-container", "output": "```python\nclass DockerSandbox:\n    \"\"\"\n    S√§ker kodexekvering i Docker-container.\n    Isolerar kod fr√•n v√§rdmaskinen med:\n    - Ingen n√§tverks√•tkomst (--network=none)\n    - Begr√§nsad RAM (--memory)\n    - Begr√§nsad CPU (--cpus)\n    - Icke-root anv√§ndare\n    \"\"\"\n\n    def __init__(\n        self,\n        image: str = \"ai-sandbox\",\n        memory_limit: str = \"512m\",\n        cpu_limit: float = 0.5,\n        timeout: int = 30\n    ):\n        self.image = image\n        self.memory_limit = memory_limit\n        self.cpu_limit = cpu_limit\n        self.timeout = timeout\n\n    async def execute(self, code: str) -> Dict[str, Any]:\n        \"\"\"\n        K√∂r Python-kod i isolerad Docker-container.\n\n        Args:\n            code: Python-kod att k√∂ra\n\n        Returns:\n            {\"success\": bool, \"output\": str, \"error\": str, \"duration_ms\": int}\n        \"\"\"\n        import tempfile\n        import time\n\n        start_time = time.time()\n\n        try:\n            # Skriv kod till tempor√§r fil\n            with tempfile.NamedTemporaryFile(\n                mode='w',\n                suffix='.py',\n                delete=False\n            ) as f:\n                f.write(code)\n                script_path = f.name\n\n            logger.info(f\"DockerSandbox: Executing code in {self.image}\")\n\n            # K√∂r i Docker med s√§kerhetsbegr√§nsningar\n            proc = await asyncio.wait_for(\n                asyncio.create_subprocess_exec(\n                    \"docker\", \"run\", \"--rm\",\n                    \"--network=none\",              # Ingen internet\n                    f\"--memory={self.memory_limit}\",  # RAM-gr√§ns\n                    f\"--cpus={self.cpu_limit}\",    # CPU-gr√§ns\n                    \"--read-only\",                 # Read-only filesystem\n                    \"--tmpfs=/tmp:size=64m\",       # Tillf√§llig skrivbar /tmp\n                    \"-v\", f\"{script_path}:/sandbox/script.py:ro\",\n                    self.image,\n                    \"python\", \"/sandbox/script.py\",\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE\n                ),\n                timeout=self.timeout\n            )\n\n            stdout, stderr = await proc.communicate()\n            duration_ms = int((time.time() - start_time) * 1000)\n\n            # Rensa temp-fil\n            import os\n            os.unlink(script_path)\n\n            if proc.returncode == 0:\n                logger.info(f\"DockerSandbox: Success ({duration_ms}ms)\")\n                return {\n                    \"success\": True,\n                    \"output\": stdout.decode()[:10000],  # Max 10KB output\n                    \"error\": \"\",\n                    \"duration_ms\": duration_ms\n                }\n            else:\n                logger.warning(f\"DockerSandbox: Failed with code {proc.returncode}\")\n                return {\n                    \"success\": False,\n                    \"output\": stdout.decode()[:5000],\n                    \"error\": stderr.decode()[:5000],\n                    \"duration_ms\": duration_ms\n                }\n\n        except asyncio.TimeoutError:\n            duration_ms = int((time.time() - start_time) * 1000)\n            logger.error(f\"DockerSandbox: Timeout after {self.timeout}s\")\n            return {\n                \"success\": False,\n                \"output\": \"\",\n                \"error\": f\"Timeout efter {self.timeout} sekunder\",\n                \"duration_ms\": duration_ms\n            }\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/qwen_agent_provider.py`."}
{"instruction": "Implementera en Python-funktion: Check if nvidia-smi is available", "output": "```python\n    async def _check_nvidia_smi(self) -> bool:\n        \"\"\"Check if nvidia-smi is available\"\"\"\n        if self._nvidia_smi_available is not None:\n            return self._nvidia_smi_available\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--version\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            self._nvidia_smi_available = proc.returncode == 0\n        except FileNotFoundError:\n            self._nvidia_smi_available = False\n        except Exception as e:\n            logger.warning(f\"nvidia-smi check failed: {e}\")\n            self._nvidia_smi_available = False\n\n        return self._nvidia_smi_available\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Run nvidia-smi with arguments and return stdout", "output": "```python\n    async def _run_nvidia_smi(self, args: list[str]) -> Optional[str]:\n        \"\"\"Run nvidia-smi with arguments and return stdout\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", *args,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            stdout, stderr = await proc.communicate()\n\n            if proc.returncode == 0:\n                return stdout.decode(\"utf-8\").strip()\n            else:\n                logger.warning(f\"nvidia-smi failed: {stderr.decode()}\")\n                return None\n\n        except Exception as e:\n            logger.error(f\"Error running nvidia-smi: {e}\")\n            return None\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Fetch current GPU stats from nvidia-smi", "output": "```python\n    async def _fetch_stats(self) -> GPUStats:\n        \"\"\"Fetch current GPU stats from nvidia-smi\"\"\"\n        # Query format: index,name,temp,util,mem_util,mem_total,mem_used,mem_free,power_draw,power_limit,fan\n        query = (\n            \"--query-gpu=\"\n            \"index,name,temperature.gpu,utilization.gpu,utilization.memory,\"\n            \"memory.total,memory.used,memory.free,\"\n            \"power.draw,power.limit,fan.speed,\"\n            \"driver_version\"\n        )\n\n        output = await self._run_nvidia_smi([query, \"--format=csv,noheader,nounits\"])\n\n        if not output:\n            return self._get_fallback_stats()\n\n        try:\n            # Parse CSV output\n            parts = [p.strip() for p in output.split(\",\")]\n\n            if len(parts) < 11:\n                logger.warning(f\"Unexpected nvidia-smi output: {output}\")\n                return self._get_fallback_stats()\n\n            # Parse values with error handling\n            def safe_float(val: str, default: float = 0.0) -> float:\n                try:\n                    return float(val)\n                except (ValueError, TypeError):\n                    return default\n\n            def safe_int(val: str, default: int = 0) -> int:\n                try:\n                    return int(float(val))\n                except (ValueError, TypeError):\n                    return default\n\n            vram_total = safe_float(parts[5]) / 1024  # MiB to GB\n            vram_used = safe_float(parts[6]) / 1024\n            vram_free = safe_float(parts[7]) / 1024\n\n            stats = GPUStats(\n                name=parts[1].strip(),\n                vram_total_gb=round(vram_total, 2),\n                vram_used_gb=round(vram_used, 2),\n                vram_free_gb=round(vram_free, 2),\n                vram_percent=round((vram_used / vram_total) * 100, 1) if vram_total > 0 else 0,\n                temperature_c=safe_int(parts[2]),\n                gpu_util_percent=safe_int(parts[3]),\n                memory_util_percent=safe_int(parts[4]),\n                power_draw_w=safe_int(parts[8]),\n                power_limit_w=safe_int(parts[9]),\n                fan_speed_percent=safe_int(parts[10]) if parts[10] != \"[N/A]\" else None,\n                driver_version=parts[11].strip() if len(parts) > 11 else None,\n                is_available=True\n            )\n\n            return stats\n\n        except Exception as e:\n            logger.error(f\"Failed to parse nvidia-smi output: {e}\")\n            return self._get_fallback_stats()\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Return fallback stats when GPU monitoring unavailable", "output": "```python\n    def _get_fallback_stats(self) -> GPUStats:\n        \"\"\"Return fallback stats when GPU monitoring unavailable\"\"\"\n        return GPUStats(\n            name=\"NVIDIA GeForce RTX 4070\",\n            vram_total_gb=12.0,\n            vram_used_gb=0.0,\n            vram_free_gb=12.0,\n            vram_percent=0.0,\n            temperature_c=0,\n            power_draw_w=0,\n            power_limit_w=200,\n            gpu_util_percent=0,\n            memory_util_percent=0,\n            is_available=False\n        )\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Get CUDA version from nvidia-smi", "output": "```python\n    async def get_cuda_version(self) -> Optional[str]:\n        \"\"\"Get CUDA version from nvidia-smi\"\"\"\n        output = await self._run_nvidia_smi([])\n\n        if output:\n            # Parse CUDA version from standard nvidia-smi output\n            match = re.search(r\"CUDA Version:\\s*(\\d+\\.\\d+)\", output)\n            if match:\n                return match.group(1)\n\n        return None\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Check if GPU is available and accessible", "output": "```python\n    async def is_gpu_available(self) -> bool:\n        \"\"\"Check if GPU is available and accessible\"\"\"\n        return await self._check_nvidia_smi()\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Quick method to get just VRAM usage", "output": "```python\n    async def get_vram_usage_gb(self) -> float:\n        \"\"\"Quick method to get just VRAM usage\"\"\"\n        stats = await self.get_stats()\n        return stats.vram_used_gb\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Check if there's enough free VRAM for a model", "output": "```python\n    async def has_sufficient_vram(self, required_gb: float) -> bool:\n        \"\"\"Check if there's enough free VRAM for a model\"\"\"\n        stats = await self.get_stats()\n        return stats.vram_free_gb >= required_gb\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Skapa en Python-funktion som safe float", "output": "```python\n            def safe_float(val: str, default: float = 0.0) -> float:\n                try:\n                    return float(val)\n                except (ValueError, TypeError):\n                    return default\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Skapa en Python-funktion som safe int", "output": "```python\n            def safe_int(val: str, default: int = 0) -> int:\n                try:\n                    return int(float(val))\n                except (ValueError, TypeError):\n                    return default\n```\n\nDenna funktion finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-klass: Cached GPU statistics with timestamp", "output": "```python\nclass CachedStats:\n    \"\"\"Cached GPU statistics with timestamp\"\"\"\n    stats: GPUStats\n    timestamp: datetime\n```\n\nKlassen finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-klass: Monitors NVIDIA GPU statistics using nvidia-smi", "output": "```python\nclass GPUMonitor:\n    \"\"\"\n    Monitors NVIDIA GPU statistics using nvidia-smi.\n\n    Features:\n    - Async subprocess calls to nvidia-smi\n    - Caching to avoid excessive calls\n    - Fallback values when GPU unavailable\n    \"\"\"\n\n    def __init__(self):\n        self._cache: Optional[CachedStats] = None\n        self._lock = asyncio.Lock()\n        self._nvidia_smi_available: Optional[bool] = None\n\n    async def _check_nvidia_smi(self) -> bool:\n        \"\"\"Check if nvidia-smi is available\"\"\"\n        if self._nvidia_smi_available is not None:\n            return self._nvidia_smi_available\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--version\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            await proc.communicate()\n            self._nvidia_smi_available = proc.returncode == 0\n        except FileNotFoundError:\n            self._nvidia_smi_available = False\n        except Exception as e:\n            logger.warning(f\"nvidia-smi check failed: {e}\")\n            self._nvidia_smi_available = False\n\n        return self._nvidia_smi_available\n\n    async def _run_nvidia_smi(self, args: list[str]) -> Optional[str]:\n        \"\"\"Run nvidia-smi with arguments and return stdout\"\"\"\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", *args,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            stdout, stderr = await proc.communicate()\n\n            if proc.returncode == 0:\n                return stdout.decode(\"utf-8\").strip()\n            else:\n                logger.warning(f\"nvidia-smi failed: {stderr.decode()}\")\n                return None\n\n        except Exception as e:\n            logger.error(f\"Error running nvidia-smi: {e}\")\n            return None\n\n    async def get_stats(self, force_refresh: bool = False) -> GPUStats:\n        \"\"\"\n        Get current GPU statistics.\n\n        Args:\n            force_refresh: Bypass cache and fetch fresh stats\n\n        Returns:\n            GPUStats with current GPU information\n        \"\"\"\n        async with self._lock:\n            # Check cache validity\n            if not force_refresh and self._cache:\n                age = (datetime.utcnow() - self._cache.timestamp).total_seconds()\n                if age < CACHE_DURATION_SECONDS:\n                    return self._cache.stats\n\n            # Check if nvidia-smi is available\n            if not await self._check_nvidia_smi():\n                return self._get_fallback_stats()\n\n            # Fetch fresh stats\n            stats = await self._fetch_stats()\n\n            # Update cache\n            self._cache = CachedStats(\n                stats=stats,\n                timestamp=datetime.utcnow()\n            )\n\n            return stats\n\n    async def _fetch_stats(self) -> GPUStats:\n        \"\"\"Fetch current GPU stats from nvidia-smi\"\"\"\n        # Query format: index,name,temp,util,mem_util,mem_total,mem_used,mem_free,power_draw,power_limit,fan\n        query = (\n            \"--query-gpu=\"\n            \"index,name,temperature.gpu,utilization.gpu,utilization.memory,\"\n            \"memory.total,memory.used,memory.free,\"\n            \"power.draw,power.limit,fan.speed,\"\n            \"driver_version\"\n        )\n\n        output = await self._run_nvidia_smi([query, \"--format=csv,noheader,nounits\"])\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/gpu_monitor.py`."}
{"instruction": "Implementera en Python-funktion: Returnera verktygs-definitioner f√∂r system prompt", "output": "```python\ndef get_tool_definitions() -> dict[str, dict]:\n    \"\"\"Returnera verktygs-definitioner f√∂r system prompt\"\"\"\n    return {\n        name: {\n            \"description\": tool.description,\n            \"safety\": tool.safety.value,\n            \"params\": tool.params\n        }\n        for name, tool in TOOLS.items()\n    }\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r ett verktyg och returnera resultat", "output": "```python\n    async def execute(self, tool_name: str, params: dict) -> ToolResult:\n        \"\"\"K√∂r ett verktyg och returnera resultat\"\"\"\n        import time\n        start_time = time.time()\n\n        if tool_name not in TOOLS:\n            return ToolResult(\n                success=False,\n                error=f\"Ok√§nt verktyg: {tool_name}\",\n                tool_name=tool_name\n            )\n\n        tool = TOOLS[tool_name]\n\n        # Validera obligatoriska parametrar\n        missing = [p for p in tool.params if p not in params and p != \"lines\"]\n        if missing:\n            return ToolResult(\n                success=False,\n                error=f\"Saknar obligatoriska parametrar: {', '.join(missing)}\",\n                tool_name=tool_name\n            )\n\n        # Dispatch till r√§tt metod\n        method = getattr(self, f\"_exec_{tool_name}\", None)\n        if not method:\n            return ToolResult(\n                success=False,\n                error=f\"Verktyget {tool_name} √§r inte implementerat\",\n                tool_name=tool_name\n            )\n\n        try:\n            result = await asyncio.wait_for(\n                method(params),\n                timeout=self.timeout\n            )\n            result.tool_name = tool_name\n            result.duration_ms = int((time.time() - start_time) * 1000)\n            logger.info(f\"Tool executed: {tool_name} (success={result.success}, {result.duration_ms}ms)\")\n            return result\n\n        except asyncio.TimeoutError:\n            logger.error(f\"Tool timeout: {tool_name}\")\n            return ToolResult(\n                success=False,\n                error=f\"Timeout efter {self.timeout} sekunder\",\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n        except Exception as e:\n            logger.error(f\"Tool error: {tool_name} - {e}\")\n            return ToolResult(\n                success=False,\n                error=str(e),\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista alla Docker containers", "output": "```python\n    async def _exec_docker_ps(self, params: dict) -> ToolResult:\n        \"\"\"Lista alla Docker containers\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"ps\", \"-a\", \"--format\",\n            \"table {{.Names}}\\t{{.Status}}\\t{{.Image}}\\t{{.Ports}}\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa container-loggar", "output": "```python\n    async def _exec_docker_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa container-loggar\"\"\"\n        container = params[\"container\"]\n        lines = params.get(\"lines\", 50)\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"logs\", \"--tail\", str(lines), container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        # Docker skriver ofta loggar till stderr\n        output = stdout.decode() + stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta om en container", "output": "```python\n    async def _exec_docker_restart(self, params: dict) -> ToolResult:\n        \"\"\"Starta om en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"restart\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har startats om\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Stoppa en container", "output": "```python\n    async def _exec_docker_stop(self, params: dict) -> ToolResult:\n        \"\"\"Stoppa en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"stop\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har stoppats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta en container", "output": "```python\n    async def _exec_docker_start(self, params: dict) -> ToolResult:\n        \"\"\"Starta en container\"\"\"\n        container = params[\"container\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"start\", container,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Container '{container}' har startats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa status f√∂r en systemd-tj√§nst", "output": "```python\n    async def _exec_service_status(self, params: dict) -> ToolResult:\n        \"\"\"Visa status f√∂r en systemd-tj√§nst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"systemctl\", \"status\", service, \"--no-pager\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        # systemctl status returnerar non-zero f√∂r inaktiva tj√§nster\n        output = stdout.decode() or stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa journalctl-loggar f√∂r en tj√§nst", "output": "```python\n    async def _exec_service_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa journalctl-loggar f√∂r en tj√§nst\"\"\"\n        service = params[\"service\"]\n        lines = params.get(\"lines\", 50)\n\n        proc = await asyncio.create_subprocess_exec(\n            \"journalctl\", \"-u\", service, \"-n\", str(lines), \"--no-pager\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode() or stderr.decode()\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta om en systemd-tj√§nst", "output": "```python\n    async def _exec_service_restart(self, params: dict) -> ToolResult:\n        \"\"\"Starta om en systemd-tj√§nst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"restart\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tj√§nsten '{service}' har startats om\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Stoppa en systemd-tj√§nst", "output": "```python\n    async def _exec_service_stop(self, params: dict) -> ToolResult:\n        \"\"\"Stoppa en systemd-tj√§nst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"stop\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tj√§nsten '{service}' har stoppats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Starta en systemd-tj√§nst", "output": "```python\n    async def _exec_service_start(self, params: dict) -> ToolResult:\n        \"\"\"Starta en systemd-tj√§nst\"\"\"\n        service = params[\"service\"]\n\n        proc = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"start\", service,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=f\"Tj√§nsten '{service}' har startats\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kontrollera att s√∂kv√§g √§r till√•ten", "output": "```python\n    def _is_allowed_path(self, path: str) -> bool:\n        \"\"\"Kontrollera att s√∂kv√§g √§r till√•ten\"\"\"\n        try:\n            resolved = Path(path).resolve()\n            return any(str(resolved).startswith(p) for p in self.allowed_paths)\n        except Exception:\n            return False\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: L√§s inneh√•llet i en fil", "output": "```python\n    async def _exec_file_read(self, params: dict) -> ToolResult:\n        \"\"\"L√§s inneh√•llet i en fil\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otill√•ten s√∂kv√§g: {path}. Till√•tna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            file_path = Path(path)\n            if not file_path.exists():\n                return ToolResult(success=False, error=f\"Filen finns inte: {path}\")\n\n            if file_path.is_dir():\n                return ToolResult(success=False, error=f\"S√∂kv√§gen √§r en katalog, anv√§nd file_list ist√§llet\")\n\n            content = file_path.read_text(encoding='utf-8', errors='replace')\n            return ToolResult(success=True, output=content[:self.max_output_size])\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen l√§sbeh√∂righet f√∂r: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Skriv inneh√•ll till en fil", "output": "```python\n    async def _exec_file_write(self, params: dict) -> ToolResult:\n        \"\"\"Skriv inneh√•ll till en fil\"\"\"\n        path = params[\"path\"]\n        content = params[\"content\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otill√•ten s√∂kv√§g: {path}. Till√•tna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            file_path = Path(path)\n\n            # Skapa parent-kataloger om de inte finns\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n\n            file_path.write_text(content, encoding='utf-8')\n            return ToolResult(success=True, output=f\"Skrev {len(content)} tecken till {path}\")\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen skrivbeh√∂righet f√∂r: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista filer i en katalog", "output": "```python\n    async def _exec_file_list(self, params: dict) -> ToolResult:\n        \"\"\"Lista filer i en katalog\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(\n                success=False,\n                error=f\"Otill√•ten s√∂kv√§g: {path}. Till√•tna: {', '.join(self.allowed_paths)}\"\n            )\n\n        try:\n            dir_path = Path(path)\n            if not dir_path.exists():\n                return ToolResult(success=False, error=f\"Katalogen finns inte: {path}\")\n\n            if not dir_path.is_dir():\n                return ToolResult(success=False, error=f\"S√∂kv√§gen √§r inte en katalog: {path}\")\n\n            entries = []\n            for entry in sorted(dir_path.iterdir()):\n                entry_type = \"D\" if entry.is_dir() else \"F\"\n                size = entry.stat().st_size if entry.is_file() else 0\n                entries.append(f\"[{entry_type}] {entry.name} ({size} bytes)\")\n\n            output = f\"Inneh√•ll i {path}:\\n\" + \"\\n\".join(entries[:100])  # Max 100 entries\n            return ToolResult(success=True, output=output)\n\n        except PermissionError:\n            return ToolResult(success=False, error=f\"Ingen l√§sbeh√∂righet f√∂r: {path}\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Kontrollera om en fil finns", "output": "```python\n    async def _exec_file_exists(self, params: dict) -> ToolResult:\n        \"\"\"Kontrollera om en fil finns\"\"\"\n        path = params[\"path\"]\n\n        try:\n            file_path = Path(path)\n            exists = file_path.exists()\n            file_type = \"katalog\" if file_path.is_dir() else \"fil\" if file_path.is_file() else \"ok√§nd\"\n\n            if exists:\n                return ToolResult(success=True, output=f\"Ja, {path} finns ({file_type})\")\n            else:\n                return ToolResult(success=True, output=f\"Nej, {path} finns inte\")\n\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa system-statistik", "output": "```python\n    async def _exec_system_stats(self, params: dict) -> ToolResult:\n        \"\"\"Visa system-statistik\"\"\"\n        try:\n            import psutil\n\n            # CPU\n            cpu_percent = psutil.cpu_percent(interval=0.5)\n            cpu_count = psutil.cpu_count()\n\n            # RAM\n            mem = psutil.virtual_memory()\n            ram_used = mem.used / (1024**3)\n            ram_total = mem.total / (1024**3)\n            ram_percent = mem.percent\n\n            # Disk\n            disk = psutil.disk_usage('/')\n            disk_used = disk.used / (1024**3)\n            disk_total = disk.total / (1024**3)\n            disk_percent = disk.percent\n\n            output = f\"\"\"SYSTEM STATS\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nCPU:  {cpu_percent:.1f}% ({cpu_count} cores)\nRAM:  {ram_used:.1f} / {ram_total:.1f} GB ({ram_percent:.1f}%)\nDisk: {disk_used:.1f} / {disk_total:.1f} GB ({disk_percent:.1f}%)\n\"\"\"\n\n            # GPU (nvidia-smi)\n            gpu_proc = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,temperature.gpu,utilization.gpu\",\n                \"--format=csv,noheader,nounits\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            gpu_stdout, _ = await gpu_proc.communicate()\n\n            if gpu_proc.returncode == 0:\n                gpu_data = gpu_stdout.decode().strip().split(\", \")\n                if len(gpu_data) >= 5:\n                    output += f\"\"\"\nGPU:  {gpu_data[0]}\nVRAM: {gpu_data[1]} / {gpu_data[2]} MB\nTemp: {gpu_data[3]}¬∞C\nUtil: {gpu_data[4]}%\n\"\"\"\n\n            return ToolResult(success=True, output=output)\n\n        except ImportError:\n            return ToolResult(success=False, error=\"psutil √§r inte installerat\")\n        except Exception as e:\n            return ToolResult(success=False, error=str(e))\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista installerade Ollama-modeller", "output": "```python\n    async def _exec_ollama_list(self, params: dict) -> ToolResult:\n        \"\"\"Lista installerade Ollama-modeller\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"ollama\", \"list\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa k√∂rande/laddade Ollama-modeller", "output": "```python\n    async def _exec_ollama_ps(self, params: dict) -> ToolResult:\n        \"\"\"Visa k√∂rande/laddade Ollama-modeller\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"ollama\", \"ps\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        output = stdout.decode()\n        if not output.strip() or \"NAME\" not in output:\n            output = \"Inga modeller √§r laddade just nu\"\n\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r ett godtyckligt shell-kommando", "output": "```python\n    async def _exec_shell(self, params: dict) -> ToolResult:\n        \"\"\"K√∂r ett godtyckligt shell-kommando\"\"\"\n        command = params[\"command\"]\n\n        # S√§kerhetskontroll - blockera extremt farliga kommandon\n        blocked_patterns = [\n            \"rm -rf /\",\n            \"rm -rf /*\",\n            \"> /dev/sd\",\n            \"mkfs\",\n            \":(){ :|:& };:\",  # Fork bomb\n            \"dd if=\",\n        ]\n\n        for pattern in blocked_patterns:\n            if pattern in command:\n                return ToolResult(\n                    success=False,\n                    error=f\"Kommandot √§r blockerat av s√§kerhetssk√§l: {pattern}\"\n                )\n\n        proc = await asyncio.create_subprocess_shell(\n            command,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            cwd=\"/home/ai-server\"\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else f\"Exit code: {proc.returncode}\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa git status", "output": "```python\n    async def _exec_git_status(self, params: dict) -> ToolResult:\n        \"\"\"Visa git status\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"status\", \"--short\", \"--branch\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa git diff", "output": "```python\n    async def _exec_git_diff(self, params: dict) -> ToolResult:\n        \"\"\"Visa git diff\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"diff\", \"--stat\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        output = stdout.decode()\n        if not output.strip():\n            output = \"Inga √§ndringar (clean working tree)\"\n\n        return ToolResult(success=True, output=output[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Visa senaste commits", "output": "```python\n    async def _exec_git_log(self, params: dict) -> ToolResult:\n        \"\"\"Visa senaste commits\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"log\", \"--oneline\", \"-n\", \"10\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: Lista git branches", "output": "```python\n    async def _exec_git_branch(self, params: dict) -> ToolResult:\n        \"\"\"Lista git branches\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"git\", \"branch\", \"-a\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r pytest i en katalog", "output": "```python\n    async def _exec_pytest_run(self, params: dict) -> ToolResult:\n        \"\"\"K√∂r pytest i en katalog\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"python\", \"-m\", \"pytest\", \"-v\", \"--tb=short\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else \"Tester misslyckades\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r pytest med coverage", "output": "```python\n    async def _exec_pytest_coverage(self, params: dict) -> ToolResult:\n        \"\"\"K√∂r pytest med coverage\"\"\"\n        path = params[\"path\"]\n\n        if not self._is_allowed_path(path):\n            return ToolResult(success=False, error=f\"Otill√•ten s√∂kv√§g: {path}\")\n\n        proc = await asyncio.create_subprocess_exec(\n            \"python\", \"-m\", \"pytest\", \"--cov=.\", \"--cov-report=term-missing\",\n            cwd=path,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        output = stdout.decode()\n        if stderr:\n            output += f\"\\n[STDERR]\\n{stderr.decode()}\"\n\n        return ToolResult(\n            success=proc.returncode == 0,\n            output=output[:self.max_output_size],\n            error=\"\" if proc.returncode == 0 else \"Coverage-k√∂rning misslyckades\"\n        )\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-funktion: S√∂k p√• webben via DuckDuckGo", "output": "```python\n    async def _exec_web_search(self, params: dict) -> ToolResult:\n        \"\"\"S√∂k p√• webben via DuckDuckGo\"\"\"\n        query = params[\"query\"]\n        max_results = params.get(\"max_results\", 5)\n\n        try:\n            from duckduckgo_search import DDGS\n\n            results = []\n            with DDGS() as ddgs:\n                for r in ddgs.text(query, max_results=max_results):\n                    results.append(f\"**{r['title']}**\\n{r['href']}\\n{r['body']}\\n\")\n\n            if not results:\n                return ToolResult(success=True, output=f\"Inga resultat f√∂r: {query}\")\n\n            output = f\"S√∂kresultat f√∂r '{query}':\\n\\n\" + \"\\n---\\n\".join(results)\n            return ToolResult(success=True, output=output[:self.max_output_size])\n\n        except ImportError:\n            return ToolResult(\n                success=False,\n                error=\"duckduckgo-search √§r inte installerat. K√∂r: pip install duckduckgo-search\"\n            )\n        except Exception as e:\n            return ToolResult(success=False, error=f\"S√∂kfel: {str(e)}\")\n```\n\nDenna funktion finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: S√§kerhetsniv√• f√∂r verktyg", "output": "```python\nclass SafetyLevel(Enum):\n    \"\"\"S√§kerhetsniv√• f√∂r verktyg\"\"\"\n    SAFE = \"safe\"           # K√∂rs direkt\n    DANGEROUS = \"dangerous\" # Kr√§ver bekr√§ftelse\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Definition av ett verktyg", "output": "```python\nclass ToolDefinition:\n    \"\"\"Definition av ett verktyg\"\"\"\n    name: str\n    description: str\n    safety: SafetyLevel\n    params: list[str] = field(default_factory=list)\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Resultat fr√•n verktygsexekvering", "output": "```python\nclass ToolResult:\n    \"\"\"Resultat fr√•n verktygsexekvering\"\"\"\n    success: bool\n    output: str = \"\"\n    error: str = \"\"\n    tool_name: str = \"\"\n    duration_ms: int = 0\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"output\": self.output,\n            \"error\": self.error,\n            \"tool_name\": self.tool_name,\n            \"duration_ms\": self.duration_ms,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n        }\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Executor f√∂r QWEN SysAdmin-verktyg", "output": "```python\nclass SystemTools:\n    \"\"\"Executor f√∂r QWEN SysAdmin-verktyg\"\"\"\n\n    def __init__(self):\n        # Till√•tna s√∂kv√§gar f√∂r filoperationer\n        self.allowed_paths = [\n            \"/home/ai-server\",\n            \"/var/log\",\n            \"/etc/nginx\",\n            \"/tmp\",\n            \"/opt\"\n        ]\n        # Max output storlek (bytes)\n        self.max_output_size = 10240  # 10KB\n        # Timeout f√∂r kommandon (sekunder)\n        self.timeout = 60\n\n    async def execute(self, tool_name: str, params: dict) -> ToolResult:\n        \"\"\"K√∂r ett verktyg och returnera resultat\"\"\"\n        import time\n        start_time = time.time()\n\n        if tool_name not in TOOLS:\n            return ToolResult(\n                success=False,\n                error=f\"Ok√§nt verktyg: {tool_name}\",\n                tool_name=tool_name\n            )\n\n        tool = TOOLS[tool_name]\n\n        # Validera obligatoriska parametrar\n        missing = [p for p in tool.params if p not in params and p != \"lines\"]\n        if missing:\n            return ToolResult(\n                success=False,\n                error=f\"Saknar obligatoriska parametrar: {', '.join(missing)}\",\n                tool_name=tool_name\n            )\n\n        # Dispatch till r√§tt metod\n        method = getattr(self, f\"_exec_{tool_name}\", None)\n        if not method:\n            return ToolResult(\n                success=False,\n                error=f\"Verktyget {tool_name} √§r inte implementerat\",\n                tool_name=tool_name\n            )\n\n        try:\n            result = await asyncio.wait_for(\n                method(params),\n                timeout=self.timeout\n            )\n            result.tool_name = tool_name\n            result.duration_ms = int((time.time() - start_time) * 1000)\n            logger.info(f\"Tool executed: {tool_name} (success={result.success}, {result.duration_ms}ms)\")\n            return result\n\n        except asyncio.TimeoutError:\n            logger.error(f\"Tool timeout: {tool_name}\")\n            return ToolResult(\n                success=False,\n                error=f\"Timeout efter {self.timeout} sekunder\",\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n        except Exception as e:\n            logger.error(f\"Tool error: {tool_name} - {e}\")\n            return ToolResult(\n                success=False,\n                error=str(e),\n                tool_name=tool_name,\n                duration_ms=int((time.time() - start_time) * 1000)\n            )\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # DOCKER VERKTYG\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n    async def _exec_docker_ps(self, params: dict) -> ToolResult:\n        \"\"\"Lista alla Docker containers\"\"\"\n        proc = await asyncio.create_subprocess_exec(\n            \"docker\", \"ps\", \"-a\", \"--format\",\n            \"table {{.Names}}\\t{{.Status}}\\t{{.Image}}\\t{{.Ports}}\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n        stdout, stderr = await proc.communicate()\n\n        if proc.returncode != 0:\n            return ToolResult(success=False, error=stderr.decode()[:500])\n\n        return ToolResult(success=True, output=stdout.decode()[:self.max_output_size])\n\n    async def _exec_docker_logs(self, params: dict) -> ToolResult:\n        \"\"\"Visa container-loggar\"\"\"\n        container = params[\"container\"]\n        lines = params.get(\"lines\", 50)\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/system_tools.py`."}
{"instruction": "Implementera en Python-klass: Result from a deploy operation", "output": "```python\nclass DeployResult:\n    \"\"\"Result from a deploy operation\"\"\"\n    success: bool\n    operation: str\n    output: str\n    error: Optional[str] = None\n    duration_ms: int = 0\n    timestamp: str = \"\"\n\n    def to_dict(self) -> dict:\n        return {\n            \"success\": self.success,\n            \"operation\": self.operation,\n            \"output\": self.output,\n            \"error\": self.error,\n            \"duration_ms\": self.duration_ms,\n            \"timestamp\": self.timestamp,\n        }\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-klass: Progress update during deployment", "output": "```python\nclass DeployProgress:\n    \"\"\"Progress update during deployment\"\"\"\n    step: str\n    status: str  # \"pending\", \"running\", \"complete\", \"failed\"\n    message: str\n    output: Optional[str] = None\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-klass: Manages deployment operations:\n- Git pull\n- Service restart\n- Full deploy (pull + restart)", "output": "```python\nclass DeployManager:\n    \"\"\"\n    Manages deployment operations:\n    - Git pull\n    - Service restart\n    - Full deploy (pull + restart)\n    \"\"\"\n\n    def __init__(self, project_root: Path = PROJECT_ROOT):\n        self.project_root = project_root\n\n    async def git_pull(self) -> DeployResult:\n        \"\"\"\n        Pull latest changes from git.\n\n        Uses --ff-only to prevent merge conflicts.\n        \"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n        start_time = asyncio.get_event_loop().time()\n\n        logger.info(f\"Running git pull in {self.project_root}\")\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"git\", \"-C\", str(self.project_root), \"pull\", \"--ff-only\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    proc.communicate(),\n                    timeout=GIT_TIMEOUT\n                )\n            except asyncio.TimeoutError:\n                proc.kill()\n                await proc.wait()\n                return DeployResult(\n                    success=False,\n                    operation=\"git_pull\",\n                    output=\"\",\n                    error=f\"Git pull timed out after {GIT_TIMEOUT}s\",\n                    duration_ms=int((asyncio.get_event_loop().time() - start_time) * 1000),\n                    timestamp=timestamp,\n                )\n\n            duration_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)\n\n            stdout_str = stdout.decode(\"utf-8\", errors=\"replace\").strip()\n            stderr_str = stderr.decode(\"utf-8\", errors=\"replace\").strip()\n\n            if proc.returncode == 0:\n                logger.info(f\"Git pull successful: {stdout_str}\")\n                return DeployResult(\n                    success=True,\n                    operation=\"git_pull\",\n                    output=stdout_str or \"Already up to date.\",\n                    duration_ms=duration_ms,\n                    timestamp=timestamp,\n                )\n            else:\n                logger.error(f\"Git pull failed: {stderr_str}\")\n                return DeployResult(\n                    success=False,\n                    operation=\"git_pull\",\n                    output=stdout_str,\n                    error=stderr_str or \"Git pull failed\",\n                    duration_ms=duration_ms,\n                    timestamp=timestamp,\n                )\n\n        except Exception as e:\n            logger.error(f\"Git pull error: {e}\")\n            return DeployResult(\n                success=False,\n                operation=\"git_pull\",\n                output=\"\",\n                error=str(e),\n                duration_ms=int((asyncio.get_event_loop().time() - start_time) * 1000),\n                timestamp=timestamp,\n            )\n\n    async def git_status(self) -> DeployResult:\n        \"\"\"Get current git status\"\"\"\n        timestamp = datetime.utcnow().isoformat() + \"Z\"\n        start_time = asyncio.get_event_loop().time()\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"git\", \"-C\", str(self.project_root), \"status\", \"--short\",\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n\n            stdout, stderr = await asyncio.wait_for(\n                proc.communicate(),\n                timeout=10\n            )\n\n            duration_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/deploy_manager.py`."}
{"instruction": "Implementera en Python-funktion: Warmup the QWEN-SONNET model using the existing ollama_client", "output": "```python\nasync def warmup_model() -> Dict[str, Any]:\n    \"\"\"\n    Warmup the QWEN-SONNET model using the existing ollama_client.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Starting model warmup for qwen-sonnet\")\n\n        # Get the QWEN-SONNET profile\n        profile = get_profile(ProfileId.QWEN_SONNET)\n\n        # Use the ollama_client warmup function\n        success = await ollama_client.warmup_model(profile)\n\n        if success:\n            logger.info(\"Model warmup completed successfully\")\n            return {\n                \"success\": True,\n                \"message\": f\"Model {profile.model} warmed up successfully\",\n                \"model\": profile.model\n            }\n        else:\n            logger.error(\"Model warmup failed\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to warmup model {profile.model}\",\n                \"model\": profile.model\n            }\n\n    except Exception as e:\n        logger.error(f\"Error during model warmup: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error during warmup: {str(e)}\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Restart the simons-ai-backend systemd service", "output": "```python\nasync def restart_backend() -> Dict[str, Any]:\n    \"\"\"\n    Restart the simons-ai-backend systemd service.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Attempting to restart simons-ai-backend service\")\n\n        # Run systemctl restart command\n        process = await asyncio.create_subprocess_exec(\n            \"sudo\", \"systemctl\", \"restart\", \"simons-ai-backend\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n\n        stdout, stderr = await process.communicate()\n\n        if process.returncode == 0:\n            logger.info(\"Backend service restarted successfully\")\n            return {\n                \"success\": True,\n                \"message\": \"Backend service restarted successfully\",\n                \"service\": \"simons-ai-backend\"\n            }\n        else:\n            error_msg = stderr.decode().strip() if stderr else \"Unknown error\"\n            logger.error(f\"Failed to restart backend service: {error_msg}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to restart backend: {error_msg}\",\n                \"service\": \"simons-ai-backend\",\n                \"error\": error_msg\n            }\n\n    except Exception as e:\n        logger.error(f\"Error restarting backend service: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error restarting backend: {str(e)}\",\n            \"service\": \"simons-ai-backend\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Cast dashboard to Nest Hub \"Sovis\" using catt", "output": "```python\nasync def recast_dashboard() -> Dict[str, Any]:\n    \"\"\"\n    Cast dashboard to Nest Hub \"Sovis\" using catt.\n\n    Returns:\n        Dict with success status and message\n    \"\"\"\n    try:\n        logger.info(\"Casting dashboard to Nest Hub Sovis\")\n\n        process = await asyncio.create_subprocess_exec(\n            \"/home/ai-server/catt-venv/bin/catt\",\n            \"-d\", \"Sovis\",\n            \"cast_site\", \"http://192.168.86.32:5173/kiosk\",\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE\n        )\n\n        stdout, stderr = await process.communicate()\n\n        if process.returncode == 0:\n            logger.info(\"Dashboard cast to Sovis successfully\")\n            return {\n                \"success\": True,\n                \"message\": \"Dashboard cast to Sovis\",\n                \"device\": \"Sovis\"\n            }\n        else:\n            error_msg = stderr.decode().strip() if stderr else \"Unknown error\"\n            logger.error(f\"Failed to cast dashboard: {error_msg}\")\n            return {\n                \"success\": False,\n                \"message\": f\"Failed to cast: {error_msg}\",\n                \"device\": \"Sovis\",\n                \"error\": error_msg\n            }\n\n    except Exception as e:\n        logger.error(f\"Error casting dashboard: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error casting: {str(e)}\",\n            \"device\": \"Sovis\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Unload all running models from Ollama by setting keep_alive=0", "output": "```python\nasync def unload_all_models() -> Dict[str, Any]:\n    \"\"\"\n    Unload all running models from Ollama by setting keep_alive=0.\n\n    Returns:\n        Dict with success status and list of unloaded models\n    \"\"\"\n    try:\n        logger.info(\"Attempting to unload all models\")\n\n        # First, get list of running models\n        running_models = await ollama_client.list_running_models()\n\n        if not running_models:\n            logger.info(\"No models currently running\")\n            return {\n                \"success\": True,\n                \"message\": \"No models were running\",\n                \"unloaded_models\": []\n            }\n\n        model_names = [model.get(\"name\", \"unknown\") for model in running_models]\n        logger.info(f\"Found {len(model_names)} running models: {model_names}\")\n\n        # Use httpx to send unload request to each model\n        async with httpx.AsyncClient(timeout=30.0) as client:\n            for model_info in running_models:\n                model_name = model_info.get(\"name\", \"\")\n                if model_name:\n                    try:\n                        logger.info(f\"Unloading model: {model_name}\")\n                        response = await client.post(\n                            OLLAMA_GENERATE_ENDPOINT,\n                            json={\n                                \"model\": model_name,\n                                \"prompt\": \"\",\n                                \"keep_alive\": 0\n                            }\n                        )\n\n                        if response.status_code == 200:\n                            logger.info(f\"Successfully unloaded: {model_name}\")\n                        else:\n                            logger.warning(f\"Unexpected response unloading {model_name}: {response.status_code}\")\n\n                    except Exception as model_error:\n                        logger.error(f\"Error unloading {model_name}: {model_error}\")\n\n        logger.info(f\"Unloaded {len(model_names)} models\")\n        return {\n            \"success\": True,\n            \"message\": f\"Unloaded {len(model_names)} model(s)\",\n            \"unloaded_models\": model_names\n        }\n\n    except Exception as e:\n        logger.error(f\"Error unloading models: {e}\", exc_info=True)\n        return {\n            \"success\": False,\n            \"message\": f\"Error unloading models: {str(e)}\",\n            \"error\": str(e)\n        }\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: Get today's usage statistics", "output": "```python\nasync def get_today_stats() -> Dict[str, Any]:\n    \"\"\"\n    Get today's usage statistics.\n    Currently returns mock data - will be wired up to real analytics later.\n\n    Returns:\n        Dict with today's stats:\n        - sessions_count: Number of chat sessions today\n        - total_tokens: Total tokens processed today\n        - avg_ttft_ms: Average time to first token in milliseconds\n        - agents_used: List of agent profiles used today\n    \"\"\"\n    logger.info(\"Fetching today's stats (mock data)\")\n\n    # TODO: Wire up to real analytics database\n    # For now, return mock data\n    mock_stats = {\n        \"sessions_count\": 42,\n        \"total_tokens\": 15847,\n        \"avg_ttft_ms\": 245,\n        \"agents_used\": [\"QWEN\"],\n        \"is_mock\": True  # Flag to indicate this is mock data\n    }\n\n    logger.info(f\"Returning mock stats: {mock_stats}\")\n    return mock_stats\n```\n\nDenna funktion finns i `app/services/system_control.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r full cascade pipeline: Planner ‚Üí Coder ‚Üí Reviewer\n\nArgs:\n    task: Uppgiftsbeskrivning fr√•n anv√§ndaren\n    request_id: Unik request ID\n    skip_review: Hoppa √∂ver review-fasen (snabbare)\n\nYields:\n    CascadeResult f√∂r varje fas med streaming content", "output": "```python\n    async def execute_cascade(\n        self,\n        task: str,\n        request_id: str,\n        skip_review: bool = False,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        K√∂r full cascade pipeline: Planner ‚Üí Coder ‚Üí Reviewer\n\n        Args:\n            task: Uppgiftsbeskrivning fr√•n anv√§ndaren\n            request_id: Unik request ID\n            skip_review: Hoppa √∂ver review-fasen (snabbare)\n\n        Yields:\n            CascadeResult f√∂r varje fas med streaming content\n        \"\"\"\n        logger.info(f\"[{request_id}] Starting cascade for task: {task[:100]}...\")\n\n        # === FAS 1: PLANNING ===\n        logger.info(f\"[{request_id}] Phase 1: PLANNING\")\n        plan_content = \"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.PLANNING,\n            input_content=task,\n            request_id=f\"{request_id}-plan\"\n        ):\n            if result.content:\n                plan_content += result.content\n            yield result\n\n        if not plan_content:\n            logger.error(f\"[{request_id}] Planning phase produced no output\")\n            return\n\n        # === FAS 2: CODING ===\n        logger.info(f\"[{request_id}] Phase 2: CODING\")\n        code_content = \"\"\n\n        # Bygg context f√∂r coder\n        coder_input = f\"\"\"UPPGIFT:\n{task}\n\nPLAN FR√ÖN PLANNER:\n{plan_content}\n\nImplementera nu koden enligt planen ovan.\"\"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.CODING,\n            input_content=coder_input,\n            request_id=f\"{request_id}-code\"\n        ):\n            if result.content:\n                code_content += result.content\n            yield result\n\n        if not code_content:\n            logger.error(f\"[{request_id}] Coding phase produced no output\")\n            return\n\n        # === FAS 3: REVIEWING (optional) ===\n        if not skip_review:\n            logger.info(f\"[{request_id}] Phase 3: REVIEWING\")\n\n            # Bygg context f√∂r reviewer\n            reviewer_input = f\"\"\"URSPRUNGLIG UPPGIFT:\n{task}\n\nPLAN:\n{plan_content}\n\nKOD ATT GRANSKA:\n{code_content}\n\nGranska koden ovan och ge feedback.\"\"\"\n\n            async for result in self._run_phase(\n                phase=CascadePhase.REVIEWING,\n                input_content=reviewer_input,\n                request_id=f\"{request_id}-review\"\n            ):\n                yield result\n\n        # === COMPLETE ===\n        yield CascadeResult(\n            phase=CascadePhase.COMPLETE,\n            agent_id=\"cascade\",\n            content=\"\",\n            stats=None\n        )\n        logger.info(f\"[{request_id}] Cascade complete\")\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r en enskild fas i cascade-pipelinen", "output": "```python\n    async def _run_phase(\n        self,\n        phase: CascadePhase,\n        input_content: str,\n        request_id: str,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        K√∂r en enskild fas i cascade-pipelinen.\n\n        Hanterar:\n        - Hot-swap av modell (unload andra)\n        - Streaming av output\n        - Stats collection\n        \"\"\"\n        config = self.AGENT_ROLES[phase]\n        profile = config[\"profile\"]\n\n        # Hot-swap: Unload andra modeller f√∂r att frig√∂ra VRAM\n        logger.info(f\"[{request_id}] Hot-swap: Loading {profile}\")\n        try:\n            unloaded = await self._ollama.unload_other_models(profile)\n            if unloaded:\n                logger.info(f\"[{request_id}] Unloaded models: {unloaded}\")\n        except Exception as e:\n            logger.warning(f\"[{request_id}] Hot-swap warning: {e}\")\n\n        # Bygg messages med custom system prompt\n        messages = [\n            {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": input_content}\n        ]\n\n        # Streama fr√•n orchestrator\n        try:\n            async for token, stats, agent_id in self._orchestrator.chat_stream(\n                messages=messages,\n                request_id=request_id,\n                profile=profile,\n                temperature=config[\"temperature\"]\n            ):\n                if stats:\n                    # Final stats\n                    yield CascadeResult(\n                        phase=phase,\n                        agent_id=agent_id,\n                        content=\"\",\n                        stats=stats\n                    )\n                else:\n                    # Streaming token\n                    yield CascadeResult(\n                        phase=phase,\n                        agent_id=agent_id,\n                        content=token,\n                        stats=None\n                    )\n        except Exception as e:\n            logger.error(f\"[{request_id}] Phase {phase.value} error: {e}\")\n            yield CascadeResult(\n                phase=phase,\n                agent_id=profile,\n                content=f\"\\n[ERROR] {phase.value} failed: {str(e)}\",\n                stats=None\n            )\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: K√∂r cascade och returnera sammanslaget resultat (ej streaming)", "output": "```python\n    async def quick_cascade(\n        self,\n        task: str,\n        request_id: str,\n    ) -> Dict[str, str]:\n        \"\"\"\n        K√∂r cascade och returnera sammanslaget resultat (ej streaming).\n\n        Returns:\n            {\"plan\": \"...\", \"code\": \"...\", \"review\": \"...\"}\n        \"\"\"\n        results = {\"plan\": \"\", \"code\": \"\", \"review\": \"\"}\n\n        async for result in self.execute_cascade(task, request_id):\n            if result.phase == CascadePhase.PLANNING:\n                results[\"plan\"] += result.content\n            elif result.phase == CascadePhase.CODING:\n                results[\"code\"] += result.content\n            elif result.phase == CascadePhase.REVIEWING:\n                results[\"review\"] += result.content\n\n        return results\n```\n\nDenna funktion finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Faser i cascade-pipelinen", "output": "```python\nclass CascadePhase(str, Enum):\n    \"\"\"Faser i cascade-pipelinen\"\"\"\n    PLANNING = \"planning\"\n    CODING = \"coding\"\n    REVIEWING = \"reviewing\"\n    COMPLETE = \"complete\"\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Resultat fr√•n en cascade-fas", "output": "```python\nclass CascadeResult:\n    \"\"\"Resultat fr√•n en cascade-fas\"\"\"\n    phase: CascadePhase\n    agent_id: str\n    content: str\n    stats: Optional[UnifiedStats] = None\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-klass: Orkestrerar multi-agent cascade: Planner ‚Üí Coder ‚Üí Reviewer\n\nHanterar:\n- Sekventiell k√∂rning av agenter\n- Hot-swap av modeller (VRAM-effektivt)\n- Streaming av output fr√•n varje fas\n- Sammanslagning av resultat", "output": "```python\nclass CascadeOrchestrator:\n    \"\"\"\n    Orkestrerar multi-agent cascade: Planner ‚Üí Coder ‚Üí Reviewer\n\n    Hanterar:\n    - Sekventiell k√∂rning av agenter\n    - Hot-swap av modeller (VRAM-effektivt)\n    - Streaming av output fr√•n varje fas\n    - Sammanslagning av resultat\n    \"\"\"\n\n    # Agent-konfiguration f√∂r varje roll\n    AGENT_ROLES = {\n        CascadePhase.PLANNING: {\n            \"profile\": \"devstral-openhands\",\n            \"system_prompt\": PLANNER_SYSTEM_PROMPT,\n            \"temperature\": 0.3,  # Mer fokuserad f√∂r planering\n        },\n        CascadePhase.CODING: {\n            \"profile\": \"qwen-sonnet\",\n            \"system_prompt\": CODER_SYSTEM_PROMPT,\n            \"temperature\": 0.15,  # Mycket fokuserad f√∂r kod\n        },\n        CascadePhase.REVIEWING: {\n            \"profile\": \"qwen-openhands\",\n            \"system_prompt\": REVIEWER_SYSTEM_PROMPT,\n            \"temperature\": 0.2,  # Balanserad f√∂r granskning\n        },\n    }\n\n    def __init__(self):\n        self._ollama = ollama_client\n        self._orchestrator = orchestrator\n\n    async def execute_cascade(\n        self,\n        task: str,\n        request_id: str,\n        skip_review: bool = False,\n    ) -> AsyncGenerator[CascadeResult, None]:\n        \"\"\"\n        K√∂r full cascade pipeline: Planner ‚Üí Coder ‚Üí Reviewer\n\n        Args:\n            task: Uppgiftsbeskrivning fr√•n anv√§ndaren\n            request_id: Unik request ID\n            skip_review: Hoppa √∂ver review-fasen (snabbare)\n\n        Yields:\n            CascadeResult f√∂r varje fas med streaming content\n        \"\"\"\n        logger.info(f\"[{request_id}] Starting cascade for task: {task[:100]}...\")\n\n        # === FAS 1: PLANNING ===\n        logger.info(f\"[{request_id}] Phase 1: PLANNING\")\n        plan_content = \"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.PLANNING,\n            input_content=task,\n            request_id=f\"{request_id}-plan\"\n        ):\n            if result.content:\n                plan_content += result.content\n            yield result\n\n        if not plan_content:\n            logger.error(f\"[{request_id}] Planning phase produced no output\")\n            return\n\n        # === FAS 2: CODING ===\n        logger.info(f\"[{request_id}] Phase 2: CODING\")\n        code_content = \"\"\n\n        # Bygg context f√∂r coder\n        coder_input = f\"\"\"UPPGIFT:\n{task}\n\nPLAN FR√ÖN PLANNER:\n{plan_content}\n\nImplementera nu koden enligt planen ovan.\"\"\"\n\n        async for result in self._run_phase(\n            phase=CascadePhase.CODING,\n            input_content=coder_input,\n            request_id=f\"{request_id}-code\"\n        ):\n            if result.content:\n                code_content += result.content\n            yield result\n\n        if not code_content:\n            logger.error(f\"[{request_id}] Coding phase produced no output\")\n            return\n\n        # === FAS 3: REVIEWING (optional) ===\n        if not skip_review:\n            logger.info(f\"[{request_id}] Phase 3: REVIEWING\")\n\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/cascade_orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Generate a filtered file tree for model context injection", "output": "```python\ndef get_project_structure(\n    root_path: str = \"/home/ai-server/local-llm-backend\",\n    max_depth: int = 4,\n    include_sizes: bool = False\n) -> ProjectContext:\n    \"\"\"\n    Generate a filtered file tree for model context injection.\n\n    Args:\n        root_path: Root directory to scan\n        max_depth: Maximum directory depth\n        include_sizes: Include file sizes in output\n\n    Returns:\n        ProjectContext with file tree and metadata\n    \"\"\"\n    root = Path(root_path)\n    if not root.exists():\n        logger.warning(f\"Project root not found: {root_path}\")\n        return ProjectContext(\n            root_path=root_path,\n            file_tree=\"[Project not found]\",\n            file_count=0,\n            directory_count=0\n        )\n\n    lines = []\n    file_count = 0\n    dir_count = 0\n    languages = set()\n    key_files = []\n\n    # Language detection by extension\n    lang_map = {\n        '.py': 'Python',\n        '.js': 'JavaScript',\n        '.ts': 'TypeScript',\n        '.tsx': 'TypeScript/React',\n        '.jsx': 'JavaScript/React',\n        '.html': 'HTML',\n        '.css': 'CSS',\n        '.scss': 'SCSS',\n        '.json': 'JSON',\n        '.yaml': 'YAML',\n        '.yml': 'YAML',\n        '.md': 'Markdown',\n        '.sh': 'Shell',\n        '.sql': 'SQL',\n        '.rs': 'Rust',\n        '.go': 'Go',\n    }\n\n    # Key file patterns\n    key_patterns = [\n        'main.py', 'app.py', 'index.ts', 'index.js',\n        'package.json', 'requirements.txt', 'pyproject.toml',\n        'Dockerfile', 'docker-compose.yml', '.env.example',\n        'README.md', 'Makefile', 'setup.py'\n    ]\n\n    def should_ignore(name: str, is_dir: bool) -> bool:\n        \"\"\"Check if path should be ignored\"\"\"\n        if is_dir:\n            return name in IGNORE_DIRS or name.startswith('.')\n        return any(\n            name == pat or (pat.startswith('*') and name.endswith(pat[1:]))\n            for pat in IGNORE_FILES\n        )\n\n    def build_tree(path: Path, prefix: str = \"\", depth: int = 0):\n        nonlocal file_count, dir_count\n\n        if depth > max_depth:\n            return\n\n        try:\n            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n        except PermissionError:\n            return\n\n        # Filter entries\n        entries = [e for e in entries if not should_ignore(e.name, e.is_dir())]\n\n        for i, entry in enumerate(entries):\n            is_last = i == len(entries) - 1\n            connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n            new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n\n            if entry.is_dir():\n                dir_count += 1\n                lines.append(f\"{prefix}{connector}{entry.name}/\")\n                build_tree(entry, new_prefix, depth + 1)\n            else:\n                file_count += 1\n                suffix = entry.suffix.lower()\n\n                # Detect language\n                if suffix in lang_map:\n                    languages.add(lang_map[suffix])\n\n                # Check for key files\n                if entry.name in key_patterns:\n                    key_files.append(str(entry.relative_to(root)))\n\n                # Format line\n                size_str = \"\"\n                if include_sizes:\n                    try:\n                        size = entry.stat().st_size\n                        if size > 1024 * 1024:\n                            size_str = f\" ({size / 1024 / 1024:.1f}MB)\"\n                        elif size > 1024:\n                            size_str = f\" ({size / 1024:.1f}KB)\"\n                    except OSError:\n                        pass\n\n                lines.append(f\"{prefix}{connector}{entry.name}{size_str}\")\n\n    # Build the tree\n    lines.append(f\"{root.name}/\")\n    build_tree(root)\n\n    return ProjectContext(\n        root_path=root_path,\n        file_tree=\"\\n\".join(lines),\n        file_count=file_count,\n        directory_count=dir_count,\n        languages_detected=sorted(languages),\n        key_files=key_files\n    )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build a context-aware system prompt addition", "output": "```python\ndef build_context_prompt(project: ProjectContext) -> str:\n    \"\"\"\n    Build a context-aware system prompt addition.\n\n    Args:\n        project: ProjectContext from get_project_structure()\n\n    Returns:\n        String to append to system prompt\n    \"\"\"\n    return f\"\"\"\n## PROJECT CONTEXT\n\nDu arbetar med f√∂ljande projektstruktur:\n\n```\n{project.file_tree}\n```\n\n**Projekt√∂versikt:**\n- Filer: {project.file_count}\n- Mappar: {project.directory_count}\n- Spr√•k: {', '.join(project.languages_detected) if project.languages_detected else 'Ok√§nt'}\n- Viktiga filer: {', '.join(project.key_files) if project.key_files else 'Inga hittade'}\n\nN√§r du skriver kod, anv√§nd denna struktur f√∂r att:\n1. Referera till r√§tt fils√∂kv√§gar\n2. F√∂lja projektets konventioner\n3. F√∂rst√• var ny kod ska placeras\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion:     Extract code blocks from markdown-formatted text", "output": "```python\ndef extract_code_blocks(text: str) -> List[Tuple[str, str]]:\n    \"\"\"\n    Extract code blocks from markdown-formatted text.\n\n    Args:\n        text: Text containing ```language\\ncode``` blocks\n\n    Returns:\n        List of (language, code) tuples\n    \"\"\"\n    blocks = []\n    for match in CODE_BLOCK_PATTERN.finditer(text):\n        language = match.group(1) or \"unknown\"\n        code = match.group(2).strip()\n        blocks.append((language.lower(), code))\n    return blocks\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Validate Python code syntax using AST parser", "output": "```python\ndef validate_python_syntax(code: str) -> SyntaxCheckResult:\n    \"\"\"\n    Validate Python code syntax using AST parser.\n\n    Args:\n        code: Python code string\n\n    Returns:\n        SyntaxCheckResult with validation details\n    \"\"\"\n    try:\n        ast.parse(code)\n        return SyntaxCheckResult(\n            is_valid=True,\n            language=\"python\",\n            code=code\n        )\n    except SyntaxError as e:\n        return SyntaxCheckResult(\n            is_valid=False,\n            language=\"python\",\n            code=code,\n            error_message=str(e.msg) if e.msg else str(e),\n            error_line=e.lineno\n        )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build a prompt for self-correction when syntax error is detected", "output": "```python\ndef build_correction_prompt(result: SyntaxCheckResult) -> str:\n    \"\"\"\n    Build a prompt for self-correction when syntax error is detected.\n\n    Args:\n        result: Failed SyntaxCheckResult\n\n    Returns:\n        Prompt for model to correct the code\n    \"\"\"\n    return f\"\"\"\n## SYNTAX ERROR DETECTED\n\nDin kod inneh√•ller ett syntaxfel som m√•ste korrigeras:\n\n**Fel:** {result.error_message}\n**Rad:** {result.error_line}\n\n**Felaktig kod:**\n```python\n{result.code}\n```\n\nKorrigera koden och returnera ENDAST den korrigerade versionen utan f√∂rklaringar.\nSvara med:\n```python\n[korrigerad kod h√§r]\n```\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Validate all code blocks in a response", "output": "```python\ndef validate_code_in_response(text: str) -> Tuple[bool, List[SyntaxCheckResult], str]:\n    \"\"\"\n    Validate all code blocks in a response.\n\n    Args:\n        text: Full response text with potential code blocks\n\n    Returns:\n        Tuple of (all_valid, results, corrected_text)\n    \"\"\"\n    blocks = extract_code_blocks(text)\n    results = []\n    all_valid = True\n    corrections_needed = []\n\n    for language, code in blocks:\n        if language == \"python\":\n            result = validate_python_syntax(code)\n            results.append(result)\n            if not result.is_valid:\n                all_valid = False\n                corrections_needed.append((code, result))\n\n    # For now, return original text - correction happens in orchestrator\n    return all_valid, results, text\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build prompt addition to enforce JSON output", "output": "```python\ndef build_json_enforcement_prompt(schema_hint: Optional[Dict] = None) -> str:\n    \"\"\"\n    Build prompt addition to enforce JSON output.\n\n    Args:\n        schema_hint: Optional JSON schema for structure guidance\n\n    Returns:\n        Prompt addition for JSON enforcement\n    \"\"\"\n    schema_str = \"\"\n    if schema_hint:\n        schema_str = f\"\"\"\nAnv√§nd f√∂ljande struktur:\n```json\n{json.dumps(schema_hint, indent=2, ensure_ascii=False)}\n```\n\"\"\"\n\n    return f\"\"\"\n## OUTPUT FORMAT: JSON\n\nVIKTIGT: Svara ENDAST med valid JSON. Ingen annan text.\n{schema_str}\nRegler:\n- B√∂rja med {{ eller [\n- Sluta med }} eller ]\n- Anv√§nd dubbla citattecken f√∂r str√§ngar\n- Inga trailing commas\n- Inga kommentarer\n\"\"\"\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Simple validation of JSON against required fields", "output": "```python\ndef validate_json_against_schema(\n    data: Any,\n    required_fields: List[str]\n) -> Tuple[bool, List[str]]:\n    \"\"\"\n    Simple validation of JSON against required fields.\n\n    Args:\n        data: Parsed JSON data\n        required_fields: List of required field names\n\n    Returns:\n        Tuple of (is_valid, missing_fields)\n    \"\"\"\n    if not isinstance(data, dict):\n        return False, required_fields\n\n    missing = [f for f in required_fields if f not in data]\n    return len(missing) == 0, missing\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Check if path should be ignored", "output": "```python\n    def should_ignore(name: str, is_dir: bool) -> bool:\n        \"\"\"Check if path should be ignored\"\"\"\n        if is_dir:\n            return name in IGNORE_DIRS or name.startswith('.')\n        return any(\n            name == pat or (pat.startswith('*') and name.endswith(pat[1:]))\n            for pat in IGNORE_FILES\n        )\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Skapa en Python-funktion som build tree", "output": "```python\n    def build_tree(path: Path, prefix: str = \"\", depth: int = 0):\n        nonlocal file_count, dir_count\n\n        if depth > max_depth:\n            return\n\n        try:\n            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n        except PermissionError:\n            return\n\n        # Filter entries\n        entries = [e for e in entries if not should_ignore(e.name, e.is_dir())]\n\n        for i, entry in enumerate(entries):\n            is_last = i == len(entries) - 1\n            connector = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n            new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n\n            if entry.is_dir():\n                dir_count += 1\n                lines.append(f\"{prefix}{connector}{entry.name}/\")\n                build_tree(entry, new_prefix, depth + 1)\n            else:\n                file_count += 1\n                suffix = entry.suffix.lower()\n\n                # Detect language\n                if suffix in lang_map:\n                    languages.add(lang_map[suffix])\n\n                # Check for key files\n                if entry.name in key_patterns:\n                    key_files.append(str(entry.relative_to(root)))\n\n                # Format line\n                size_str = \"\"\n                if include_sizes:\n                    try:\n                        size = entry.stat().st_size\n                        if size > 1024 * 1024:\n                            size_str = f\" ({size / 1024 / 1024:.1f}MB)\"\n                        elif size > 1024:\n                            size_str = f\" ({size / 1024:.1f}KB)\"\n                    except OSError:\n                        pass\n\n                lines.append(f\"{prefix}{connector}{entry.name}{size_str}\")\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Get cached or fresh project context", "output": "```python\n    def get_project_context(self, force_refresh: bool = False) -> ProjectContext:\n        \"\"\"Get cached or fresh project context\"\"\"\n        if self._project_context is None or force_refresh:\n            self._project_context = get_project_structure(self.project_root)\n            self._context_prompt = build_context_prompt(self._project_context)\n            logger.info(\n                f\"Project context loaded: {self._project_context.file_count} files, \"\n                f\"{self._project_context.directory_count} dirs\"\n            )\n        return self._project_context\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Build enhanced system prompt with intelligence features", "output": "```python\n    def get_enhanced_system_prompt(\n        self,\n        base_prompt: str,\n        include_project: bool = True,\n        output_format: OutputFormat = OutputFormat.TEXT,\n        json_schema: Optional[Dict] = None\n    ) -> str:\n        \"\"\"\n        Build enhanced system prompt with intelligence features.\n\n        Args:\n            base_prompt: Original system prompt\n            include_project: Include project structure context\n            output_format: Desired output format\n            json_schema: Schema hint for JSON output\n\n        Returns:\n            Enhanced system prompt\n        \"\"\"\n        parts = [base_prompt]\n\n        if include_project:\n            self.get_project_context()\n            if self._context_prompt:\n                parts.append(self._context_prompt)\n\n        if output_format == OutputFormat.JSON:\n            parts.append(build_json_enforcement_prompt(json_schema))\n\n        return \"\\n\\n\".join(parts)\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Process model response with intelligence checks", "output": "```python\n    def process_response(\n        self,\n        response: str,\n        validate_python: bool = True,\n        expect_json: bool = False,\n        required_json_fields: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process model response with intelligence checks.\n\n        Args:\n            response: Raw model response\n            validate_python: Check Python syntax in code blocks\n            expect_json: Parse and validate JSON output\n            required_json_fields: Required fields for JSON validation\n\n        Returns:\n            Dict with processed response and metadata\n        \"\"\"\n        result = {\n            \"original\": response,\n            \"processed\": response,\n            \"syntax_valid\": True,\n            \"syntax_errors\": [],\n            \"json_valid\": None,\n            \"json_data\": None,\n            \"needs_correction\": False,\n            \"correction_prompt\": None\n        }\n\n        # Python syntax validation\n        if validate_python:\n            all_valid, checks, _ = validate_code_in_response(response)\n            result[\"syntax_valid\"] = all_valid\n\n            for check in checks:\n                if not check.is_valid:\n                    result[\"syntax_errors\"].append({\n                        \"error\": check.error_message,\n                        \"line\": check.error_line,\n                        \"code_snippet\": check.code[:100] + \"...\" if len(check.code) > 100 else check.code\n                    })\n                    result[\"needs_correction\"] = True\n                    result[\"correction_prompt\"] = build_correction_prompt(check)\n\n        # JSON validation\n        if expect_json:\n            success, data, raw = extract_json_from_response(response)\n            result[\"json_valid\"] = success\n\n            if success:\n                result[\"json_data\"] = data\n                result[\"processed\"] = raw\n\n                if required_json_fields:\n                    fields_valid, missing = validate_json_against_schema(data, required_json_fields)\n                    if not fields_valid:\n                        result[\"json_valid\"] = False\n                        result[\"json_data\"] = {\"error\": f\"Missing fields: {missing}\"}\n            else:\n                result[\"json_data\"] = {\"error\": data}\n\n        return result\n```\n\nDenna funktion finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Supported output formats", "output": "```python\nclass OutputFormat(str, Enum):\n    \"\"\"Supported output formats\"\"\"\n    TEXT = \"text\"\n    JSON = \"json\"\n    CODE = \"code\"\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Result of syntax validation", "output": "```python\nclass SyntaxCheckResult:\n    \"\"\"Result of syntax validation\"\"\"\n    is_valid: bool\n    language: str\n    code: str\n    error_message: Optional[str] = None\n    error_line: Optional[int] = None\n    corrected_code: Optional[str] = None\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Project structure context for model injection", "output": "```python\nclass ProjectContext:\n    \"\"\"Project structure context for model injection\"\"\"\n    root_path: str\n    file_tree: str\n    file_count: int\n    directory_count: int\n    languages_detected: List[str] = field(default_factory=list)\n    key_files: List[str] = field(default_factory=list)\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-klass: Central manager for all intelligence features", "output": "```python\nclass IntelligenceManager:\n    \"\"\"\n    Central manager for all intelligence features.\n    Integrates with the orchestrator for enhanced responses.\n    \"\"\"\n\n    def __init__(self, project_root: str = \"/home/ai-server/local-llm-backend\"):\n        self.project_root = project_root\n        self._project_context: Optional[ProjectContext] = None\n        self._context_prompt: Optional[str] = None\n\n    def get_project_context(self, force_refresh: bool = False) -> ProjectContext:\n        \"\"\"Get cached or fresh project context\"\"\"\n        if self._project_context is None or force_refresh:\n            self._project_context = get_project_structure(self.project_root)\n            self._context_prompt = build_context_prompt(self._project_context)\n            logger.info(\n                f\"Project context loaded: {self._project_context.file_count} files, \"\n                f\"{self._project_context.directory_count} dirs\"\n            )\n        return self._project_context\n\n    def get_enhanced_system_prompt(\n        self,\n        base_prompt: str,\n        include_project: bool = True,\n        output_format: OutputFormat = OutputFormat.TEXT,\n        json_schema: Optional[Dict] = None\n    ) -> str:\n        \"\"\"\n        Build enhanced system prompt with intelligence features.\n\n        Args:\n            base_prompt: Original system prompt\n            include_project: Include project structure context\n            output_format: Desired output format\n            json_schema: Schema hint for JSON output\n\n        Returns:\n            Enhanced system prompt\n        \"\"\"\n        parts = [base_prompt]\n\n        if include_project:\n            self.get_project_context()\n            if self._context_prompt:\n                parts.append(self._context_prompt)\n\n        if output_format == OutputFormat.JSON:\n            parts.append(build_json_enforcement_prompt(json_schema))\n\n        return \"\\n\\n\".join(parts)\n\n    def process_response(\n        self,\n        response: str,\n        validate_python: bool = True,\n        expect_json: bool = False,\n        required_json_fields: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process model response with intelligence checks.\n\n        Args:\n            response: Raw model response\n            validate_python: Check Python syntax in code blocks\n            expect_json: Parse and validate JSON output\n            required_json_fields: Required fields for JSON validation\n\n        Returns:\n            Dict with processed response and metadata\n        \"\"\"\n        result = {\n            \"original\": response,\n            \"processed\": response,\n            \"syntax_valid\": True,\n            \"syntax_errors\": [],\n            \"json_valid\": None,\n            \"json_data\": None,\n            \"needs_correction\": False,\n            \"correction_prompt\": None\n        }\n\n        # Python syntax validation\n        if validate_python:\n            all_valid, checks, _ = validate_code_in_response(response)\n            result[\"syntax_valid\"] = all_valid\n\n            for check in checks:\n                if not check.is_valid:\n                    result[\"syntax_errors\"].append({\n                        \"error\": check.error_message,\n                        \"line\": check.error_line,\n                        \"code_snippet\": check.code[:100] + \"...\" if len(check.code) > 100 else check.code\n                    })\n                    result[\"needs_correction\"] = True\n                    result[\"correction_prompt\"] = build_correction_prompt(check)\n\n        # JSON validation\n        if expect_json:\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/intelligence.py`."}
{"instruction": "Implementera en Python-funktion: Run async coroutine in sync context", "output": "```python\ndef _run_async(coro):\n    \"\"\"Run async coroutine in sync context\"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            # If we're in an async context, create a new thread\n            import concurrent.futures\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                future = executor.submit(asyncio.run, coro)\n                return future.result(timeout=60)\n        else:\n            return loop.run_until_complete(coro)\n    except RuntimeError:\n        # No event loop, create one\n        return asyncio.run(coro)\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file read operation", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file read operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileReadTool: Reading {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_read\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileReadTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute directory listing", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute directory listing\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileListTool: Listing {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_list\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileListTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file exists check", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file exists check\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileExistsTool: Checking {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_exists\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileExistsTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-funktion: Execute file write operation", "output": "```python\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file write operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                return \"Error: Kunde inte tolka parametrar som JSON\"\n\n        path = params.get(\"path\", \"\")\n        content = params.get(\"content\", \"\")\n\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n        if not content:\n            return \"Error: Inget inneh√•ll angivet\"\n\n        logger.info(f\"FileWriteTool: Writing to {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_write\", {\"path\": path, \"content\": content}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileWriteTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nDenna funktion finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for reading file contents on the server", "output": "```python\nclass FileReadTool(BaseTool):\n    \"\"\"Tool for reading file contents on the server\"\"\"\n\n    description = \"L√§s inneh√•llet i en fil p√• servern (max 10KB). Anv√§ndbart f√∂r att granska kod, konfigurationsfiler, loggar, etc.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut s√∂kv√§g till filen som ska l√§sas, t.ex. /home/ai-server/app/main.py\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file read operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileReadTool: Reading {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_read\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileReadTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for listing directory contents", "output": "```python\nclass FileListTool(BaseTool):\n    \"\"\"Tool for listing directory contents\"\"\"\n\n    description = \"Lista filer och mappar i en katalog p√• servern. Visar filtyp, storlek och namn.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut s√∂kv√§g till katalogen som ska listas, t.ex. /home/ai-server/\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute directory listing\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileListTool: Listing {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_list\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileListTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for checking if a file or directory exists", "output": "```python\nclass FileExistsTool(BaseTool):\n    \"\"\"Tool for checking if a file or directory exists\"\"\"\n\n    description = \"Kontrollera om en fil eller katalog finns p√• servern.\"\n    parameters = [{\n        \"name\": \"path\",\n        \"type\": \"string\",\n        \"description\": \"Absolut s√∂kv√§g att kontrollera\",\n        \"required\": True\n    }]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file exists check\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                params = {\"path\": params}\n\n        path = params.get(\"path\", \"\")\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n\n        logger.info(f\"FileExistsTool: Checking {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_exists\", {\"path\": path}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileExistsTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Implementera en Python-klass: Tool for writing files to the server", "output": "```python\nclass FileWriteTool(BaseTool):\n    \"\"\"Tool for writing files to the server\"\"\"\n\n    description = \"Skriv inneh√•ll till en fil p√• servern. Kan skapa nya filer eller skriva √∂ver befintliga. Till√•tna s√∂kv√§gar: /home/ai-server, /tmp, /var/log, /etc/nginx, /opt\"\n    parameters = [\n        {\n            \"name\": \"path\",\n            \"type\": \"string\",\n            \"description\": \"Absolut s√∂kv√§g till filen som ska skrivas, t.ex. /tmp/test.py\",\n            \"required\": True\n        },\n        {\n            \"name\": \"content\",\n            \"type\": \"string\",\n            \"description\": \"Inneh√•llet som ska skrivas till filen\",\n            \"required\": True\n        }\n    ]\n\n    def call(self, params: Union[str, dict], **kwargs) -> str:\n        \"\"\"Execute file write operation\"\"\"\n        # Handle both string and dict params\n        if isinstance(params, str):\n            import json\n            try:\n                params = json.loads(params)\n            except json.JSONDecodeError:\n                return \"Error: Kunde inte tolka parametrar som JSON\"\n\n        path = params.get(\"path\", \"\")\n        content = params.get(\"content\", \"\")\n\n        if not path:\n            return \"Error: Ingen s√∂kv√§g angiven\"\n        if not content:\n            return \"Error: Inget inneh√•ll angivet\"\n\n        logger.info(f\"FileWriteTool: Writing to {path}\")\n\n        try:\n            result = _run_async(system_tools.execute(\"file_write\", {\"path\": path, \"content\": content}))\n            if result.success:\n                return result.output\n            else:\n                return f\"Error: {result.error}\"\n        except Exception as e:\n            logger.error(f\"FileWriteTool error: {e}\")\n            return f\"Error: {str(e)}\"\n```\n\nKlassen finns i `app/services/qwen_file_tools.py`."}
{"instruction": "Skapa en Python-funktion som from xai", "output": "```python\n    def from_xai(cls, stats: XAIStats, agent_id: str = \"cloud\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_tokens,\n            provider=\"xai\",\n            model=\"grok-2\",\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Skapa en Python-funktion som from claude", "output": "```python\n    def from_claude(cls, stats: ClaudeStats, agent_id: str = \"claude\") -> \"UnifiedStats\":\n        return cls(\n            tokens_generated=stats.tokens_generated,\n            tokens_per_second=stats.tokens_per_second,\n            total_duration_ms=stats.total_duration_ms,\n            prompt_tokens=stats.prompt_tokens,\n            provider=\"claude_ui\",\n            model=\"sonnet\",\n            agent_id=agent_id\n        )\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Get a fallback agent when primary fails", "output": "```python\n    def _get_fallback_agent(self, failed_agent: AgentConfig) -> Optional[AgentConfig]:\n        \"\"\"Get a fallback agent when primary fails\"\"\"\n        if failed_agent.provider == Provider.XAI:\n            # Cloud failed, fallback to local QWEN-SONNET\n            return AGENTS[AgentId.QWEN_SONNET]\n        elif failed_agent.provider == Provider.OLLAMA:\n            # Local failed, try cloud if available\n            if self._xai.is_configured:\n                return AGENTS[AgentId.CLOUD]\n        return None\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Stream from fallback agent", "output": "```python\n    async def _fallback_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        agent: AgentConfig,\n        temperature: float,\n        max_tokens: int\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str], None]:\n        \"\"\"Stream from fallback agent\"\"\"\n        try:\n            if agent.provider == Provider.XAI:\n                async for token, stats in self._xai.chat_stream(\n                    messages=messages,\n                    request_id=request_id,\n                    system_prompt=agent.system_prompt,\n                    temperature=temperature,\n                    max_tokens=max_tokens\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_xai(stats, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n            else:\n                msgs_with_system = [{\"role\": \"system\", \"content\": agent.system_prompt}]\n                msgs_with_system.extend([\n                    {\"role\": m.get(\"role\"), \"content\": m.get(\"content\")}\n                    for m in messages if m.get(\"role\") != \"system\"\n                ])\n\n                class SimpleProfile:\n                    def __init__(self, agent_id: str, model: str, temp: float, max_tok: int):\n                        self.id = agent_id\n                        self.model = model\n                        self.temperature = temp\n                        self.top_p = 0.9\n                        self.repeat_penalty = 1.1\n                        self.max_tokens = max_tok\n                        self.context_length = 8192\n\n                profile_obj = SimpleProfile(agent.id.value, agent.model, temperature, max_tokens)\n\n                async for token, stats in self._ollama.chat_stream(\n                    profile=profile_obj,\n                    messages=msgs_with_system,\n                    request_id=request_id\n                ):\n                    if stats:\n                        yield \"\", UnifiedStats.from_ollama(stats, agent.model, agent.id.value), agent.id.value\n                    else:\n                        yield token, None, agent.id.value\n        except Exception as e:\n            logger.error(f\"[{request_id}] Fallback also failed: {e}\")\n            raise\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Chat with self-reflection/debate loop", "output": "```python\n    async def chat_with_reflection(\n        self,\n        messages: list[dict],\n        request_id: str,\n        profile: str = \"qwen-sonnet\",\n        temperature: Optional[float] = None,\n        max_iterations: int = 2,\n    ) -> AsyncGenerator[tuple[str, Optional[UnifiedStats], str, str], None]:\n        \"\"\"\n        Chat with self-reflection/debate loop.\n\n        Modellen kritiserar och f√∂rb√§ttrar sitt eget svar iterativt.\n        Inspirerat av Claudes self-reflection och Groks debate-tips.\n\n        Yields:\n            Tuple of (token, stats, agent_id, phase)\n            - phase: \"initial\", \"critique\", \"improvement\", \"final\"\n        \"\"\"\n        # Phase 1: Initial response\n        logger.info(f\"[{request_id}] Debate loop: Phase 1 - Initial response\")\n        full_response = \"\"\n\n        async for token, stats, agent_id in self.chat_stream(\n            messages=messages,\n            request_id=f\"{request_id}-initial\",\n            profile=profile,\n            temperature=temperature\n        ):\n            if stats:\n                yield \"\", stats, agent_id, \"initial\"\n            else:\n                full_response += token\n                yield token, None, agent_id, \"initial\"\n\n        # Phase 2-3: Critique and improve (iterate)\n        for i in range(max_iterations):\n            # Critique prompt\n            critique_messages = [\n                {\"role\": \"user\", \"content\": f\"\"\"Kritiskt granska denna kod/text:\n\n{full_response}\n\nIdentifiera:\n1. Logiska fel eller buggar\n2. S√§kerhetsproblem\n3. F√∂rb√§ttringsm√∂jligheter\n4. Missade edge cases\n\nOm allt ser bra ut, svara med \"APPROVED - ingen f√∂rb√§ttring beh√∂vs\".\nAnnars lista konkreta f√∂rb√§ttringar.\"\"\"}\n            ]\n\n            logger.info(f\"[{request_id}] Debate loop: Iteration {i+1} - Critique\")\n            critique_response = \"\"\n\n            async for token, stats, agent_id in self.chat_stream(\n                messages=critique_messages,\n                request_id=f\"{request_id}-critique-{i}\",\n                profile=profile,\n                temperature=0.3  # Lower temp for critique\n            ):\n                if stats:\n                    yield \"\", stats, agent_id, \"critique\"\n                else:\n                    critique_response += token\n                    yield token, None, agent_id, \"critique\"\n\n            # Check if approved\n            if \"APPROVED\" in critique_response.upper():\n                logger.info(f\"[{request_id}] Debate loop: Response approved after {i+1} iterations\")\n                break\n\n            # Improvement phase\n            improve_messages = [\n                {\"role\": \"user\", \"content\": messages[-1].get(\"content\", \"\")},\n                {\"role\": \"assistant\", \"content\": full_response},\n                {\"role\": \"user\", \"content\": f\"\"\"F√∂rb√§ttra ditt svar baserat p√• denna kritik:\n\n{critique_response}\n\nGe en f√∂rb√§ttrad version av hela svaret.\"\"\"}\n            ]\n\n            logger.info(f\"[{request_id}] Debate loop: Iteration {i+1} - Improvement\")\n            improved_response = \"\"\n\n            async for token, stats, agent_id in self.chat_stream(\n                messages=improve_messages,\n                request_id=f\"{request_id}-improve-{i}\",\n                profile=profile,\n                temperature=temperature\n            ):\n                if stats:\n                    yield \"\", stats, agent_id, \"improvement\"\n                else:\n                    improved_response += token\n                    yield token, None, agent_id, \"improvement\"\n\n            full_response = improved_response\n\n        # Final marker\n        yield \"\", None, agent_id, \"final\"\n        logger.info(f\"[{request_id}] Debate loop: Complete\")\n```\n\nDenna funktion finns i `app/services/orchestrator.py`."}
{"instruction": "Implementera en Python-funktion: Check if API key is configured", "output": "```python\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if Gemini API is reachable and configured", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Gemini API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Gemini API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - list models\n            response = await client.get(\n                f\"{GEMINI_BASE_URL}/models\",\n                params={\"key\": self.api_key},\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Gemini connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Convert chat messages to Gemini format", "output": "```python\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> tuple[list[dict], Optional[dict]]:\n        \"\"\"Convert chat messages to Gemini format\"\"\"\n        gemini_contents = []\n        system_instruction = None\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Gemini uses system_instruction separately\n                system_instruction = {\"parts\": [{\"text\": content}]}\n            elif role == \"user\":\n                gemini_contents.append({\n                    \"role\": \"user\",\n                    \"parts\": [{\"text\": content}]\n                })\n            elif role == \"assistant\":\n                gemini_contents.append({\n                    \"role\": \"model\",\n                    \"parts\": [{\"text\": content}]\n                })\n\n        # Apply external system prompt if provided and none in messages\n        if system_prompt and not system_instruction:\n            system_instruction = {\"parts\": [{\"text\": system_prompt}]}\n\n        return gemini_contents, system_instruction\n```\n\nDenna funktion finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from Gemini API response", "output": "```python\nclass GeminiStats:\n    \"\"\"Statistics from Gemini API response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for Gemini errors", "output": "```python\nclass GeminiError(Exception):\n    \"\"\"Base exception for Gemini errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to Gemini failed", "output": "```python\nclass GeminiConnectionError(GeminiError):\n    \"\"\"Connection to Gemini failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Gemini API returned an error", "output": "```python\nclass GeminiAPIError(GeminiError):\n    \"\"\"Gemini API returned an error\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for Google Gemini API with streaming support", "output": "```python\nclass GeminiClient:\n    \"\"\"\n    Async client for Google Gemini API with streaming support.\n\n    Used for FAST mode - immediate responses without GPU load.\n    \"\"\"\n\n    def __init__(self, api_key: str = GEMINI_API_KEY, model: str = GEMINI_MODEL):\n        self.api_key = api_key\n        self.model = model\n        self._client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if Gemini API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"Gemini API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - list models\n            response = await client.get(\n                f\"{GEMINI_BASE_URL}/models\",\n                params={\"key\": self.api_key},\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"Gemini connection check failed: {e}\")\n            return False\n\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> tuple[list[dict], Optional[dict]]:\n        \"\"\"Convert chat messages to Gemini format\"\"\"\n        gemini_contents = []\n        system_instruction = None\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Gemini uses system_instruction separately\n                system_instruction = {\"parts\": [{\"text\": content}]}\n            elif role == \"user\":\n                gemini_contents.append({\n                    \"role\": \"user\",\n                    \"parts\": [{\"text\": content}]\n                })\n            elif role == \"assistant\":\n                gemini_contents.append({\n                    \"role\": \"model\",\n                    \"parts\": [{\"text\": content}]\n                })\n\n        # Apply external system prompt if provided and none in messages\n        if system_prompt and not system_instruction:\n            system_instruction = {\"parts\": [{\"text\": system_prompt}]}\n\n        return gemini_contents, system_instruction\n\n    async def chat_stream(\n        self,\n        messages: list[dict],\n        request_id: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 4096\n    ) -> AsyncGenerator[tuple[str, Optional[GeminiStats]], None]:\n        \"\"\"\n        Stream chat completion tokens from Gemini.\n\n        Yields:\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/gemini_client.py`."}
{"instruction": "Implementera en Python-funktion: Check if xAI API is reachable and configured", "output": "```python\n    async def is_connected(self) -> bool:\n        \"\"\"Check if xAI API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"xAI API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - send a minimal request\n            response = await client.post(\n                self.base_url,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": self.model,\n                    \"messages\": [{\"role\": \"user\", \"content\": \"ping\"}],\n                    \"max_tokens\": 10,\n                    \"stream\": False\n                },\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"xAI connection check failed: {e}\")\n            return False\n```\n\nDenna funktion finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Statistics from xAI API response", "output": "```python\nclass XAIStats:\n    \"\"\"Statistics from xAI API response\"\"\"\n    tokens_generated: int = 0\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    start_time: float = 0.0\n    first_token_time: Optional[float] = None\n    end_time: float = 0.0\n\n    @property\n    def total_duration_ms(self) -> int:\n        if self.end_time and self.start_time:\n            return int((self.end_time - self.start_time) * 1000)\n        return 0\n\n    @property\n    def tokens_per_second(self) -> float:\n        duration_s = (self.end_time - self.start_time) if self.end_time else 0\n        if duration_s > 0:\n            return self.tokens_generated / duration_s\n        return 0.0\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Base exception for xAI errors", "output": "```python\nclass XAIError(Exception):\n    \"\"\"Base exception for xAI errors\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Connection to xAI failed", "output": "```python\nclass XAIConnectionError(XAIError):\n    \"\"\"Connection to xAI failed\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: xAI API returned an error", "output": "```python\nclass XAIAPIError(XAIError):\n    \"\"\"xAI API returned an error\"\"\"\n    pass\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Implementera en Python-klass: Async client for xAI Grok API with streaming support", "output": "```python\nclass XAIClient:\n    \"\"\"\n    Async client for xAI Grok API with streaming support.\n\n    Used for FAST mode - immediate responses without GPU load.\n    Compatible with OpenAI API format.\n    \"\"\"\n\n    def __init__(self, api_key: str = XAI_API_KEY, model: str = XAI_MODEL, base_url: str = XAI_BASE_URL):\n        self.api_key = api_key\n        self.model = model\n        self.base_url = base_url\n        self._client: Optional[httpx.AsyncClient] = None\n\n    @property\n    def is_configured(self) -> bool:\n        \"\"\"Check if API key is configured\"\"\"\n        return bool(self.api_key)\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client\"\"\"\n        if self._client is None or self._client.is_closed:\n            self._client = httpx.AsyncClient(\n                timeout=httpx.Timeout(\n                    connect=CONNECT_TIMEOUT,\n                    read=READ_TIMEOUT,\n                    write=30.0,\n                    pool=5.0\n                )\n            )\n        return self._client\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            self._client = None\n\n    async def is_connected(self) -> bool:\n        \"\"\"Check if xAI API is reachable and configured\"\"\"\n        if not self.is_configured:\n            logger.warning(\"xAI API key not configured\")\n            return False\n\n        try:\n            client = await self._get_client()\n            # Simple check - send a minimal request\n            response = await client.post(\n                self.base_url,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": self.model,\n                    \"messages\": [{\"role\": \"user\", \"content\": \"ping\"}],\n                    \"max_tokens\": 10,\n                    \"stream\": False\n                },\n                timeout=CONNECT_TIMEOUT\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.warning(f\"xAI connection check failed: {e}\")\n            return False\n\n    def _build_messages(\n        self,\n        messages: list[dict],\n        system_prompt: Optional[str] = None\n    ) -> list[dict]:\n        \"\"\"Convert chat messages to xAI/OpenAI format\"\"\"\n        xai_messages = []\n\n        # Add system prompt if provided\n        if system_prompt:\n            xai_messages.append({\n                \"role\": \"system\",\n                \"content\": system_prompt\n            })\n\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                # Skip system messages if we already added system_prompt\n                if not system_prompt:\n                    xai_messages.append({\n                        \"role\": \"system\",\n                        \"content\": content\n                    })\n            else:\n                # user and assistant roles are directly compatible\n                xai_messages.append({\n                    \"role\": role,\n                    \"content\": content\n                })\n\n        return xai_messages\n\n    async def chat_stream(\n        self,\n    # ... (truncated)\n```\n\nKlassen finns i `app/services/xai_client.py`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1iosk \\1gent", "output": "```typescript\ninterface KioskAgent {\n  id: string\n  name: string\n  description: string\n  color: string\n  glowColor: string\n  iconName: 'brain' | 'code'\n  model: string\n  vramGb: number\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1og \\1tate", "output": "```typescript\ninterface LogState {\n  logs: LogEntry[];\n\n  addLog: (message: string, type?: LogEntry['type']) => void;\n  handleSystemLog: (data: { message: string; level?: string }\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1", "output": "```typescript\nconst BACKEND_URL = `http://${typeof window !== 'undefined' ? window.location.hostname : 'localhost'}:8000`;\n\n// Initial agent states for 3D - SVEN Dual-Expert System\nconst initialAgentStates: Record<string, AgentState> = {\n  'orchestrator': 'idle',\n  // Primary agents (new names)\n  'qwen-sonnet': 'idle',\n  'devstral-openhands': 'idle',\n  'qwen-openhands': 'idle',\n  // Legacy aliases (backwards compatibility)\n  'gpt-oss': 'idle',\n  'devstral': 'idle',\n}\n```\n\nKomponenten finns i `frontend/src/stores/agentStore.ts`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1gent \\1tore \\1tate", "output": "```typescript\ninterface AgentStoreState {\n  // Profiles\n  profiles: Profile[];\n  activeAgentId: string | null;\n  loadingAgent: string | null;\n\n  // Agent network state (for 2D panel)\n  mutedAgents: Set<string>;\n  soloAgent: string | null;\n\n  // 3D Visualization state\n  agentStates: Record<string, AgentState>;\n  activeFlows: FlowConnection[];\n\n  // Actions\n  setActiveAgent: (id: string | null) => void;\n  setLoadingAgent: (id: string | null) => void;\n  toggleMute: (id: string) => void;\n  toggleSolo: (id: string) => void;\n\n  // 3D actions\n  setAgentState: (agentId: string, state: AgentState) => void;\n  startFlow: (from: string, to: string, taskId: string) => void;\n  endFlow: (taskId: string) => void;\n\n  // WebSocket handler\n  handleWSMessage: (data: any) => void;\n\n  // Profile actions\n  fetchProfileStatus: () => Promise<void>;\n  selectAgent: (profileId: string) => Promise<void>;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1essage", "output": "```typescript\ninterface Message {\n  id?: string;\n  role: 'system' | 'ai' | 'user';\n  text: string;\n  time: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1og \\1ntry", "output": "```typescript\ninterface LogEntry {\n  id: string;\n  time: string;\n  message: string;\n  type: 'info' | 'success' | 'warning' | 'error';\n  count: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1rofile", "output": "```typescript\ninterface Profile {\n  id: string;\n  name: string;\n  display_name: string;\n  description: string;\n  model: string;\n  estimated_vram_gb: number;\n  icon: string;\n  color: string;\n  is_active: boolean;\n  is_loading: boolean;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1 \\1 \\1 \\1tats", "output": "```typescript\ninterface GPUStats {\n  vram_used_gb: number;\n  vram_total_gb: number;\n  vram_percent: number;\n  temperature_c: number;\n  utilization_percent?: number;\n  power_draw_w?: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1 \\1 \\1essage", "output": "```typescript\ninterface WSMessage {\n  type: WSMessageType;\n  data?: any;\n  error?: string;\n  profile?: string;\n  timestamp?: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1obile \\1ctivity \\1tate", "output": "```typescript\ninterface MobileActivityState {\n  status: 'idle' | 'processing' | 'complete';\n  profile: string;\n  userPrompt: string;\n  streamingText: string;\n  tokensGenerated: number;\n  lastQuestion: string;\n  lastAnswer: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1low \\1onnection", "output": "```typescript\ninterface FlowConnection {\n  id: string;\n  from: string;\n  to: string;\n  active: boolean;\n  progress: number;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ystem \\1tatus", "output": "```typescript\ninterface SystemStatus {\n  backend_online: boolean;\n  ollama_online: boolean;\n  gpu_available: boolean;\n  active_profile: string | null;\n  last_error: string | null;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1onversation \\1ontext", "output": "```typescript\ninterface ConversationContext {\n  session_id?: string;\n  messages: Message[];\n  active_profile: string;\n  started_at: string;\n  last_activity: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1treaming \\1tats", "output": "```typescript\ninterface StreamingStats {\n  tokens_generated: number;\n  tokens_per_second: number;\n  time_elapsed_ms: number;\n  is_streaming: boolean;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ar \\1oom \\1vent", "output": "```typescript\ninterface WarRoomEvent {\n  id: string;\n  type: 'agent_join' | 'agent_leave' | 'agent_speak' | 'consensus' | 'vote';\n  agent_id: string;\n  timestamp: string;\n  data?: any;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1gent \\1etrics", "output": "```typescript\ninterface AgentMetrics {\n  profile_id: string;\n  total_requests: number;\n  total_tokens: number;\n  average_latency_ms: number;\n  success_rate: number;\n  last_used: string;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1 \\1 \\1tate", "output": "```typescript\ninterface WSState {\n  ws: WebSocket | null;\n  isConnected: boolean;\n  reconnectAttempts: number;\n\n  // Actions\n  connect: () => void;\n  disconnect: () => void;\n  send: (data: object) => void;\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1hat \\1tate", "output": "```typescript\ninterface ChatState {\n  messages: Message[];\n  currentResponse: string;\n  isStreaming: boolean;\n  selectedProfile: string;\n  isModelLoading: boolean;\n\n  // Actions\n  addMessage: (message: Omit<Message, 'id'>) => void;\n  setSelectedProfile: (profile: string) => void;\n  setIsModelLoading: (loading: boolean) => void;\n  handleWSMessage: (data: any) => void;\n  sendMessage: (text: string) => void;\n  warmupModel: (profileId: string) => Promise<void>;\n  clearMessages: () => void;\n}\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nexport const STATUS_COLORS = {\n  active: '#22c55e',    // Green\n  loading: '#f59e0b',   // Amber\n  idle: 'rgba(255,255,255,0.4)'\n}\n```\n\nKomponenten finns i `frontend/src/config/KioskConfig.ts`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ode \\1osition", "output": "```typescript\ninterface NodePosition {\n  id: string\n  position: Vector3Tuple\n  label: string\n  color: string\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1onnection \\1onfig", "output": "```typescript\ninterface ConnectionConfig {\n  id: string\n  from: string\n  to: string\n  color: string\n  particleCount?: number\n  active: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1etwork \\1tate", "output": "```typescript\ninterface NetworkState {\n  nodes: AgentNode[]\n  connections: ConnectionConfig[]\n  activeAgent: string | null\n}\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst NODE_CONFIG = [\n    { id: 'orchestrator', name: 'ORCHESTRATOR', type: 'orchestrator', color: '#f472b6', subLabel: 'ROUTING' }\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1ontext \\1enu", "output": "```typescript\nconst ContextMenu = ({ x, y, onClose, onAction }\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1gent \\1ode", "output": "```typescript\nconst AgentNode = ({ \n    config, \n    isActive, \n    isThinking,\n    onSelect,\n    isSolo,\n    isMuted,\n    onToggleMute,\n    onToggleSolo\n}\n```\n\nKomponenten finns i `frontend/src/Frontend_Agentpanel.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst THEME = {\n    black: '#020204',\n    cyan: '#00f0ff',        // QWEN-SONNET\n    purple: '#bd00ff',      // DEVSTRAL-OPENHANDS\n    steel: '#d0d8e0',       // QWEN-OPENHANDS (ljusgra)\n    steelEmissive: '#c0c8ff', // QWEN-OPENHANDS emissive\n    white: '#ffffff',\n    warning: '#ff0055'\n}\n```\n\nKomponenten finns i `frontend/src/Frontend_3D_Bakgrund.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst AGENT_COLORS = {\n    'qwen-sonnet': new THREE.Color('#00f0ff'),      // Cyan\n    'devstral-openhands': new THREE.Color('#bd00ff'), // Magenta\n    'qwen-openhands': new THREE.Color('#d0d8e0'),   // Gray\n    cloud: new THREE.Color('#38bdf8'),               // Sky blue\n    default: new THREE.Color('#00f0ff')              // Default to QWEN-SONNET\n}\n```\n\nKomponenten finns i `frontend/src/SynapticStream.tsx`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ynaptic \\1tream \\1rops", "output": "```typescript\ninterface SynapticStreamProps {\n    isTyping: boolean\n    activeAgent: string | null\n    startPos?: [number, number, number] // Orb Position\n    endPos?: [number, number, number]   // Chat Panel Position\n}\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1_ \\1 \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst AI_MODULES = [\n  { id: 'qwen-sonnet', name: 'QWEN-SONNET', type: 'DAILY' }\n```\n\nKomponenten finns i `frontend/src/Frontend_Huvudsida.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1ystem \\1ontrol \\1ard", "output": "```typescript\nconst SystemControlCard = lazy(() => import('./components/SystemControlCard'))\n\n// Loading spinner for Suspense fallback\nconst LoadingSpinner = () => (\n  <div className=\"flex flex-col h-full justify-center items-center\">\n    <div className=\"w-16 h-16 border-4 border-cyan-500/30 border-t-cyan-400 rounded-full animate-spin\" />\n    <p className=\"text-cyan-400 text-lg font-mono mt-4\">Loading System Control...</p>\n  </div>\n)\n\n// Ripple Effect Component\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1oading \\1pinner", "output": "```typescript\nconst LoadingSpinner = () => (\n  <div className=\"flex flex-col h-full justify-center items-center\">\n    <div className=\"w-16 h-16 border-4 border-cyan-500/30 border-t-cyan-400 rounded-full animate-spin\" />\n    <p className=\"text-cyan-400 text-lg font-mono mt-4\">Loading System Control...</p>\n  </div>\n)\n\n// Ripple Effect Component\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1ipple \\1ffect", "output": "```typescript\nconst RippleEffect = ({ ripples }\n```\n\nKomponenten finns i `frontend/src/KioskDashboard.tsx`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1eural \\1ode \\1lock \\1rops", "output": "```typescript\ninterface NeuralCodeBlockProps {\n  code: string\n  language?: string\n  isStreaming?: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1article", "output": "```typescript\ninterface Particle {\n  x: number\n  y: number\n  vx: number\n  vy: number\n  size: number\n  connections: number[]\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1onfetti", "output": "```typescript\ninterface Confetti {\n  id: number\n  x: number\n  y: number\n  color: string\n  rotation: number\n  scale: number\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ystem \\1ontrol \\1ard \\1rops", "output": "```typescript\ninterface SystemControlCardProps {\n  addLog: (message: string, type: 'info' | 'success' | 'warning' | 'error') => void\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1laude \\1tats", "output": "```typescript\ninterface ClaudeStats {\n  success: boolean\n  stats?: {\n    tokens: number\n    cost: number\n    models: string[]\n    input_tokens: number\n    output_tokens: number\n    cache_read: number\n    cache_create: number\n  }\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ode \\1lock \\1rops", "output": "```typescript\ninterface CodeBlockProps {\n  code: string\n  language?: string\n  isStreaming?: boolean\n}\n```"}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1arsed \\1egment", "output": "```typescript\ninterface ParsedSegment {\n  type: 'text' | 'code'\n  content: string\n  language?: string\n}\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1peech \\1ecognition", "output": "```typescript\nconst SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition\n\n    if (!SpeechRecognition) {\n      setErrorMsg('R√∂st st√∂ds ej')\n      onLog?.('Speech Recognition not available', 'error')\n      setState('error')\n      setTimeout(() => setState('idle'), 3000)\n      return\n    }\n```\n\nKomponenten finns i `frontend/src/components/VoiceButton.tsx`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1oice \\1utton \\1rops", "output": "```typescript\ninterface VoiceButtonProps {\n  onLog?: (message: string, type: 'info' | 'success' | 'warning' | 'error') => void\n  backendUrl: string\n  selectedProfile?: string  // Agent profile (qwen-sonnet, devstral-openhands, cloud)\n}\n```"}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst POSITIONS = {\n  orchestrator: [0, 0, 0] as [number, number, number],\n  // QWEN-SONNET (cyan cube) - upper left\n  'qwen-sonnet': [-3, 1.5, -2] as [number, number, number],\n  // DEVSTRAL-OPENHANDS (magenta icosahedron) - upper right\n  'devstral-openhands': [3, 1, -2] as [number, number, number],\n  // QWEN-OPENHANDS (steel dodecahedron) - lower center\n  'qwen-openhands': [0, -2.5, -3] as [number, number, number],\n  // Legacy positions for backwards compatibility\n  'gpt-oss': [-3, 1.5, -2] as [number, number, number],\n  'devstral': [3, 1, -2] as [number, number, number],\n}\n```\n\nKomponenten finns i `frontend/src/components/3d/AgentGraph.tsx`."}
{"instruction": "Skapa en React-komponent f√∂r \\1 \\1 \\1 \\1 \\1 \\1", "output": "```typescript\nconst COLORS = {\n  orchestrator: '#ffffff',\n  gptOss: '#00f0ff',\n  devstral: '#ff00ff'\n}\n```\n\nKomponenten finns i `frontend/src/components/3d/AgentNetworkScene.tsx`."}
{"instruction": "Definiera ett TypeScript interface f√∂r \\1ode \\1phere \\1rops", "output": "```typescript\ninterface NodeSphereProps {\n  position: [number, number, number]\n  color: string\n  active: boolean\n}\n```"}
{"instruction": "Vad √§r Table of Contents?", "output": "1. [System Architecture](#1-system-architecture)\n2. [Technology Stack](#2-technology-stack)\n3. [Component Details](#3-component-details)\n4. [API Reference](#4-api-reference)\n5. [Database & State](#5-database--state)\n6. [Development Setup](#6-development-setup)\n7. [Deployment](#7-deployment)\n8. [Troubleshooting](#8-troubleshooting)\n9. [Performance Optimization](#9-performance-optimization)\n10. [Security](#10-security)\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Overview?", "output": "Simons AI is a local AI assistant system running on RTX 4070 GPU with a multi-client architecture:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                        SIMONS AI SYSTEM                          ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                  ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ   ‚îÇ  MOBILE  ‚îÇ        ‚îÇ  SERVER  ‚îÇ        ‚îÇDASHBOARD ‚îÇ         ‚îÇ\n‚îÇ   ‚îÇController‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇBroadcaster‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Monitor  ‚îÇ         ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ        ‚îÇ                   ‚îÇ                    ‚îÇ               ‚îÇ\n‚îÇ   React Native        Python/FastAPI        React Web           ‚îÇ\n‚îÇ   Port: N/A           Port: 8000            Port: 5173/kiosk    ‚îÇ\n‚îÇ   Whisper: 8001       Ollama: 11434                             ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ\n‚îÇ   ‚îÇ DESKTOP  ‚îÇ        (Separate session, does not mirror mobile)‚îÇ\n‚îÇ   ‚îÇWorkstation‚îÇ                                                 ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ\n‚îÇ   Port: 5173/                                                   ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Component Roles?", "output": "| Component | Role | Purpose |\n|-----------|------|---------|\n| **Backend** | Broadcaster | Runs AI agents, broadcasts events to all clients |\n| **Mobile** | Controller | Voice/text input, sends commands to backend |\n| **Dashboard** | Monitor | Passive display for visualization (Nest Hub) |\n| **Desktop** | Workstation | Full IDE with advanced chat features |\n| **Whisper** | Voice-to-Text | Converts speech to text |\n| **Ollama** | LLM Runtime | Runs QWEN models locally on GPU |\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Backend?", "output": "- **Framework**: FastAPI (Python)\n- **WebSocket**: Native WebSocket for real-time communication\n- **LLM**: Ollama with QWEN 3 14B model\n- **Voice**: OpenAI Whisper (small model)\n- **Process Management**: systemd user services\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Frontend (Desktop & Dashboard)?", "output": "- **Framework**: React + TypeScript\n- **Build Tool**: Vite\n- **State Management**: Zustand\n- **3D Graphics**: Three.js for WebGL backgrounds\n- **Code Editor**: Monaco Editor / CodeMirror\n- **Styling**: TailwindCSS + Glassmorphism\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Mobile?", "output": "- **Framework**: React Native (Expo 52)\n- **State Management**: Zustand\n- **WebSocket**: Native WebSocket\n- **Voice Input**: Integration with Whisper server\n- **UI**: Native Animations (Animated API)\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r Infrastructure i Simon's AI", "output": "- **GPU**: NVIDIA RTX 4070 (12 GB VRAM)\n- **OS**: Linux with NVIDIA drivers\n- **Network**: Static IP 192.168.86.32\n- **Storage**: ~15 GB for AI models\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 3.1 Backend (Python/FastAPI)?", "output": "**Location**: `/app/`\n\n**Key Files**:\n- `main.py` - FastAPI application entry point\n- `api/Backend_Chat_Stream.py` - WebSocket + War Room events\n- `api/Backend_Agent_Prompts.py` - AI agent profiles\n- `api/routes.py` - REST endpoints\n- `config.py` - Configuration management\n\n**Responsibilities**:\n- Handle WebSocket connections from all clients\n- Run AI agents via Ollama\n- Broadcast `war_room_event` to all dashboards\n- Manage SystemTools (file operations, shell commands)\n- GPU monitoring and telemetry\n\n**Port**: 8000\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r 3.2 Frontend (React Web)?", "output": "**Location**: `/frontend/`, `/frontend-next/`\n\n**Key Files**:\n- `src/App.tsx` - Router and main application\n- `src/Frontend_Huvudsida.tsx` - Desktop chat interface\n- `src/Frontend_3D_Bakgrund.tsx` - Three.js 3D backgrounds\n- `src/KioskDashboard.tsx` - Nest Hub dashboard\n\n**Features**:\n- Desktop: Full IDE with file browser and code editor\n- Dashboard: Passive monitor with GPU telemetry\n- 3D WebGL backgrounds for visual appeal\n- Glassmorphism design (\"Antigravity\")\n\n**Port**: 5173 (Vite dev server)\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r 3.3 Mobile (React Native/Expo)?", "output": "**Location**: `/mobile/`, `/simonsai-mobile-expo/`\n\n**Key Files**:\n- `App.tsx` - Main application entry\n- `src/hooks/useWarRoomSocket.ts` - WebSocket management\n- `src/hooks/useVoiceInput.ts` - Voice input handling\n- `src/constants/config.ts` - Configuration\n\n**Features**:\n- Voice control via Whisper\n- Text input via WebSocket\n- \"The Monolith\" 3D UI (Native Views, not SVG)\n- Safe areas for Android navigation\n\n**Special Notes**:\n- NO heavy SVG libraries (crash risk on Android 16)\n- Use native animations with `useNativeDriver: true`\n- Function first design philosophy\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r 3.4 AI Models?", "output": "**Available Profiles**:\n- **THINK** (QWEN_NEXUS) - Deep analysis, complex code, architecture\n- **CHILL** (QWEN_LITE) - Quick chat, simple help\n- **Nerdy** - Code-focused assistant\n- **Devstral** - Development tasks\n- **Qwen-sonnet** - Balanced performance\n\n**Model**: qwen3:14b (~9 GB VRAM)\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.1 WebSocket API?", "output": "**Endpoint**: `ws://localhost:8000/api/chat`\n\n**Client ‚Üí Server**:\n```json\n{\n  \"text\": \"User message\",\n  \"profile\": \"QWEN_NEXUS\"\n}\n```\n\n**Server ‚Üí Client** (Streaming):\n```json\n{\n  \"sender\": \"agent\",\n  \"text\": \"Response chunk\",\n  \"is_finished\": false\n}\n```\n\n**War Room Events** (Broadcast to all):\n```json\n{\n  \"type\": \"war_room_event\",\n  \"event\": \"start|stream|end\",\n  \"data\": {\n    \"text\": \"...\",\n    \"profile\": \"...\",\n    \"tokens\": [\"...\"]\n  }\n}\n```\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.2 REST API?", "output": "**Base URL**: `http://localhost:8000`\n\n#### GET `/api/profiles`\nList all available AI agent profiles.\n\n**Response**:\n```json\n[\n  {\n    \"id\": \"QWEN_NEXUS\",\n    \"name\": \"THINK\",\n    \"description\": \"Deep analysis\"\n  }\n]\n```\n\n#### POST `/api/profiles/{id}/warmup`\nPre-load AI model into memory.\n\n#### GET `/api/gpu/stats`\nGet GPU telemetry.\n\n**Response**:\n```json\n{\n  \"vram_used\": 9012,\n  \"vram_total\": 12288,\n  \"gpu_temp\": 65,\n  \"gpu_util\": 87\n}\n```\n\n#### POST `/api/voice-command`\nProcess voice command via Whisper + QWEN.\n\n**Request**:\n```json\n{\n  \"audio\": \"base64_encoded_audio\"\n}\n```\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.3 Whisper Server API?", "output": "**Base URL**: `http://localhost:8001`\n\n#### GET `/health`\nHealth check endpoint.\n\n#### POST `/transcribe`\nTranscribe audio to text.\n\n**Request**: `multipart/form-data` with audio file\n\n**Response**:\n```json\n{\n  \"text\": \"Transcribed text\"\n}\n```\n\n#### POST `/voice-command`\nTranscribe + send to QWEN agent.\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r State Management?", "output": "**Frontend**: Zustand stores\n- Chat history per session\n- Active agent profile\n- GPU telemetry\n- War room events\n\n**Mobile**: Zustand stores\n- Voice input state\n- WebSocket connection status\n- Message queue\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Persistence?", "output": "Currently using **in-memory** state. No persistent database.\n\nFor future persistence:\n- Consider SQLite for chat history\n- Redis for session management\n- File-based storage for agent memory\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r System requirements?", "output": "- NVIDIA RTX 4070 (12 GB VRAM)\n- Linux with NVIDIA drivers\n- ~15 GB disk space\n- Node.js 18+\n- Python 3.10+\n```\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Beskriv Development Workflow processen", "output": "**Backend**:\n```bash\ncd app/\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\nuvicorn main:app --reload --port 8000\n```\n\n**Frontend**:\n```bash\ncd frontend/\nnpm install\nnpm run dev\n```\n\n**Mobile**:\n```bash\ncd simonsai-mobile-expo/\nnpm install\nnpm start\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Enable services?", "output": "systemctl --user enable simons-ai-backend\nsystemctl --user enable simons-ai-frontend\nloginctl enable-linger ai-server\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Hur konfigurerar jag Port Configuration?", "output": "**Server IP**: `192.168.86.32` (DHCP-reserved)\n\n| Port | Service | Systemd Unit |\n|------|---------|--------------|\n| 5173 | Frontend (Vite) | simons-ai-frontend |\n| 8000 | Backend (FastAPI) | simons-ai-backend |\n| 8001 | Whisper Server | manual |\n| 11434 | Ollama | ollama.service |\n| 3000 | STABLE BACKUP | **NEVER CHANGE** |\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Or manually:?", "output": "systemctl --user restart simons-ai-backend\njournalctl --user -u simons-ai-backend -f\n```\n\n**WebSocket disconnects**:\n- Check `ping_timeout` settings (currently disabled to prevent disconnects)\n- Verify network connectivity\n- Check backend logs for errors\n\n**GPU out of memory**:\n- Only one model loaded at a time (qwen3:14b uses ~9 GB)\n- Check GPU usage: `nvidia-smi`\n- Restart Ollama: `systemctl restart ollama`\n\n**Mobile app crashes**:\n- Avoid heavy SVG libraries on Android 16\n- Use `useNativeDriver: true` for animations\n- Check React Native logs: `npm start` ‚Üí press 'r' to reload\n\n**Voice input not working**:\n- Ensure Whisper server is running on port 8001\n- Check audio permissions on mobile\n- Test with `curl http://localhost:8001/health`\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r GPU Optimization?", "output": "- Pre-warm models with `/api/profiles/{id}/warmup`\n- Use fp16 quantization for models (already default in Ollama)\n- Monitor VRAM usage to prevent OOM\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r WebSocket Optimization?", "output": "- Token batching (100ms intervals) for streaming\n- Use binary messages for large payloads\n- Implement reconnection logic with exponential backoff\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Frontend Optimization?", "output": "- Lazy load components with React.lazy()\n- Use WebGL for 3D graphics (Three.js)\n- Virtualize long lists\n- Code splitting for faster initial load\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Mobile Optimization?", "output": "- Native animations with `useNativeDriver`\n- Avoid SVG for complex graphics\n- Optimize bundle size with Hermes\n- Use React.memo for expensive components\n\n---\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Security Recommendations?", "output": "- [ ] Add authentication for WebSocket connections\n- [ ] Implement rate limiting on API endpoints\n- [ ] Use HTTPS/WSS for production\n- [ ] Add input validation and sanitization\n- [ ] Implement CORS policy\n- [ ] Add API key management for multi-user scenarios\n- [ ] Regular security audits of dependencies\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r File Structure i Simon's AI", "output": "```\nwar-room-sync-simonsai/\n‚îú‚îÄ‚îÄ app/                          # Backend (Python/FastAPI)\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Backend_Chat_Stream.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Backend_Agent_Prompts.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îî‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ frontend/                     # Desktop + Dashboard (React)\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ App.tsx\n‚îÇ       ‚îú‚îÄ‚îÄ Frontend_Huvudsida.tsx\n‚îÇ       ‚îú‚îÄ‚îÄ Frontend_3D_Bakgrund.tsx\n‚îÇ       ‚îî‚îÄ‚îÄ KioskDashboard.tsx\n‚îú‚îÄ‚îÄ mobile/                       # React Native app\n‚îÇ   ‚îú‚îÄ‚îÄ App.tsx\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ hooks/\n‚îÇ       ‚îú‚îÄ‚îÄ components/\n‚îÇ       ‚îî‚îÄ‚îÄ constants/\n‚îú‚îÄ‚îÄ simonsai-mobile-expo/         # Expo mobile app\n‚îú‚îÄ‚îÄ systemd/                      # Systemd service files\n‚îú‚îÄ‚îÄ scripts/                      # Utility scripts\n‚îú‚îÄ‚îÄ docs/                         # Documentation\n‚îú‚îÄ‚îÄ tests/                        # Test files\n‚îú‚îÄ‚îÄ ARCHITECTURE_MASTER.md        # Architecture overview\n‚îú‚îÄ‚îÄ CHANGELOG.md                  # Version history\n‚îú‚îÄ‚îÄ DEEP_DOCS.md                  # This file\n‚îú‚îÄ‚îÄ FELS√ñKNING.md                 # Troubleshooting (Swedish)\n‚îú‚îÄ‚îÄ README.md                     # Quick start guide\n‚îú‚îÄ‚îÄ install.sh                    # Installation script\n‚îú‚îÄ‚îÄ start.sh                      # Start all services\n‚îú‚îÄ‚îÄ stop.sh                       # Stop all services\n‚îú‚îÄ‚îÄ status.sh                     # Check service status\n‚îú‚îÄ‚îÄ fix.sh                        # Auto-fix common issues\n‚îî‚îÄ‚îÄ simons-ai                     # CLI management tool\n```\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Deprecated Features?", "output": "Do NOT use these deprecated concepts:\n- ‚ùå \"Breathing orb\" / SVG animations (mobile crash risk)\n- ‚ùå Geofencing\n- ‚ùå Socket.IO (use native WebSocket)\n- ‚ùå \"03_NEON-UNICORN-AI\" (old reference)\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Vad √§r Related Documentation?", "output": "- [README.md](README.md) - Quick start guide\n- [ARCHITECTURE_MASTER.md](ARCHITECTURE_MASTER.md) - System architecture\n- [FELS√ñKNING.md](FELS√ñKNING.md) - Troubleshooting (Swedish)\n- [CHANGELOG.md](CHANGELOG.md) - Version history\n\n---\n\n**Last Updated**: 2025-12-03\n**Maintained By**: Claude AI Assistant\n**License**: MIT\n\n*K√§lla: DEEP_DOCS.md*"}
{"instruction": "Visa hur man ### cli management", "output": "```bash\n# Status check\n./simons-ai status\n\n# Restart all\n./simons-ai restart\n\n# Rebuild frontend\n./simons-ai rebuild\n\n# View logs\n./simons-ai logs\n```"}
{"instruction": "Visa hur man ### log files", "output": "```bash\n# Backend logs\njournalctl --user -u simons-ai-backend -f\n\n# Frontend logs (if using systemd)\njournalctl --user -u simons-ai-frontend -f\n\n# Ollama logs\njournalctl -u ollama -f\n```"}
{"instruction": "Vad √§r A. SYSTEM√ñVERSIKT - The Triad?", "output": "```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                        SIMONS AI                                 ‚îÇ\n‚îÇ                      \"The Triad\"                                 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                  ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n‚îÇ   ‚îÇ  MOBILE  ‚îÇ        ‚îÇ  SERVER  ‚îÇ        ‚îÇDASHBOARD ‚îÇ         ‚îÇ\n‚îÇ   ‚îÇController‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇBroadcaster‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Monitor  ‚îÇ         ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n‚îÇ        ‚îÇ                   ‚îÇ                    ‚îÇ               ‚îÇ\n‚îÇ   React Native        Python/FastAPI        React Web           ‚îÇ\n‚îÇ   Port: N/A           Port: 8000            Port: 5173/kiosk    ‚îÇ\n‚îÇ   Whisper: 8001       Ollama: 11434                             ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                  ‚îÇ\n‚îÇ   ‚îÇ DESKTOP  ‚îÇ        (Separat session, ej speglar mobil)       ‚îÇ\n‚îÇ   ‚îÇWorkstation‚îÇ                                                 ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                  ‚îÇ\n‚îÇ   Port: 5173/                                                   ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r Komponenter?", "output": "| Komponent | Roll | Tech Stack | Port |\n|-----------|------|------------|------|\n| **Backend** | Broadcaster | Python/FastAPI + WebSocket | 8000 |\n| **Mobile** | Controller | React Native (Expo 52) | - |\n| **Dashboard** | Monitor | React (Kiosk mode) | 5173/kiosk |\n| **Desktop** | Workstation | React (Full IDE) | 5173/ |\n| **Whisper** | Voice-to-Text | OpenAI Whisper (small) | 8001 |\n| **Ollama** | LLM Runtime | QWEN 3 14B | 11434 |\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r Roller f√∂rklarade?", "output": "- **Backend (Broadcaster)**:\n  - K√∂r QWEN/Nerdy-agenterna via Ollama\n  - Tar emot input fr√•n Mobile/Desktop\n  - Broadcastar `war_room_event` till alla dashboards\n  - Hanterar SystemTools (fil, shell)\n\n- **Mobile (Controller)**:\n  - R√∂ststyrning via Whisper\n  - Text-input via WebSocket\n  - \"The Monolith\" 3D UI (Native Views, ej SVG)\n  - Skickar `{text, profile}` till backend\n\n- **Dashboard (Monitor)**:\n  - Passiv sk√§rm (Nest Hub)\n  - Lyssnar p√• `war_room_event` f√∂r visualisering\n  - Visar GPU-telemetri\n  - Matrix-terminal stil\n\n- **Desktop (Workstation)**:\n  - **EGEN session (speglar EJ mobilen!)**\n  - Full IDE med Monaco/CodeMirror\n  - Avancerad chatt med fil-hantering\n  - Delar filsystem med backend\n\n---\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r WebSocket Events?", "output": "| Event | Riktning | Beskrivning |\n|-------|----------|-------------|\n| `{text, profile}` | Client ‚Üí Server | Chat-meddelande (Antigravity) |\n| `{sender: \"agent\", text, is_finished}` | Server ‚Üí Client | Streaming svar |\n| `war_room_event: start` | Server ‚Üí ALL | Mobil-session startar |\n| `war_room_event: stream` | Server ‚Üí ALL | Token-batch (100ms) |\n| `war_room_event: end` | Server ‚Üí ALL | Session klar |\n| `status_update` | Server ‚Üí ALL | GPU-telemetri |\n| `mobile_activity` | Server ‚Üí ALL | Legacy (voice commands) |\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Hur fungerar HTTP Endpoints?", "output": "| Endpoint | Method | Beskrivning |\n|----------|--------|-------------|\n| `/api/chat` | WebSocket | Huvudchatt |\n| `/api/voice-command` | POST | Whisper + QWEN |\n| `/api/profiles` | GET | Lista agenter |\n| `/api/profiles/{id}/warmup` | POST | Ladda modell |\n| `/api/gpu/stats` | GET | GPU-status |\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r Whisper Server (Port 8001)?", "output": "| Endpoint | Method | Beskrivning |\n|----------|--------|-------------|\n| `/health` | GET | Status-check |\n| `/transcribe` | POST | Endast transkribering |\n| `/voice-command` | POST | Transkribera + skicka till QWEN |\n\n---\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r Desktop?", "output": "- \"Antigravity\" design (Glassmorphism)\n- Full IDE med fil-browser\n- Monaco/CodeMirror f√∂r kod\n- 3D WebGL bakgrund (Three.js)\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r Dashboard (Kiosk)?", "output": "- High contrast f√∂r l√§sbarhet\n- Stor text (touch-friendly)\n- Matrix terminal-stil\n- Auto-flip mellan Agent Cards och Mobile Activity\n\n---\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r D. FILER & STRUKTUR i Simon's AI", "output": "```\n01_AI-VIBE-WORLD/\n‚îú‚îÄ‚îÄ app/                          # Backend\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Backend_Chat_Stream.py   # WebSocket + War Room\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Backend_Agent_Prompts.py # Agent profiles\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py                # REST endpoints\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îî‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ frontend/                     # Desktop + Dashboard\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ App.tsx                  # Router\n‚îÇ       ‚îú‚îÄ‚îÄ Frontend_Huvudsida.tsx   # Desktop chat\n‚îÇ       ‚îú‚îÄ‚îÄ Frontend_3D_Bakgrund.tsx # Three.js\n‚îÇ       ‚îî‚îÄ‚îÄ KioskDashboard.tsx       # Nest Hub\n‚îú‚îÄ‚îÄ mobile/                       # React Native app\n‚îÇ   ‚îú‚îÄ‚îÄ App.tsx\n‚îÇ   ‚îî‚îÄ‚îÄ src/\n‚îÇ       ‚îú‚îÄ‚îÄ hooks/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ useWarRoomSocket.ts\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ useVoiceInput.ts\n‚îÇ       ‚îú‚îÄ‚îÄ components/\n‚îÇ       ‚îî‚îÄ‚îÄ constants/config.ts\n‚îú‚îÄ‚îÄ simons-ai                     # CLI management tool\n‚îú‚îÄ‚îÄ ARCHITECTURE_MASTER.md        # THIS FILE\n‚îî‚îÄ‚îÄ docs/DEPRECATED/              # Old documentation\n```\n\n---\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r E. PORTAR & IP?", "output": "**Server IP:** `192.168.86.32` (DHCP-reserverad)\n\n| Port | Tj√§nst | Systemd Service |\n|------|--------|-----------------|\n| 5173 | Frontend (Vite) | simons-ai-frontend |\n| 8000 | Backend (FastAPI) | simons-ai-backend |\n| 8001 | Whisper Server | manuell |\n| 11434 | Ollama | ollama |\n| 3000 | STABLE BACKUP | **ALDRIG √ÑNDRA** |\n\n---\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad √§r G. DEPRECATED CONCEPTS?", "output": "F√∂ljande begrepp/features √§r **UTDATERADE** och ska ignoreras:\n\n- ‚ùå \"Breathing orb\" / SVG-animationer\n- ‚ùå Geofencing\n- ‚ùå \"03_NEON-UNICORN-AI\" som mobilapp (gammal referens)\n- ‚ùå Socket.IO (vi anv√§nder native WebSocket)\n\n---\n\n*Skapad: 2025-12-01 som ers√§ttning f√∂r CLAUDE.md, PROJECT.md, SERVER.md*\n\n*K√§lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Visa hur man ## f. kommandon", "output": "```bash\n# Status\n./simons-ai status\n\n# Omstart\n./simons-ai restart\n\n# Rebuild frontend\n./simons-ai rebuild\n\n# Loggar\n./simons-ai logs\n```"}
{"instruction": "Vad √§r Changelog?", "output": "All notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r Added?", "output": "- CHANGELOG.md and DEEP_DOCS.md for better project documentation\n- üé® Shiki code blocks med catppuccin-mocha tema (ers√§tter react-syntax-highlighter)\n- üß† LoRA Fine-Tuning setup: 37 tr√§ningsexempel fr√•n n8n workflows (scripts/training_data.jsonl)\n- üìö `orange-md` skill f√∂r CLAUDE.md best practices\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r Changed?", "output": "- üîß Prompt-arkitektur: EN k√§lla (Modelfiles √§ger prompten, backend injicerar inget)\n- Backend_Fraga_Router.py: 772 ‚Üí 322 rader (alla prompts borttagna)\n- Backend_Agent_Prompts.py: Rensat fr√•n prompts, beh√•ller bara UI-metadata\n- ~/CLAUDE.md trimmat: 193 ‚Üí 59 rader med referenser till SERVER-MAINTENANCE.md\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r Fixed?", "output": "- üêõ Modeller fr√•gade ist√§llet f√∂r att koda - lade till \"FR√ÖGA ALDRIG\" i alla Modelfiles\n- üêõ Prompt-konflikt mellan Modelfile och runtime API (nu endast Modelfile)\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r Removed?", "output": "- =12.0 file (pip error output)\n- frontend_backup_20251126_190923 folder (old backup)\n- simonsai-mobile-link (1) duplicate folder\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r How to Update This Changelog?", "output": "When making changes, add them under the `[Unreleased]` section in the appropriate category:\n\n- **Added** for new features\n- **Changed** for changes in existing functionality\n- **Deprecated** for soon-to-be removed features\n- **Removed** for now removed features\n- **Fixed** for any bug fixes\n- **Security** for vulnerability fixes\n\nWhen releasing a new version, move items from `[Unreleased]` to a new version section with the date.\n\n*K√§lla: CHANGELOG.md*"}
{"instruction": "Vad √§r CLAUDE.md?", "output": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Vad √§r detta??", "output": "Simons AI - Lokal AI-assistent p√• RTX 4070 (12GB VRAM). Multi-client chat med WebSocket-broadcasting till Desktop, Dashboard (Kiosk) och Mobile.\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Frontend (React/Vite)?", "output": "```bash\ncd frontend\nnpm run dev      # Dev server p√• port 5173\nnpm run build    # Bygg f√∂r produktion\nnpm run lint     # ESLint\n```\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Tester?", "output": "```bash\npytest tests/                    # Backend\ncd frontend && npm test          # Frontend (om konfigurerat)\n```\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r CLI Management?", "output": "```bash\n./simons-ai status    # Visa tj√§nstestatus\n./simons-ai restart   # Starta om alla tj√§nster\n./simons-ai rebuild   # Bygg om frontend\n./simons-ai logs      # Visa loggar\n./fix.sh              # Automatisk fels√∂kning\n```\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r Arkitektur i Simon's AI", "output": "```\nBackend (Python/FastAPI) ‚îÄ‚îÄ‚îÄ‚îÄ Ollama (QWEN 3 14B) ‚îÄ‚îÄ‚îÄ‚îÄ GPU\n    ‚îÇ port 8000                  port 11434\n    ‚îÇ WebSocket: /api/chat\n    ‚îÇ WebSocket: /api/cascade (Multi-agent)\n    ‚îÇ\n    ‚îú‚îÄ‚ñ∫ Desktop (React)  ‚Üê‚îÄ‚îÄ Full IDE, egen session\n    ‚îÇ   port 5173/\n    ‚îÇ\n    ‚îú‚îÄ‚ñ∫ Dashboard (React) ‚Üê‚îÄ‚îÄ Passiv monitor, war_room_event\n    ‚îÇ   port 5173/kiosk\n    ‚îÇ\n    ‚îî‚îÄ‚ñ∫ Mobile (Expo) ‚Üê‚îÄ‚îÄ Controller, r√∂ststyrning\n        Whisper: port 8001\n```\n\n**Roller:**\n- Backend: Broadcaster - k√∂r AI-agenter, broadcastar events till alla klienter\n- Desktop: Workstation - full IDE med fil-browser (egen session, speglar EJ mobil)\n- Dashboard: Monitor - passiv visualisering f√∂r Nest Hub\n- Mobile: Controller - r√∂st/text-input via Whisper\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Backend (`app/`)?", "output": "| Fil | Syfte |\n|-----|-------|\n| `main.py` | FastAPI entrypoint, lifespan-hantering |\n| `api/Backend_Chat_Stream.py` | WebSocket endpoints + war_room_event broadcasting |\n| `api/routes.py` | REST endpoints |\n| `models/Backend_Agent_Prompts.py` | Agent-profiler (endast UI-metadata, prompts i Modelfiles) |\n| `services/ollama_client.py` | Ollama API-klient |\n| `services/orchestrator.py` | Multi-agent orkestrering |\n| `config.py` | Pydantic settings |\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Frontend (`frontend/src/`)?", "output": "| Fil | Syfte |\n|-----|-------|\n| `App.tsx` | Router |\n| `Frontend_Huvudsida.tsx` | Desktop chat-interface |\n| `Frontend_3D_Bakgrund.tsx` | Three.js WebGL bakgrund |\n| `KioskDashboard.tsx` | Nest Hub passiv monitor |\n| `stores/` | Zustand state management |\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Konventioner?", "output": "- **Spr√•k:** Svenska f√∂r konversation, engelska f√∂r kod\n- **Prompts:** EN k√§lla = Modelfiles. Backend skickar inte system-prompt.\n- **WebSocket:** Native WebSocket, inte Socket.IO\n- **State:** Zustand f√∂r frontend\n- **Styling:** TailwindCSS + Glassmorphism (\"Antigravity\")\n- **3D:** Three.js via @react-three/fiber\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Portar?", "output": "| Port | Tj√§nst |\n|------|--------|\n| 5173 | Frontend (Vite dev) |\n| 8000 | Backend (FastAPI) |\n| 8001 | Whisper (voice-to-text) |\n| 11434 | Ollama |\n| 3000 | STABLE BACKUP - **√ÑNDRA ALDRIG** |\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r Deprecated (ignorera dessa)?", "output": "- ‚ùå \"Breathing orb\" / SVG-animationer (crash p√• Android)\n- ‚ùå Socket.IO\n- ‚ùå Prompts i backend (anv√§nd Modelfiles)\n\n*K√§lla: CLAUDE.md*"}
{"instruction": "Vad √§r React + TypeScript + Vite?", "output": "This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n\n*K√§lla: frontend/README.md*"}
{"instruction": "Vad √§r React Compiler?", "output": "The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).\n\n*K√§lla: frontend/README.md*"}
{"instruction": "Hur konfigurerar jag Expanding the ESLint configuration?", "output": "If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:\n\n```js\nexport default defineConfig([\n  globalIgnores(['dist']),\n  {\n    files: ['**/*.{ts,tsx}'],\n    extends: [\n      // Other configs...\n\n      // Remove tseslint.configs.recommended and replace with this\n      tseslint.configs.recommendedTypeChecked,\n      // Alternatively, use this for stricter rules\n      tseslint.configs.strictTypeChecked,\n      // Optionally, add this for stylistic rules\n      tseslint.configs.stylisticTypeChecked,\n\n      // Other configs...\n    ],\n    languageOptions: {\n      parserOptions: {\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\n        tsconfigRootDir: import.meta.dirname,\n      },\n      // other options...\n    },\n  },\n])\n```\n\nYou can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:\n\n```js\n// eslint.config.js\nimport reactX from 'eslint-plugin-react-x'\nimport reactDom from 'eslint-plugin-react-dom'\n\nexport default defineConfig([\n  globalIgnores(['dist']),\n  {\n    files: ['**/*.{ts,tsx}'],\n    extends: [\n      // Other configs...\n      // Enable lint rules for React\n      reactX.configs['recommended-typescript'],\n      // Enable lint rules for React DOM\n      reactDom.configs.recommended,\n    ],\n    languageOptions: {\n      parserOptions: {\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\n        tsconfigRootDir: import.meta.dirname,\n      },\n      // other options...\n    },\n  },\n])\n```\n\n*K√§lla: frontend/README.md*"}
{"instruction": "Vad √§r WebSocket Chat?", "output": "```\nws://192.168.86.32:8000/api/chat\n```\n\n**Skicka meddelande:**\n```json\n{\n  \"type\": \"chat\",\n  \"profile\": \"gpt-oss\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hej!\"}],\n  \"request_id\": \"unique-id-123\"\n}\n```\n\n**Ta emot (streaming):**\n```json\n{\"type\": \"start\", \"agent_id\": \"gpt-oss\"}\n{\"type\": \"token\", \"content\": \"Hej\"}\n{\"type\": \"token\", \"content\": \"!\"}\n{\"type\": \"done\", \"stats\": {...}}\n```\n\n*K√§lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad √§r Tillg√§ngliga modeller?", "output": "| Profile ID | Modell | Beskrivning |\n|------------|--------|-------------|\n| `gpt-oss` | GPT-OSS 20B | Arkitekt - resonemang, planering |\n| `devstral` | Devstral 24B | Kodare - implementation |\n\n*K√§lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Hur fungerar REST Endpoints?", "output": "```\nGET  /api/health              # Systemstatus\nPOST /api/profiles/{id}/warmup # V√§rm upp modell\n```\n\n*K√§lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad √§r Viktigt f√∂r mobilappen?", "output": "1. **WebSocket-format:** Samma som ovan\n2. **Streaming:** Tokens kommer en i taget, buffra och visa\n3. **Profile-switch:** Skicka annat `profile`-v√§rde f√∂r att byta modell\n4. **Request ID:** Generera unikt ID per meddelande (UUID)\n\n*K√§lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad √§r Exempel: Minimal klient?", "output": "```javascript\nconst ws = new WebSocket('ws://192.168.86.32:8000/api/chat');\n\nws.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  if (data.type === 'token') {\n    // Visa token i chatten\n    appendToChat(data.content);\n  }\n};\n\n// Skicka meddelande\nws.send(JSON.stringify({\n  type: 'chat',\n  profile: 'gpt-oss',\n  messages: [{role: 'user', content: 'Hej!'}],\n  request_id: crypto.randomUUID()\n}));\n```\n\n*K√§lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Visa hur man minimal klient", "output": "```javascript\nconst ws = new WebSocket('ws://192.168.86.32:8000/api/chat');\n\nws.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  if (data.type === 'token') {\n    // Visa token i chatten\n    appendToChat(data.content);\n  }\n};\n\n// Skicka meddelande\nws.send(JSON.stringify({\n  type: 'chat',\n  profile: 'gpt-oss',\n  messages: [{role: 'user', content: 'Hej!'}],\n  request_id: crypto.randomUUID()\n}));\n```"}
{"instruction": "Vad √§r üîÑ √ñverl√§mning: V√§ck liv i AI-modellerna?", "output": "**Datum:** 2025-12-02\n**F√∂reg√•ende session:** Polish features f√∂r Kiosk Dashboard (auto-rotate, ripple, ljud, lazy-load)\n\n---\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur fungerar Backend (FastAPI) - FUNGERAR ‚úÖ?", "output": "- **URL:** http://192.168.86.32:8000\n- **Docs:** http://192.168.86.32:8000/docs\n- **WebSocket:** ws://192.168.86.32:8000/api/chat\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur s√§tter jag upp Ollama-modeller installerade?", "output": "```\nsven-gpt:latest    13 GB   ‚Üê GPT-OSS 20B (custom Modelfile) - anv√§nds i Ollama\ngpt-oss:20b        13 GB   ‚Üê Basmodell\ndevstral:24b       14 GB   ‚Üê Mistral kodmodell\n```\n\n**OBS:** Profile-IDs i backend/frontend √§r `gpt-oss` och `devstral`, men Ollama-modellnamnet f√∂r GPT-OSS √§r `sven-gpt`.\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r üèóÔ∏è Arkitektur i Simon's AI", "output": "```\nFrontend (React)\n    ‚Üì WebSocket\nBackend_Chat_Stream.py  ‚Üê Huvud-WebSocket endpoint\n    ‚Üì\nBackend_Fraga_Router.py ‚Üê Routar till r√§tt agent/modell\n    ‚Üì\nBackend_Agent_Prompts.py ‚Üê System prompts per agent\n    ‚Üì\nollama_client.py ‚Üê Ollama API-anrop\n```\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r Nyckelfilar?", "output": "| Fil | Ansvar |\n|-----|--------|\n| `app/api/Backend_Chat_Stream.py` | WebSocket, streaming, GPU telemetry |\n| `app/services/Backend_Fraga_Router.py` | Agent routing logic |\n| `app/models/Backend_Agent_Prompts.py` | Profile definitions & prompts |\n| `app/services/ollama_client.py` | Ollama HTTP client |\n| `app/services/gpu_monitor.py` | nvidia-smi parsing |\n\n---\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur fungerar Fr√•n /api/profiles endpoint:?", "output": "\"gpt-oss\"    ‚Üí model: \"sven-gpt\"   (13GB)  # Arkitekten (GPT-OSS 20B)\n\"devstral\"   ‚Üí model: \"devstral\"   (14GB)  # Kodaren (Devstral 24B)\n\"qwen\"       ‚Üí model: \"sven-gpt\"   (Legacy)\n```\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r Kiosk Dashboard visar?", "output": "- **GPT-OSS** (cyan) - id: `gpt-oss`\n- **Devstral** (gold) - id: `devstral`\n\n‚úÖ **Status:** Frontend och backend anv√§nder nu samma profile-IDs (gpt-oss, devstral)\n\n---\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r Warmup en modell (ladda i VRAM)?", "output": "curl -X POST http://192.168.86.32:8000/api/profiles/gpt-oss/warmup\ncurl -X POST http://192.168.86.32:8000/api/profiles/devstral/warmup\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r 1. Synka Frontend ‚Üî Backend profiles?", "output": "- Kolla `frontend/src/config/KioskConfig.ts`\n- Matcha agent IDs med backend `/api/profiles`\n- Uppdatera `KIOSK_AGENTS` om n√∂dv√§ndigt\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Beskriv 2. Verifiera warmup-fl√∂det processen", "output": "```\nKiosk \"Tap to Load\" ‚Üí handleAgentSelect()\n  ‚Üí POST /api/profiles/{id}/warmup  (ex: gpt-oss eller devstral)\n  ‚Üí ollama_client.warmup_model()\n  ‚Üí Ollama laddar modell i VRAM (sven-gpt eller devstral)\n```\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r 3. Testa chat-streaming?", "output": "```\nFrontend skickar via WebSocket:\n{\n  \"type\": \"chat_message\",\n  \"content\": \"Hej!\",\n  \"profile\": \"gpt-oss\"\n}\n\nBackend svarar med streaming tokens:\n{\n  \"type\": \"stream_token\",\n  \"token\": \"Hej\",\n  \"agent_id\": \"gpt-oss\"\n}\n```\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r 4. Koppla modeller i Kiosk?", "output": "- Testa att trycka p√• agent i Kiosk\n- Verifiera att modell faktiskt laddas (`ollama ps`)\n- Kolla att `is_active` uppdateras i frontend\n\n---\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r Testa modeller direkt i Ollama?", "output": "ollama run sven-gpt \"Hej, vem √§r du?\"  # GPT-OSS 20B\nollama run devstral \"Write a Python function\"  # Devstral 24B\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "F√∂rklara arkitekturen f√∂r üìÅ Projektstruktur i Simon's AI", "output": "```\n/home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/\n‚îú‚îÄ‚îÄ app/                    # FastAPI backend\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Backend_Chat_Stream.py\n‚îÇ   ‚îú‚îÄ‚îÄ models/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Backend_Agent_Prompts.py\n‚îÇ   ‚îú‚îÄ‚îÄ services/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Backend_Fraga_Router.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gpu_monitor.py\n‚îÇ   ‚îî‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ frontend/               # Vite React\n‚îÇ   ‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KioskDashboard.tsx      # ‚Üê Kiosk huvudfil\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/KioskConfig.ts   # ‚Üê Agent definitions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ SystemControlCard.tsx\n‚îÇ   ‚îî‚îÄ‚îÄ dist/               # Built files\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ HANDOVER_AI_MODELS.md  # ‚Üê Denna fil\n```\n\n---\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad √§r üí° Tips?", "output": "1. **K√∂r alltid `ollama ps`** f√∂r att se vilka modeller som √§r laddade\n2. **VRAM √§r 12GB** - bara en 13-14GB modell √•t g√•ngen\n3. **WebSocket reconnect** - Frontend har auto-reconnect efter 3s\n4. **Kiosk auto-rotate** - St√§ngs av vid touch, √•teraktiveras efter 30s\n\nLycka till! üöÄ\n\n*K√§lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Visa hur man ## üîå api endpoints att testa", "output": "```bash\n# Health check\ncurl http://192.168.86.32:8000/health\n\n# GPU stats\ncurl http://192.168.86.32:8000/api/gpu/stats\n\n# Lista profiler\ncurl http://192.168.86.32:8000/api/profiles\n\n# Warmup en modell (ladda i VRAM)\ncurl -X POST http://192.168.86.32:8000/api/profiles/gpt-oss/warmup\ncurl -X POST http://192.168.86.32:8000/api/profiles/devstral/warmup\n\n# Unload modeller (frig√∂r VRAM)\ncurl -X POST http://192.168.86.32:8000/api/system/unload-models\n\n# WebSocket test (beh√∂ver wscat eller liknande)\nwscat -c ws://192.168.86.32:8000/api/chat\n```"}
{"instruction": "Visa hur man ## üõ†Ô∏è debug-kommandon", "output": "```bash\n# Se vad som k√∂rs i Ollama\nollama ps\n\n# Se alla modeller\nollama list\n\n# Testa modeller direkt i Ollama\nollama run sven-gpt \"Hej, vem √§r du?\"  # GPT-OSS 20B\nollama run devstral \"Write a Python function\"  # Devstral 24B\n\n# Backend logs\njournalctl -u simons-ai -f\n\n# Frontend dev server\ncd frontend && npm run dev\n```"}
