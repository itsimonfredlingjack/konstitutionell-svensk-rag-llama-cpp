{"instruction": "Vad Ã¤r Table of Contents?", "output": "1. [System Architecture](#1-system-architecture)\n2. [Technology Stack](#2-technology-stack)\n3. [Component Details](#3-component-details)\n4. [API Reference](#4-api-reference)\n5. [Database & State](#5-database--state)\n6. [Development Setup](#6-development-setup)\n7. [Deployment](#7-deployment)\n8. [Troubleshooting](#8-troubleshooting)\n9. [Performance Optimization](#9-performance-optimization)\n10. [Security](#10-security)\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Overview?", "output": "Simons AI is a local AI assistant system running on RTX 4070 GPU with a multi-client architecture:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        SIMONS AI SYSTEM                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                  â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\nâ”‚   â”‚  MOBILE  â”‚        â”‚  SERVER  â”‚        â”‚DASHBOARD â”‚         â”‚\nâ”‚   â”‚Controllerâ”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚Broadcasterâ”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Monitor  â”‚         â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚        â”‚                   â”‚                    â”‚               â”‚\nâ”‚   React Native        Python/FastAPI        React Web           â”‚\nâ”‚   Port: N/A           Port: 8000            Port: 5173/kiosk    â”‚\nâ”‚   Whisper: 8001       Ollama: 11434                             â”‚\nâ”‚                                                                  â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ DESKTOP  â”‚        (Separate session, does not mirror mobile)â”‚\nâ”‚   â”‚Workstationâ”‚                                                 â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚   Port: 5173/                                                   â”‚\nâ”‚                                                                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Component Roles?", "output": "| Component | Role | Purpose |\n|-----------|------|---------|\n| **Backend** | Broadcaster | Runs AI agents, broadcasts events to all clients |\n| **Mobile** | Controller | Voice/text input, sends commands to backend |\n| **Dashboard** | Monitor | Passive display for visualization (Nest Hub) |\n| **Desktop** | Workstation | Full IDE with advanced chat features |\n| **Whisper** | Voice-to-Text | Converts speech to text |\n| **Ollama** | LLM Runtime | Runs QWEN models locally on GPU |\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Backend?", "output": "- **Framework**: FastAPI (Python)\n- **WebSocket**: Native WebSocket for real-time communication\n- **LLM**: Ollama with QWEN 3 14B model\n- **Voice**: OpenAI Whisper (small model)\n- **Process Management**: systemd user services\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Frontend (Desktop & Dashboard)?", "output": "- **Framework**: React + TypeScript\n- **Build Tool**: Vite\n- **State Management**: Zustand\n- **3D Graphics**: Three.js for WebGL backgrounds\n- **Code Editor**: Monaco Editor / CodeMirror\n- **Styling**: TailwindCSS + Glassmorphism\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Mobile?", "output": "- **Framework**: React Native (Expo 52)\n- **State Management**: Zustand\n- **WebSocket**: Native WebSocket\n- **Voice Input**: Integration with Whisper server\n- **UI**: Native Animations (Animated API)\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r Infrastructure i Simon's AI", "output": "- **GPU**: NVIDIA RTX 4070 (12 GB VRAM)\n- **OS**: Linux with NVIDIA drivers\n- **Network**: Static IP 192.168.86.32\n- **Storage**: ~15 GB for AI models\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 3.1 Backend (Python/FastAPI)?", "output": "**Location**: `/app/`\n\n**Key Files**:\n- `main.py` - FastAPI application entry point\n- `api/Backend_Chat_Stream.py` - WebSocket + War Room events\n- `api/Backend_Agent_Prompts.py` - AI agent profiles\n- `api/routes.py` - REST endpoints\n- `config.py` - Configuration management\n\n**Responsibilities**:\n- Handle WebSocket connections from all clients\n- Run AI agents via Ollama\n- Broadcast `war_room_event` to all dashboards\n- Manage SystemTools (file operations, shell commands)\n- GPU monitoring and telemetry\n\n**Port**: 8000\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r 3.2 Frontend (React Web)?", "output": "**Location**: `/frontend/`, `/frontend-next/`\n\n**Key Files**:\n- `src/App.tsx` - Router and main application\n- `src/Frontend_Huvudsida.tsx` - Desktop chat interface\n- `src/Frontend_3D_Bakgrund.tsx` - Three.js 3D backgrounds\n- `src/KioskDashboard.tsx` - Nest Hub dashboard\n\n**Features**:\n- Desktop: Full IDE with file browser and code editor\n- Dashboard: Passive monitor with GPU telemetry\n- 3D WebGL backgrounds for visual appeal\n- Glassmorphism design (\"Antigravity\")\n\n**Port**: 5173 (Vite dev server)\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r 3.3 Mobile (React Native/Expo)?", "output": "**Location**: `/mobile/`, `/simonsai-mobile-expo/`\n\n**Key Files**:\n- `App.tsx` - Main application entry\n- `src/hooks/useWarRoomSocket.ts` - WebSocket management\n- `src/hooks/useVoiceInput.ts` - Voice input handling\n- `src/constants/config.ts` - Configuration\n\n**Features**:\n- Voice control via Whisper\n- Text input via WebSocket\n- \"The Monolith\" 3D UI (Native Views, not SVG)\n- Safe areas for Android navigation\n\n**Special Notes**:\n- NO heavy SVG libraries (crash risk on Android 16)\n- Use native animations with `useNativeDriver: true`\n- Function first design philosophy\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r 3.4 AI Models?", "output": "**Available Profiles**:\n- **THINK** (QWEN_NEXUS) - Deep analysis, complex code, architecture\n- **CHILL** (QWEN_LITE) - Quick chat, simple help\n- **Nerdy** - Code-focused assistant\n- **Devstral** - Development tasks\n- **Qwen-sonnet** - Balanced performance\n\n**Model**: qwen3:14b (~9 GB VRAM)\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.1 WebSocket API?", "output": "**Endpoint**: `ws://localhost:8000/api/chat`\n\n**Client â†’ Server**:\n```json\n{\n  \"text\": \"User message\",\n  \"profile\": \"QWEN_NEXUS\"\n}\n```\n\n**Server â†’ Client** (Streaming):\n```json\n{\n  \"sender\": \"agent\",\n  \"text\": \"Response chunk\",\n  \"is_finished\": false\n}\n```\n\n**War Room Events** (Broadcast to all):\n```json\n{\n  \"type\": \"war_room_event\",\n  \"event\": \"start|stream|end\",\n  \"data\": {\n    \"text\": \"...\",\n    \"profile\": \"...\",\n    \"tokens\": [\"...\"]\n  }\n}\n```\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.2 REST API?", "output": "**Base URL**: `http://localhost:8000`\n\n#### GET `/api/profiles`\nList all available AI agent profiles.\n\n**Response**:\n```json\n[\n  {\n    \"id\": \"QWEN_NEXUS\",\n    \"name\": \"THINK\",\n    \"description\": \"Deep analysis\"\n  }\n]\n```\n\n#### POST `/api/profiles/{id}/warmup`\nPre-load AI model into memory.\n\n#### GET `/api/gpu/stats`\nGet GPU telemetry.\n\n**Response**:\n```json\n{\n  \"vram_used\": 9012,\n  \"vram_total\": 12288,\n  \"gpu_temp\": 65,\n  \"gpu_util\": 87\n}\n```\n\n#### POST `/api/voice-command`\nProcess voice command via Whisper + QWEN.\n\n**Request**:\n```json\n{\n  \"audio\": \"base64_encoded_audio\"\n}\n```\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Hur fungerar 4.3 Whisper Server API?", "output": "**Base URL**: `http://localhost:8001`\n\n#### GET `/health`\nHealth check endpoint.\n\n#### POST `/transcribe`\nTranscribe audio to text.\n\n**Request**: `multipart/form-data` with audio file\n\n**Response**:\n```json\n{\n  \"text\": \"Transcribed text\"\n}\n```\n\n#### POST `/voice-command`\nTranscribe + send to QWEN agent.\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r State Management?", "output": "**Frontend**: Zustand stores\n- Chat history per session\n- Active agent profile\n- GPU telemetry\n- War room events\n\n**Mobile**: Zustand stores\n- Voice input state\n- WebSocket connection status\n- Message queue\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Persistence?", "output": "Currently using **in-memory** state. No persistent database.\n\nFor future persistence:\n- Consider SQLite for chat history\n- Redis for session management\n- File-based storage for agent memory\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r System requirements?", "output": "- NVIDIA RTX 4070 (12 GB VRAM)\n- Linux with NVIDIA drivers\n- ~15 GB disk space\n- Node.js 18+\n- Python 3.10+\n```\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Beskriv Development Workflow processen", "output": "**Backend**:\n```bash\ncd app/\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\nuvicorn main:app --reload --port 8000\n```\n\n**Frontend**:\n```bash\ncd frontend/\nnpm install\nnpm run dev\n```\n\n**Mobile**:\n```bash\ncd simonsai-mobile-expo/\nnpm install\nnpm start\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Enable services?", "output": "systemctl --user enable simons-ai-backend\nsystemctl --user enable simons-ai-frontend\nloginctl enable-linger ai-server\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Hur konfigurerar jag Port Configuration?", "output": "**Server IP**: `192.168.86.32` (DHCP-reserved)\n\n| Port | Service | Systemd Unit |\n|------|---------|--------------|\n| 5173 | Frontend (Vite) | simons-ai-frontend |\n| 8000 | Backend (FastAPI) | simons-ai-backend |\n| 8001 | Whisper Server | manual |\n| 11434 | Ollama | ollama.service |\n| 3000 | STABLE BACKUP | **NEVER CHANGE** |\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Or manually:?", "output": "systemctl --user restart simons-ai-backend\njournalctl --user -u simons-ai-backend -f\n```\n\n**WebSocket disconnects**:\n- Check `ping_timeout` settings (currently disabled to prevent disconnects)\n- Verify network connectivity\n- Check backend logs for errors\n\n**GPU out of memory**:\n- Only one model loaded at a time (qwen3:14b uses ~9 GB)\n- Check GPU usage: `nvidia-smi`\n- Restart Ollama: `systemctl restart ollama`\n\n**Mobile app crashes**:\n- Avoid heavy SVG libraries on Android 16\n- Use `useNativeDriver: true` for animations\n- Check React Native logs: `npm start` â†’ press 'r' to reload\n\n**Voice input not working**:\n- Ensure Whisper server is running on port 8001\n- Check audio permissions on mobile\n- Test with `curl http://localhost:8001/health`\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r GPU Optimization?", "output": "- Pre-warm models with `/api/profiles/{id}/warmup`\n- Use fp16 quantization for models (already default in Ollama)\n- Monitor VRAM usage to prevent OOM\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r WebSocket Optimization?", "output": "- Token batching (100ms intervals) for streaming\n- Use binary messages for large payloads\n- Implement reconnection logic with exponential backoff\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Frontend Optimization?", "output": "- Lazy load components with React.lazy()\n- Use WebGL for 3D graphics (Three.js)\n- Virtualize long lists\n- Code splitting for faster initial load\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Mobile Optimization?", "output": "- Native animations with `useNativeDriver`\n- Avoid SVG for complex graphics\n- Optimize bundle size with Hermes\n- Use React.memo for expensive components\n\n---\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Security Recommendations?", "output": "- [ ] Add authentication for WebSocket connections\n- [ ] Implement rate limiting on API endpoints\n- [ ] Use HTTPS/WSS for production\n- [ ] Add input validation and sanitization\n- [ ] Implement CORS policy\n- [ ] Add API key management for multi-user scenarios\n- [ ] Regular security audits of dependencies\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r File Structure i Simon's AI", "output": "```\nwar-room-sync-simonsai/\nâ”œâ”€â”€ app/                          # Backend (Python/FastAPI)\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”œâ”€â”€ Backend_Chat_Stream.py\nâ”‚   â”‚   â”œâ”€â”€ Backend_Agent_Prompts.py\nâ”‚   â”‚   â””â”€â”€ routes.py\nâ”‚   â”œâ”€â”€ config.py\nâ”‚   â””â”€â”€ main.py\nâ”œâ”€â”€ frontend/                     # Desktop + Dashboard (React)\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ App.tsx\nâ”‚       â”œâ”€â”€ Frontend_Huvudsida.tsx\nâ”‚       â”œâ”€â”€ Frontend_3D_Bakgrund.tsx\nâ”‚       â””â”€â”€ KioskDashboard.tsx\nâ”œâ”€â”€ mobile/                       # React Native app\nâ”‚   â”œâ”€â”€ App.tsx\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ hooks/\nâ”‚       â”œâ”€â”€ components/\nâ”‚       â””â”€â”€ constants/\nâ”œâ”€â”€ simonsai-mobile-expo/         # Expo mobile app\nâ”œâ”€â”€ systemd/                      # Systemd service files\nâ”œâ”€â”€ scripts/                      # Utility scripts\nâ”œâ”€â”€ docs/                         # Documentation\nâ”œâ”€â”€ tests/                        # Test files\nâ”œâ”€â”€ ARCHITECTURE_MASTER.md        # Architecture overview\nâ”œâ”€â”€ CHANGELOG.md                  # Version history\nâ”œâ”€â”€ DEEP_DOCS.md                  # This file\nâ”œâ”€â”€ FELSÃ–KNING.md                 # Troubleshooting (Swedish)\nâ”œâ”€â”€ README.md                     # Quick start guide\nâ”œâ”€â”€ install.sh                    # Installation script\nâ”œâ”€â”€ start.sh                      # Start all services\nâ”œâ”€â”€ stop.sh                       # Stop all services\nâ”œâ”€â”€ status.sh                     # Check service status\nâ”œâ”€â”€ fix.sh                        # Auto-fix common issues\nâ””â”€â”€ simons-ai                     # CLI management tool\n```\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Deprecated Features?", "output": "Do NOT use these deprecated concepts:\n- âŒ \"Breathing orb\" / SVG animations (mobile crash risk)\n- âŒ Geofencing\n- âŒ Socket.IO (use native WebSocket)\n- âŒ \"03_NEON-UNICORN-AI\" (old reference)\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Vad Ã¤r Related Documentation?", "output": "- [README.md](README.md) - Quick start guide\n- [ARCHITECTURE_MASTER.md](ARCHITECTURE_MASTER.md) - System architecture\n- [FELSÃ–KNING.md](FELSÃ–KNING.md) - Troubleshooting (Swedish)\n- [CHANGELOG.md](CHANGELOG.md) - Version history\n\n---\n\n**Last Updated**: 2025-12-03\n**Maintained By**: Claude AI Assistant\n**License**: MIT\n\n*KÃ¤lla: DEEP_DOCS.md*"}
{"instruction": "Visa hur man ### cli management", "output": "```bash\n# Status check\n./simons-ai status\n\n# Restart all\n./simons-ai restart\n\n# Rebuild frontend\n./simons-ai rebuild\n\n# View logs\n./simons-ai logs\n```"}
{"instruction": "Visa hur man ### log files", "output": "```bash\n# Backend logs\njournalctl --user -u simons-ai-backend -f\n\n# Frontend logs (if using systemd)\njournalctl --user -u simons-ai-frontend -f\n\n# Ollama logs\njournalctl -u ollama -f\n```"}
{"instruction": "Vad Ã¤r A. SYSTEMÃ–VERSIKT - The Triad?", "output": "```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        SIMONS AI                                 â”‚\nâ”‚                      \"The Triad\"                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                  â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\nâ”‚   â”‚  MOBILE  â”‚        â”‚  SERVER  â”‚        â”‚DASHBOARD â”‚         â”‚\nâ”‚   â”‚Controllerâ”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚Broadcasterâ”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Monitor  â”‚         â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚        â”‚                   â”‚                    â”‚               â”‚\nâ”‚   React Native        Python/FastAPI        React Web           â”‚\nâ”‚   Port: N/A           Port: 8000            Port: 5173/kiosk    â”‚\nâ”‚   Whisper: 8001       Ollama: 11434                             â”‚\nâ”‚                                                                  â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚ DESKTOP  â”‚        (Separat session, ej speglar mobil)       â”‚\nâ”‚   â”‚Workstationâ”‚                                                 â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚   Port: 5173/                                                   â”‚\nâ”‚                                                                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r Komponenter?", "output": "| Komponent | Roll | Tech Stack | Port |\n|-----------|------|------------|------|\n| **Backend** | Broadcaster | Python/FastAPI + WebSocket | 8000 |\n| **Mobile** | Controller | React Native (Expo 52) | - |\n| **Dashboard** | Monitor | React (Kiosk mode) | 5173/kiosk |\n| **Desktop** | Workstation | React (Full IDE) | 5173/ |\n| **Whisper** | Voice-to-Text | OpenAI Whisper (small) | 8001 |\n| **Ollama** | LLM Runtime | QWEN 3 14B | 11434 |\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r Roller fÃ¶rklarade?", "output": "- **Backend (Broadcaster)**:\n  - KÃ¶r QWEN/Nerdy-agenterna via Ollama\n  - Tar emot input frÃ¥n Mobile/Desktop\n  - Broadcastar `war_room_event` till alla dashboards\n  - Hanterar SystemTools (fil, shell)\n\n- **Mobile (Controller)**:\n  - RÃ¶ststyrning via Whisper\n  - Text-input via WebSocket\n  - \"The Monolith\" 3D UI (Native Views, ej SVG)\n  - Skickar `{text, profile}` till backend\n\n- **Dashboard (Monitor)**:\n  - Passiv skÃ¤rm (Nest Hub)\n  - Lyssnar pÃ¥ `war_room_event` fÃ¶r visualisering\n  - Visar GPU-telemetri\n  - Matrix-terminal stil\n\n- **Desktop (Workstation)**:\n  - **EGEN session (speglar EJ mobilen!)**\n  - Full IDE med Monaco/CodeMirror\n  - Avancerad chatt med fil-hantering\n  - Delar filsystem med backend\n\n---\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r WebSocket Events?", "output": "| Event | Riktning | Beskrivning |\n|-------|----------|-------------|\n| `{text, profile}` | Client â†’ Server | Chat-meddelande (Antigravity) |\n| `{sender: \"agent\", text, is_finished}` | Server â†’ Client | Streaming svar |\n| `war_room_event: start` | Server â†’ ALL | Mobil-session startar |\n| `war_room_event: stream` | Server â†’ ALL | Token-batch (100ms) |\n| `war_room_event: end` | Server â†’ ALL | Session klar |\n| `status_update` | Server â†’ ALL | GPU-telemetri |\n| `mobile_activity` | Server â†’ ALL | Legacy (voice commands) |\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Hur fungerar HTTP Endpoints?", "output": "| Endpoint | Method | Beskrivning |\n|----------|--------|-------------|\n| `/api/chat` | WebSocket | Huvudchatt |\n| `/api/voice-command` | POST | Whisper + QWEN |\n| `/api/profiles` | GET | Lista agenter |\n| `/api/profiles/{id}/warmup` | POST | Ladda modell |\n| `/api/gpu/stats` | GET | GPU-status |\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r Whisper Server (Port 8001)?", "output": "| Endpoint | Method | Beskrivning |\n|----------|--------|-------------|\n| `/health` | GET | Status-check |\n| `/transcribe` | POST | Endast transkribering |\n| `/voice-command` | POST | Transkribera + skicka till QWEN |\n\n---\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r Desktop?", "output": "- \"Antigravity\" design (Glassmorphism)\n- Full IDE med fil-browser\n- Monaco/CodeMirror fÃ¶r kod\n- 3D WebGL bakgrund (Three.js)\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r Dashboard (Kiosk)?", "output": "- High contrast fÃ¶r lÃ¤sbarhet\n- Stor text (touch-friendly)\n- Matrix terminal-stil\n- Auto-flip mellan Agent Cards och Mobile Activity\n\n---\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r D. FILER & STRUKTUR i Simon's AI", "output": "```\n01_AI-VIBE-WORLD/\nâ”œâ”€â”€ app/                          # Backend\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”œâ”€â”€ Backend_Chat_Stream.py   # WebSocket + War Room\nâ”‚   â”‚   â”œâ”€â”€ Backend_Agent_Prompts.py # Agent profiles\nâ”‚   â”‚   â””â”€â”€ routes.py                # REST endpoints\nâ”‚   â”œâ”€â”€ config.py\nâ”‚   â””â”€â”€ main.py\nâ”œâ”€â”€ frontend/                     # Desktop + Dashboard\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ App.tsx                  # Router\nâ”‚       â”œâ”€â”€ Frontend_Huvudsida.tsx   # Desktop chat\nâ”‚       â”œâ”€â”€ Frontend_3D_Bakgrund.tsx # Three.js\nâ”‚       â””â”€â”€ KioskDashboard.tsx       # Nest Hub\nâ”œâ”€â”€ mobile/                       # React Native app\nâ”‚   â”œâ”€â”€ App.tsx\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ hooks/\nâ”‚       â”‚   â”œâ”€â”€ useWarRoomSocket.ts\nâ”‚       â”‚   â””â”€â”€ useVoiceInput.ts\nâ”‚       â”œâ”€â”€ components/\nâ”‚       â””â”€â”€ constants/config.ts\nâ”œâ”€â”€ simons-ai                     # CLI management tool\nâ”œâ”€â”€ ARCHITECTURE_MASTER.md        # THIS FILE\nâ””â”€â”€ docs/DEPRECATED/              # Old documentation\n```\n\n---\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r E. PORTAR & IP?", "output": "**Server IP:** `192.168.86.32` (DHCP-reserverad)\n\n| Port | TjÃ¤nst | Systemd Service |\n|------|--------|-----------------|\n| 5173 | Frontend (Vite) | simons-ai-frontend |\n| 8000 | Backend (FastAPI) | simons-ai-backend |\n| 8001 | Whisper Server | manuell |\n| 11434 | Ollama | ollama |\n| 3000 | STABLE BACKUP | **ALDRIG Ã„NDRA** |\n\n---\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Vad Ã¤r G. DEPRECATED CONCEPTS?", "output": "FÃ¶ljande begrepp/features Ã¤r **UTDATERADE** och ska ignoreras:\n\n- âŒ \"Breathing orb\" / SVG-animationer\n- âŒ Geofencing\n- âŒ \"03_NEON-UNICORN-AI\" som mobilapp (gammal referens)\n- âŒ Socket.IO (vi anvÃ¤nder native WebSocket)\n\n---\n\n*Skapad: 2025-12-01 som ersÃ¤ttning fÃ¶r CLAUDE.md, PROJECT.md, SERVER.md*\n\n*KÃ¤lla: ARCHITECTURE_MASTER.md*"}
{"instruction": "Visa hur man ## f. kommandon", "output": "```bash\n# Status\n./simons-ai status\n\n# Omstart\n./simons-ai restart\n\n# Rebuild frontend\n./simons-ai rebuild\n\n# Loggar\n./simons-ai logs\n```"}
{"instruction": "Vad Ã¤r Changelog?", "output": "All notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r Added?", "output": "- CHANGELOG.md and DEEP_DOCS.md for better project documentation\n- ğŸ¨ Shiki code blocks med catppuccin-mocha tema (ersÃ¤tter react-syntax-highlighter)\n- ğŸ§  LoRA Fine-Tuning setup: 37 trÃ¤ningsexempel frÃ¥n n8n workflows (scripts/training_data.jsonl)\n- ğŸ“š `orange-md` skill fÃ¶r CLAUDE.md best practices\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r Changed?", "output": "- ğŸ”§ Prompt-arkitektur: EN kÃ¤lla (Modelfiles Ã¤ger prompten, backend injicerar inget)\n- Backend_Fraga_Router.py: 772 â†’ 322 rader (alla prompts borttagna)\n- Backend_Agent_Prompts.py: Rensat frÃ¥n prompts, behÃ¥ller bara UI-metadata\n- ~/CLAUDE.md trimmat: 193 â†’ 59 rader med referenser till SERVER-MAINTENANCE.md\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r Fixed?", "output": "- ğŸ› Modeller frÃ¥gade istÃ¤llet fÃ¶r att koda - lade till \"FRÃ…GA ALDRIG\" i alla Modelfiles\n- ğŸ› Prompt-konflikt mellan Modelfile och runtime API (nu endast Modelfile)\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r Removed?", "output": "- =12.0 file (pip error output)\n- frontend_backup_20251126_190923 folder (old backup)\n- simonsai-mobile-link (1) duplicate folder\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r How to Update This Changelog?", "output": "When making changes, add them under the `[Unreleased]` section in the appropriate category:\n\n- **Added** for new features\n- **Changed** for changes in existing functionality\n- **Deprecated** for soon-to-be removed features\n- **Removed** for now removed features\n- **Fixed** for any bug fixes\n- **Security** for vulnerability fixes\n\nWhen releasing a new version, move items from `[Unreleased]` to a new version section with the date.\n\n*KÃ¤lla: CHANGELOG.md*"}
{"instruction": "Vad Ã¤r CLAUDE.md?", "output": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Vad Ã¤r detta??", "output": "Simons AI - Lokal AI-assistent pÃ¥ RTX 4070 (12GB VRAM). Multi-client chat med WebSocket-broadcasting till Desktop, Dashboard (Kiosk) och Mobile.\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Frontend (React/Vite)?", "output": "```bash\ncd frontend\nnpm run dev      # Dev server pÃ¥ port 5173\nnpm run build    # Bygg fÃ¶r produktion\nnpm run lint     # ESLint\n```\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Tester?", "output": "```bash\npytest tests/                    # Backend\ncd frontend && npm test          # Frontend (om konfigurerat)\n```\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r CLI Management?", "output": "```bash\n./simons-ai status    # Visa tjÃ¤nstestatus\n./simons-ai restart   # Starta om alla tjÃ¤nster\n./simons-ai rebuild   # Bygg om frontend\n./simons-ai logs      # Visa loggar\n./fix.sh              # Automatisk felsÃ¶kning\n```\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r Arkitektur i Simon's AI", "output": "```\nBackend (Python/FastAPI) â”€â”€â”€â”€ Ollama (QWEN 3 14B) â”€â”€â”€â”€ GPU\n    â”‚ port 8000                  port 11434\n    â”‚ WebSocket: /api/chat\n    â”‚ WebSocket: /api/cascade (Multi-agent)\n    â”‚\n    â”œâ”€â–º Desktop (React)  â†â”€â”€ Full IDE, egen session\n    â”‚   port 5173/\n    â”‚\n    â”œâ”€â–º Dashboard (React) â†â”€â”€ Passiv monitor, war_room_event\n    â”‚   port 5173/kiosk\n    â”‚\n    â””â”€â–º Mobile (Expo) â†â”€â”€ Controller, rÃ¶ststyrning\n        Whisper: port 8001\n```\n\n**Roller:**\n- Backend: Broadcaster - kÃ¶r AI-agenter, broadcastar events till alla klienter\n- Desktop: Workstation - full IDE med fil-browser (egen session, speglar EJ mobil)\n- Dashboard: Monitor - passiv visualisering fÃ¶r Nest Hub\n- Mobile: Controller - rÃ¶st/text-input via Whisper\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Backend (`app/`)?", "output": "| Fil | Syfte |\n|-----|-------|\n| `main.py` | FastAPI entrypoint, lifespan-hantering |\n| `api/Backend_Chat_Stream.py` | WebSocket endpoints + war_room_event broadcasting |\n| `api/routes.py` | REST endpoints |\n| `models/Backend_Agent_Prompts.py` | Agent-profiler (endast UI-metadata, prompts i Modelfiles) |\n| `services/ollama_client.py` | Ollama API-klient |\n| `services/orchestrator.py` | Multi-agent orkestrering |\n| `config.py` | Pydantic settings |\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Frontend (`frontend/src/`)?", "output": "| Fil | Syfte |\n|-----|-------|\n| `App.tsx` | Router |\n| `Frontend_Huvudsida.tsx` | Desktop chat-interface |\n| `Frontend_3D_Bakgrund.tsx` | Three.js WebGL bakgrund |\n| `KioskDashboard.tsx` | Nest Hub passiv monitor |\n| `stores/` | Zustand state management |\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Konventioner?", "output": "- **SprÃ¥k:** Svenska fÃ¶r konversation, engelska fÃ¶r kod\n- **Prompts:** EN kÃ¤lla = Modelfiles. Backend skickar inte system-prompt.\n- **WebSocket:** Native WebSocket, inte Socket.IO\n- **State:** Zustand fÃ¶r frontend\n- **Styling:** TailwindCSS + Glassmorphism (\"Antigravity\")\n- **3D:** Three.js via @react-three/fiber\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Portar?", "output": "| Port | TjÃ¤nst |\n|------|--------|\n| 5173 | Frontend (Vite dev) |\n| 8000 | Backend (FastAPI) |\n| 8001 | Whisper (voice-to-text) |\n| 11434 | Ollama |\n| 3000 | STABLE BACKUP - **Ã„NDRA ALDRIG** |\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r Deprecated (ignorera dessa)?", "output": "- âŒ \"Breathing orb\" / SVG-animationer (crash pÃ¥ Android)\n- âŒ Socket.IO\n- âŒ Prompts i backend (anvÃ¤nd Modelfiles)\n\n*KÃ¤lla: CLAUDE.md*"}
{"instruction": "Vad Ã¤r React + TypeScript + Vite?", "output": "This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n\n*KÃ¤lla: frontend/README.md*"}
{"instruction": "Vad Ã¤r React Compiler?", "output": "The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).\n\n*KÃ¤lla: frontend/README.md*"}
{"instruction": "Hur konfigurerar jag Expanding the ESLint configuration?", "output": "If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:\n\n```js\nexport default defineConfig([\n  globalIgnores(['dist']),\n  {\n    files: ['**/*.{ts,tsx}'],\n    extends: [\n      // Other configs...\n\n      // Remove tseslint.configs.recommended and replace with this\n      tseslint.configs.recommendedTypeChecked,\n      // Alternatively, use this for stricter rules\n      tseslint.configs.strictTypeChecked,\n      // Optionally, add this for stylistic rules\n      tseslint.configs.stylisticTypeChecked,\n\n      // Other configs...\n    ],\n    languageOptions: {\n      parserOptions: {\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\n        tsconfigRootDir: import.meta.dirname,\n      },\n      // other options...\n    },\n  },\n])\n```\n\nYou can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:\n\n```js\n// eslint.config.js\nimport reactX from 'eslint-plugin-react-x'\nimport reactDom from 'eslint-plugin-react-dom'\n\nexport default defineConfig([\n  globalIgnores(['dist']),\n  {\n    files: ['**/*.{ts,tsx}'],\n    extends: [\n      // Other configs...\n      // Enable lint rules for React\n      reactX.configs['recommended-typescript'],\n      // Enable lint rules for React DOM\n      reactDom.configs.recommended,\n    ],\n    languageOptions: {\n      parserOptions: {\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\n        tsconfigRootDir: import.meta.dirname,\n      },\n      // other options...\n    },\n  },\n])\n```\n\n*KÃ¤lla: frontend/README.md*"}
{"instruction": "Vad Ã¤r WebSocket Chat?", "output": "```\nws://192.168.86.32:8000/api/chat\n```\n\n**Skicka meddelande:**\n```json\n{\n  \"type\": \"chat\",\n  \"profile\": \"gpt-oss\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hej!\"}],\n  \"request_id\": \"unique-id-123\"\n}\n```\n\n**Ta emot (streaming):**\n```json\n{\"type\": \"start\", \"agent_id\": \"gpt-oss\"}\n{\"type\": \"token\", \"content\": \"Hej\"}\n{\"type\": \"token\", \"content\": \"!\"}\n{\"type\": \"done\", \"stats\": {...}}\n```\n\n*KÃ¤lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad Ã¤r TillgÃ¤ngliga modeller?", "output": "| Profile ID | Modell | Beskrivning |\n|------------|--------|-------------|\n| `gpt-oss` | GPT-OSS 20B | Arkitekt - resonemang, planering |\n| `devstral` | Devstral 24B | Kodare - implementation |\n\n*KÃ¤lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Hur fungerar REST Endpoints?", "output": "```\nGET  /api/health              # Systemstatus\nPOST /api/profiles/{id}/warmup # VÃ¤rm upp modell\n```\n\n*KÃ¤lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad Ã¤r Viktigt fÃ¶r mobilappen?", "output": "1. **WebSocket-format:** Samma som ovan\n2. **Streaming:** Tokens kommer en i taget, buffra och visa\n3. **Profile-switch:** Skicka annat `profile`-vÃ¤rde fÃ¶r att byta modell\n4. **Request ID:** Generera unikt ID per meddelande (UUID)\n\n*KÃ¤lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Vad Ã¤r Exempel: Minimal klient?", "output": "```javascript\nconst ws = new WebSocket('ws://192.168.86.32:8000/api/chat');\n\nws.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  if (data.type === 'token') {\n    // Visa token i chatten\n    appendToChat(data.content);\n  }\n};\n\n// Skicka meddelande\nws.send(JSON.stringify({\n  type: 'chat',\n  profile: 'gpt-oss',\n  messages: [{role: 'user', content: 'Hej!'}],\n  request_id: crypto.randomUUID()\n}));\n```\n\n*KÃ¤lla: docs/HANDOVER_MOBILE_APP.md*"}
{"instruction": "Visa hur man minimal klient", "output": "```javascript\nconst ws = new WebSocket('ws://192.168.86.32:8000/api/chat');\n\nws.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  if (data.type === 'token') {\n    // Visa token i chatten\n    appendToChat(data.content);\n  }\n};\n\n// Skicka meddelande\nws.send(JSON.stringify({\n  type: 'chat',\n  profile: 'gpt-oss',\n  messages: [{role: 'user', content: 'Hej!'}],\n  request_id: crypto.randomUUID()\n}));\n```"}
{"instruction": "Vad Ã¤r ğŸ”„ Ã–verlÃ¤mning: VÃ¤ck liv i AI-modellerna?", "output": "**Datum:** 2025-12-02\n**FÃ¶regÃ¥ende session:** Polish features fÃ¶r Kiosk Dashboard (auto-rotate, ripple, ljud, lazy-load)\n\n---\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur fungerar Backend (FastAPI) - FUNGERAR âœ…?", "output": "- **URL:** http://192.168.86.32:8000\n- **Docs:** http://192.168.86.32:8000/docs\n- **WebSocket:** ws://192.168.86.32:8000/api/chat\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur sÃ¤tter jag upp Ollama-modeller installerade?", "output": "```\nsven-gpt:latest    13 GB   â† GPT-OSS 20B (custom Modelfile) - anvÃ¤nds i Ollama\ngpt-oss:20b        13 GB   â† Basmodell\ndevstral:24b       14 GB   â† Mistral kodmodell\n```\n\n**OBS:** Profile-IDs i backend/frontend Ã¤r `gpt-oss` och `devstral`, men Ollama-modellnamnet fÃ¶r GPT-OSS Ã¤r `sven-gpt`.\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r ğŸ—ï¸ Arkitektur i Simon's AI", "output": "```\nFrontend (React)\n    â†“ WebSocket\nBackend_Chat_Stream.py  â† Huvud-WebSocket endpoint\n    â†“\nBackend_Fraga_Router.py â† Routar till rÃ¤tt agent/modell\n    â†“\nBackend_Agent_Prompts.py â† System prompts per agent\n    â†“\nollama_client.py â† Ollama API-anrop\n```\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r Nyckelfilar?", "output": "| Fil | Ansvar |\n|-----|--------|\n| `app/api/Backend_Chat_Stream.py` | WebSocket, streaming, GPU telemetry |\n| `app/services/Backend_Fraga_Router.py` | Agent routing logic |\n| `app/models/Backend_Agent_Prompts.py` | Profile definitions & prompts |\n| `app/services/ollama_client.py` | Ollama HTTP client |\n| `app/services/gpu_monitor.py` | nvidia-smi parsing |\n\n---\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Hur fungerar FrÃ¥n /api/profiles endpoint:?", "output": "\"gpt-oss\"    â†’ model: \"sven-gpt\"   (13GB)  # Arkitekten (GPT-OSS 20B)\n\"devstral\"   â†’ model: \"devstral\"   (14GB)  # Kodaren (Devstral 24B)\n\"qwen\"       â†’ model: \"sven-gpt\"   (Legacy)\n```\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r Kiosk Dashboard visar?", "output": "- **GPT-OSS** (cyan) - id: `gpt-oss`\n- **Devstral** (gold) - id: `devstral`\n\nâœ… **Status:** Frontend och backend anvÃ¤nder nu samma profile-IDs (gpt-oss, devstral)\n\n---\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r Warmup en modell (ladda i VRAM)?", "output": "curl -X POST http://192.168.86.32:8000/api/profiles/gpt-oss/warmup\ncurl -X POST http://192.168.86.32:8000/api/profiles/devstral/warmup\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r 1. Synka Frontend â†” Backend profiles?", "output": "- Kolla `frontend/src/config/KioskConfig.ts`\n- Matcha agent IDs med backend `/api/profiles`\n- Uppdatera `KIOSK_AGENTS` om nÃ¶dvÃ¤ndigt\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Beskriv 2. Verifiera warmup-flÃ¶det processen", "output": "```\nKiosk \"Tap to Load\" â†’ handleAgentSelect()\n  â†’ POST /api/profiles/{id}/warmup  (ex: gpt-oss eller devstral)\n  â†’ ollama_client.warmup_model()\n  â†’ Ollama laddar modell i VRAM (sven-gpt eller devstral)\n```\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r 3. Testa chat-streaming?", "output": "```\nFrontend skickar via WebSocket:\n{\n  \"type\": \"chat_message\",\n  \"content\": \"Hej!\",\n  \"profile\": \"gpt-oss\"\n}\n\nBackend svarar med streaming tokens:\n{\n  \"type\": \"stream_token\",\n  \"token\": \"Hej\",\n  \"agent_id\": \"gpt-oss\"\n}\n```\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r 4. Koppla modeller i Kiosk?", "output": "- Testa att trycka pÃ¥ agent i Kiosk\n- Verifiera att modell faktiskt laddas (`ollama ps`)\n- Kolla att `is_active` uppdateras i frontend\n\n---\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r Testa modeller direkt i Ollama?", "output": "ollama run sven-gpt \"Hej, vem Ã¤r du?\"  # GPT-OSS 20B\nollama run devstral \"Write a Python function\"  # Devstral 24B\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "FÃ¶rklara arkitekturen fÃ¶r ğŸ“ Projektstruktur i Simon's AI", "output": "```\n/home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/\nâ”œâ”€â”€ app/                    # FastAPI backend\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â””â”€â”€ Backend_Chat_Stream.py\nâ”‚   â”œâ”€â”€ models/\nâ”‚   â”‚   â””â”€â”€ Backend_Agent_Prompts.py\nâ”‚   â”œâ”€â”€ services/\nâ”‚   â”‚   â”œâ”€â”€ Backend_Fraga_Router.py\nâ”‚   â”‚   â”œâ”€â”€ ollama_client.py\nâ”‚   â”‚   â””â”€â”€ gpu_monitor.py\nâ”‚   â””â”€â”€ main.py\nâ”œâ”€â”€ frontend/               # Vite React\nâ”‚   â”œâ”€â”€ src/\nâ”‚   â”‚   â”œâ”€â”€ KioskDashboard.tsx      # â† Kiosk huvudfil\nâ”‚   â”‚   â”œâ”€â”€ config/KioskConfig.ts   # â† Agent definitions\nâ”‚   â”‚   â””â”€â”€ components/\nâ”‚   â”‚       â””â”€â”€ SystemControlCard.tsx\nâ”‚   â””â”€â”€ dist/               # Built files\nâ””â”€â”€ docs/\n    â””â”€â”€ HANDOVER_AI_MODELS.md  # â† Denna fil\n```\n\n---\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Vad Ã¤r ğŸ’¡ Tips?", "output": "1. **KÃ¶r alltid `ollama ps`** fÃ¶r att se vilka modeller som Ã¤r laddade\n2. **VRAM Ã¤r 12GB** - bara en 13-14GB modell Ã¥t gÃ¥ngen\n3. **WebSocket reconnect** - Frontend har auto-reconnect efter 3s\n4. **Kiosk auto-rotate** - StÃ¤ngs av vid touch, Ã¥teraktiveras efter 30s\n\nLycka till! ğŸš€\n\n*KÃ¤lla: docs/HANDOVER_AI_MODELS.md*"}
{"instruction": "Visa hur man ## ğŸ”Œ api endpoints att testa", "output": "```bash\n# Health check\ncurl http://192.168.86.32:8000/health\n\n# GPU stats\ncurl http://192.168.86.32:8000/api/gpu/stats\n\n# Lista profiler\ncurl http://192.168.86.32:8000/api/profiles\n\n# Warmup en modell (ladda i VRAM)\ncurl -X POST http://192.168.86.32:8000/api/profiles/gpt-oss/warmup\ncurl -X POST http://192.168.86.32:8000/api/profiles/devstral/warmup\n\n# Unload modeller (frigÃ¶r VRAM)\ncurl -X POST http://192.168.86.32:8000/api/system/unload-models\n\n# WebSocket test (behÃ¶ver wscat eller liknande)\nwscat -c ws://192.168.86.32:8000/api/chat\n```"}
{"instruction": "Visa hur man ## ğŸ› ï¸ debug-kommandon", "output": "```bash\n# Se vad som kÃ¶rs i Ollama\nollama ps\n\n# Se alla modeller\nollama list\n\n# Testa modeller direkt i Ollama\nollama run sven-gpt \"Hej, vem Ã¤r du?\"  # GPT-OSS 20B\nollama run devstral \"Write a Python function\"  # Devstral 24B\n\n# Backend logs\njournalctl -u simons-ai -f\n\n# Frontend dev server\ncd frontend && npm run dev\n```"}
