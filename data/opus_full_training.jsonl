{"instruction": "Skapa ett n8n workflow som orkestrerar flera AI-agenter med Ollama RAG och specialiserade services", "output": "## AI Agent Orchestrator med Ollama RAG\n\n### Arkitektur\nWorkflowet använder en central orchestrator-agent som koordinerar specialiserade n8n workflow-services via HTTP webhooks.\n\n### Komponenter\n1. **Webhook Trigger** - Tar emot användarförfrågningar via POST\n2. **Workflow Configuration** - Centraliserad konfiguration för alla service-URLs\n3. **Orchestrator Agent** - LangChain agent med Ollama som koordinerar tools\n4. **RAG Vector Store** - In-memory vektor-databas för kontextuell kunskap\n5. **Specialized Service Tools** - HTTP Request Tools för Database, Sentiment, PDF services\n\n### Implementation\n```javascript\n// Orchestrator System Prompt\nconst systemMessage = `You are an orchestrator agent that coordinates specialized n8n workflow services.\n\nYour role is to:\n1. Analyze the user request and determine which specialized services are needed\n2. Call the appropriate service tools in the correct sequence\n3. Coordinate data flow between services\n4. Use the RAG knowledge base when you need contextual information\n5. Return the final result to the user`;\n\n// Service Tool Configuration\nconst databaseServiceTool = {\n  toolDescription: \"Calls the specialized database service workflow\",\n  method: \"POST\",\n  url: \"{{ $('Workflow Configuration').first().json.databaseServiceUrl }}\",\n  bodyParameters: {\n    query: \"{{ $fromAI('query', 'The database query to perform') }}\",\n    parameters: \"{{ $fromAI('parameters', 'Query parameters as JSON', 'json', {}) }}\"\n  }\n};\n```\n\n### Best Practices\n- Använd `retrieve-as-tool` mode för RAG vector store\n- Separera services i egna workflows för moduläritet\n- Inkludera `includeOtherFields: true` i Set-noder för att bevara data"}
{"instruction": "Bygg ett multi-source business intelligence system med AI-driven analytics i n8n", "output": "## AI-Driven Multi-Source Business Intelligence\n\n### Syfte\nSamla data från flera källor, analysera med AI och leverera insikter anpassade för olika målgrupper.\n\n### Dataflöde\n```\nMultiple Sources → Normalize → AI Analysis → Audience Adaptation → Delivery\n```\n\n### Komponenter\n1. **Data Sources** - Google Sheets, APIs, Databases\n2. **Data Normalization** - Set-noder som standardiserar format\n3. **AI Analysis Agent** - LangChain agent för intelligent analys\n4. **Audience Adaptation** - Switch-nod som routar till rätt format\n5. **Delivery Channels** - Slack, Email, Dashboard\n\n### Implementation\n```javascript\n// Normalize data from different sources\nconst normalizeData = {\n  assignments: [\n    { name: \"dataType\", value: \"={{ $json.type || 'unknown' }}\" },\n    { name: \"value\", value: \"={{ $json.amount || $json.value || 0 }}\" },\n    { name: \"timestamp\", value: \"={{ $json.date || $now.toISO() }}\" },\n    { name: \"source\", value: \"={{ $json.source || 'manual' }}\" }\n  ]\n};\n\n// AI Analysis System Prompt\nconst analysisPrompt = `Analyze this business data and provide:\n1. Key trends and patterns\n2. Anomalies that need attention\n3. Actionable recommendations\n4. Risk assessment\n\nBe data-driven and specific.`;\n```\n\n### Tips\n- Använd Merge-nod med `combineAll` för att samla alla datakällor\n- Implementera caching för frekventa API-anrop\n- Logga alla analyser för audit trail"}
{"instruction": "Skapa ett automatiskt dokumentationssystem som genererar docs från GitHub, Slack, Jira och Linear events", "output": "## Auto-Generated Internal Documentation System\n\n### Arkitektur\nWorkflowet lyssnar på events från utvecklingsverktyg och genererar automatiskt strukturerad dokumentation.\n\n### Event Sources\n1. **GitHub Webhook** - Push, PR, Issues\n2. **Slack Webhook** - Viktiga diskussioner\n3. **Jira Webhook** - Issue updates\n4. **Linear Webhook** - Task changes\n\n### AI Agents Pipeline\n```\nEvent → Normalize → Context Analyzer → Route by Type → Specialized Writer → Vector Store → Output\n```\n\n### Implementation\n```javascript\n// Normalize Event Data\nconst normalizeEvent = {\n  eventType: \"={{ $json.body.action || $json.body.event?.type || 'unknown' }}\",\n  eventSource: \"={{ $json.headers['x-github-event'] ? 'github' : $json.headers['x-slack-signature'] ? 'slack' : 'unknown' }}\",\n  eventTitle: \"={{ $json.body.pull_request?.title || $json.body.issue?.title || 'No title' }}\",\n  eventDescription: \"={{ $json.body.pull_request?.body || $json.body.issue?.body || '' }}\",\n  eventAuthor: \"={{ $json.body.sender?.login || $json.body.user?.name || 'Unknown' }}\"\n};\n\n// Context Analyzer Agent\nconst analyzerPrompt = `Analyze this development event and determine:\n1. What changed or happened?\n2. Why did it happen? (extract intent)\n3. What systems/components are affected?\n4. What type of documentation should be updated?\n\nDocumentation types:\n- architecture: System design changes\n- api: API endpoints, formats\n- changelog: User-facing changes\n- lessons: Problems and solutions found`;\n\n// Specialized Writers\nconst architectureWriter = {\n  systemMessage: `You are an architecture documentation writer.\n  - Describe system components and relationships\n  - Explain architectural decisions\n  - Use Mermaid diagrams when relevant\n  - Maintain continuity with existing docs`\n};\n```\n\n### Output Destinations\n- **Notion** - Structured database pages\n- **GitHub Docs** - Markdown files in repo\n- **Confluence** - Wiki pages\n- **Qdrant** - Vector storage for RAG"}
{"instruction": "Implementera ett intelligent RAG-system med Ollama som har multiple tools och cross-document intelligence", "output": "## Intelligent RAG Ollama Multi-Function System\n\n### Funktioner\n- RAG-baserad Q&A med lokal Ollama\n- Real-time API integration\n- Email/Slack actions\n- Cross-document relationship analysis\n\n### Komponenter\n```\nQuery → Config → RAG Agent → Tools → Actions → Response\n         ↓\n    Auto-Update → Fetch → Clean → Vector Store → Intelligence\n```\n\n### Implementation\n```javascript\n// Workflow Configuration\nconst config = {\n  ollamaModel: \"llama3.2\",\n  embeddingModel: \"nomic-embed-text\",\n  chunkSize: 1000,\n  chunkOverlap: 200\n};\n\n// RAG Orchestrator Agent\nconst ragPrompt = `Du är en intelligent RAG-assistent på svenska.\n\nTillgängliga verktyg:\n1. knowledge_base_search: Sök i kunskapsbasen\n2. fetch_live_data: Hämta realtidsdata\n3. Gmail/Slack: Skicka meddelanden\n4. schedule_reminder: Schemalägg uppgifter\n\nAnvänd rätt verktyg, kombinera data, förklara källor.`;\n\n// Cross-Document Intelligence (Code Node)\nfunction analyzeRelationships(documents) {\n  const extractKeyTerms = (text) => {\n    return text.toLowerCase()\n      .replace(/[^\\w\\s]/g, ' ')\n      .split(/\\s+/)\n      .filter(word => word.length > 4);\n  };\n  \n  const calculateSimilarity = (doc1, doc2) => {\n    const terms1 = new Set(extractKeyTerms(doc1.content));\n    const terms2 = new Set(extractKeyTerms(doc2.content));\n    const intersection = [...terms1].filter(x => terms2.has(x));\n    const union = new Set([...terms1, ...terms2]);\n    return intersection.length / union.size; // Jaccard similarity\n  };\n  \n  // Build knowledge graph\n  const knowledgeGraph = { nodes: [], edges: [] };\n  // ... implementation\n  return knowledgeGraph;\n}\n```\n\n### Swedish Document Handling\n```javascript\n// Normalize Swedish characters\ncontent = content\n  .replace(/Ã¥/g, 'å')\n  .replace(/Ã¤/g, 'ä')\n  .replace(/Ã¶/g, 'ö');\n```"}
{"instruction": "Bygg ett Second Brain cognitive engine med multi-source intelligence processing i n8n", "output": "## Second Brain Cognitive Engine\n\n### Koncept\nEtt personligt AI-system som samlar, analyserar och organiserar information från alla dina digitala källor.\n\n### Data Sources\n1. **Gmail** - Email trigger var 5:e minut\n2. **Google Drive** - Document changes\n3. **Google Calendar** - Event updates\n4. **Slack** - Important messages\n\n### AI Agent Architecture\n```\n┌─────────────────────────────────────────────┐\n│           ORCHESTRATOR AGENT                │\n├─────────────────────────────────────────────┤\n│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n│  │Classifier│ │Summarizer│ │Task     │       │\n│  │Agent    │ │Agent    │ │Extractor│       │\n│  └─────────┘ └─────────┘ └─────────┘       │\n│  ┌─────────┐ ┌─────────┐                   │\n│  │Priority │ │Context  │                   │\n│  │Planner  │ │Finder   │                   │\n│  └─────────┘ └─────────┘                   │\n└─────────────────────────────────────────────┘\n```\n\n### Implementation\n```javascript\n// Normalize all sources to common format\nconst normalizeEmail = {\n  sourceType: \"email\",\n  content: \"={{ $json.textPlain || $json.textHtml }}\",\n  title: \"={{ $json.subject }}\",\n  timestamp: \"={{ $json.date }}\",\n  sender: \"={{ $json.from.address }}\"\n};\n\n// Classifier Agent Tool\nconst classifierPrompt = `Categorize into ONE category:\n- economy (financial matters)\n- health (medical, fitness)\n- relationships (personal)\n- projects (work initiatives)\n- tasks (actionable items)\n- ideas (creative thoughts)\n- knowledge (learning)\nReturn ONLY the category name.`;\n\n// Task Extractor with Output Parser\nconst taskSchema = {\n  type: \"object\",\n  properties: {\n    tasks: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        properties: {\n          description: { type: \"string\" },\n          deadline: { type: \"string\" },\n          priority: { type: \"string\" },\n          context: { type: \"string\" }\n        }\n      }\n    }\n  }\n};\n\n// Priority Planner - 3-day action plan\nconst plannerPrompt = `Create a 3-day action plan.\nConsider:\n- Deadlines and time sensitivity\n- Impact and importance\n- Dependencies between tasks\n- Realistic capacity (3-5 tasks/day)`;\n```\n\n### Output Channels\n- **Slack** - Daily summary with mrkdwn formatting\n- **Notion** - Structured archive database\n- **Vector Store** - Long-term memory for RAG"}
{"instruction": "Skapa ett automatiskt fakturapåminnelse-system med n8n", "output": "## Automated Invoice Payment Reminder System\n\n### Funktionalitet\nAutomatiska påminnelser vid T-3, förfallodatum och +7 dagar.\n\n### Workflow\n```\nDaily Check → Get Invoices → Calculate Stage → Filter → Prepare Email → Send → Log\n```\n\n### Implementation\n```javascript\n// Calculate Reminder Stage (Code Node)\nconst dueDate = new Date($input.item.json.dueDate);\nconst isPaid = $input.item.json.isPaid === 'TRUE';\nconst lastReminderSent = $input.item.json.lastReminderSent || '';\nconst today = new Date();\n\nconst daysUntilDue = Math.floor((dueDate - today) / (1000 * 60 * 60 * 24));\n\nlet reminderStage = null;\nlet shouldSendReminder = false;\n\nif (!isPaid) {\n  if (daysUntilDue === 3 && lastReminderSent !== 'T-3') {\n    reminderStage = 'T-3';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === 0 && lastReminderSent !== 'DueDate') {\n    reminderStage = 'DueDate';\n    shouldSendReminder = true;\n  } else if (daysUntilDue === -7 && lastReminderSent !== 'Overdue+7') {\n    reminderStage = 'Overdue+7';\n    shouldSendReminder = true;\n  }\n}\n\nreturn { ...($input.item.json), daysUntilDue, reminderStage, shouldSendReminder };\n\n// Dynamic Email Subject\nconst emailSubject = reminderStage === 'T-3' \n  ? 'Påminnelse: Faktura förfaller om 3 dagar'\n  : reminderStage === 'DueDate'\n  ? 'Påminnelse: Faktura förfaller idag'\n  : 'Viktig påminnelse: Förfallen faktura (7 dagar)';\n```\n\n### Email Template (HTML)\n```html\n<div style=\"background-color: #f5f5f5; padding: 15px;\">\n  <strong>Fakturadetaljer:</strong><br>\n  Fakturanummer: {{ $json.invoiceNumber }}<br>\n  Belopp: {{ $json.amount }} SEK<br>\n  Förfallodatum: {{ $json.dueDate }}\n</div>\n<a href=\"{{ paymentLink }}\" style=\"background-color: #4CAF50; color: white; padding: 12px 24px;\">Betala nu</a>\n```\n\n### Google Sheets Integration\n- Läs fakturalista med `returnFirstMatch: false`\n- Uppdatera `lastReminderSent` och `lastReminderDate` efter skickat"}
{"instruction": "Hur strukturerar jag ett n8n workflow med AI-agenter och verktyg?", "output": "## n8n AI Agent Workflow Structure\n\n### Grundläggande Pattern\n```\nTrigger → Configuration → Agent → Tools → Output\n                           ↓\n                    Language Model\n                           ↓\n                      Memory/RAG\n```\n\n### Best Practices\n\n#### 1. Workflow Configuration Node\nAnvänd alltid en Set-nod i början för konfiguration:\n```javascript\n{\n  assignments: [\n    { name: \"apiUrl\", value: \"<PLACEHOLDER>\", type: \"string\" },\n    { name: \"modelName\", value: \"gpt-4.1-mini\", type: \"string\" },\n    { name: \"maxTokens\", value: 2048, type: \"number\" }\n  ],\n  includeOtherFields: true\n}\n```\n\n#### 2. Agent Node Setup\n```javascript\n// Använd promptType: \"define\" för full kontroll\n{\n  promptType: \"define\",\n  text: \"={{ $json.userInput }}\",\n  options: {\n    systemMessage: \"Du är en hjälpsam assistent...\"\n  }\n}\n```\n\n#### 3. Tool Connections\nKoppla verktyg via `ai_tool` connection type:\n- Vector Store som tool → `mode: \"retrieve-as-tool\"`\n- HTTP Request Tool → `toolDescription` är kritisk\n- Code Tool → Returnera alltid JSON.stringify()\n\n#### 4. Output Parsers\nFör strukturerad output:\n```javascript\n// Structured Output Parser\n{\n  schemaType: \"manual\",\n  inputSchema: JSON.stringify(yourSchema),\n  autoFix: true  // LLM fixar formatfel\n}\n```\n\n#### 5. Memory\n```javascript\n// Buffer Window Memory\n{\n  sessionKey: \"={{ $json.sessionId || 'default' }}\",\n  contextWindowLength: 10  // Antal meddelanden att minnas\n}\n```\n\n### Vanliga Misstag\n1. Glömmer `includeOtherFields: true` → tappar data\n2. Fel `typeVersion` → nod fungerar inte\n3. Saknar `toolDescription` → agent kan inte använda tool\n4. Hårdkodar värden istället för expressions"}
{"instruction": "Hur implementerar jag RAG med Vector Store i n8n?", "output": "## RAG Implementation i n8n\n\n### Två Modes\n1. **Insert Mode** - Lägg till dokument i vector store\n2. **Retrieve-as-Tool Mode** - Sök som agent tool\n\n### Insert Pipeline\n```\nData Source → Document Loader → Text Splitter → Embeddings → Vector Store (insert)\n```\n\n### Implementation\n```javascript\n// 1. Document Loader\n{\n  textSplittingMode: \"custom\",\n  jsonMode: \"expressionData\",\n  jsonData: \"={{ $json }}\"\n}\n\n// 2. Recursive Text Splitter\n{\n  chunkSize: 1000,\n  chunkOverlap: 200  // 20% overlap är standard\n}\n\n// 3. Embeddings (Ollama eller OpenAI)\n// Ollama:\n{ model: \"nomic-embed-text\" }\n// OpenAI:\n{ model: \"text-embedding-3-small\" }\n\n// 4. Vector Store - Insert\n{\n  mode: \"insert\",\n  memoryKey: \"my_knowledge_base\"\n}\n```\n\n### Retrieve som Tool\n```javascript\n// Vector Store - Retrieve as Tool\n{\n  mode: \"retrieve-as-tool\",\n  toolDescription: \"Sök i kunskapsbasen för att hitta relevant information\",\n  memoryKey: \"my_knowledge_base\",\n  topK: 5  // Antal resultat att returnera\n}\n```\n\n### Qdrant Integration (Persistent)\n```javascript\n// Qdrant Vector Store\n{\n  qdrantCollection: {\n    __rl: true,\n    mode: \"id\",\n    value: \"={{ $('Config').first().json.collectionName }}\"\n  }\n}\n```\n\n### Tips\n- Använd `In-Memory` för utveckling, `Qdrant` för produktion\n- `topK: 5` är bra default, öka vid behov\n- `toolDescription` måste vara tydlig för att agent ska använda rätt"}
{"instruction": "Skapa ett multipart memory ingestion system som hanterar audio, bild, PDF, URL och text med AI-processning och vector storage", "output": "## Multipart Memory Ingestion System\n\n### Arkitektur\nWebhook-baserat system som tar emot olika innehållstyper, extraherar text och lagrar i Qdrant vector store.\n\n### Content Type Routing\n```\nWebhook → Parse Multipart → Switch by Type → Process → Summarize → Keywords → Qdrant\n                                │\n                    ┌───────────┼───────────┐\n                    ▼           ▼           ▼\n                  Audio       Image        PDF\n                (Whisper)    (OCR)      (Extract)\n```\n\n### Implementation\n```javascript\n// Parse Multipart Data (Code Node)\nconst parsedData = {\n  type: body.type || null,\n  title: body.title || null,\n  text: body.text || null,\n  source: body.source || null,\n  timestamp: body.timestamp || new Date().toISOString(),\n  metadata: body.metadata || {},\n  user_id: body.user_id || null\n};\n\n// Handle binary file if present\nif (item.binary && Object.keys(item.binary).length > 0) {\n  binaryData = item.binary;\n  parsedData.file = Object.keys(item.binary)[0];\n}\n\n// Extract Keywords (Code Node)\nfunction extractKeywords(text) {\n  const stopWords = new Set(['the', 'a', 'an', 'and', 'or', ...]);\n  const words = text.toLowerCase()\n    .replace(/[^a-z0-9\\s]/g, ' ')\n    .split(/\\s+/)\n    .filter(word => word.length > 3 && !stopWords.has(word));\n  \n  const wordCount = {};\n  words.forEach(word => wordCount[word] = (wordCount[word] || 0) + 1);\n  \n  return Object.entries(wordCount)\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, 10)\n    .map(entry => entry[0]);\n}\n```\n\n### Content Processors\n- **Audio**: OpenAI Whisper transcription\n- **Image**: OCR via extractFromFile\n- **URL**: HTTP Request med responseFormat: text\n- **PDF**: extractFromFile med operation: pdf\n- **Text**: Direkt genomströmning\n\n### Vector Storage\n```javascript\n// Qdrant Insert\n{\n  mode: \"insert\",\n  qdrantCollection: \"{{ $('Config').first().json.qdrantCollection }}\"\n}\n\n// Search endpoint\n{\n  mode: \"load\",\n  prompt: \"{{ $json.query }}\",\n  topK: \"{{ $json.topK }}\"\n}\n```"}
{"instruction": "Implementera bidirectional data sync mellan Linear, Gmail, Calendar, Notion och Slack med conflict resolution", "output": "## Multi-Platform Bidirectional Data Sync\n\n### Funktionalitet\n- Synkar data mellan 5 plattformar var 15:e minut\n- Normaliserar data till gemensamt format\n- Hanterar konflikter med configuerbar strategi\n- Sparar sync state för incremental updates\n\n### Sync Flow\n```\nSchedule → Server Check → Load State → Fetch All → Normalize → Detect Changes → Resolve Conflicts → Route → Update → Save State\n```\n\n### Data Normalization (Code Node)\n```javascript\n// Detect platform from data structure\nlet platform, id, title, description, timestamp, status, metadata;\n\n// Linear Issues\nif (json.identifier || json.team) {\n  platform = 'linear';\n  id = json.id || json.identifier;\n  title = json.title || '';\n  status = json.state?.name || '';\n  metadata = { assignee: json.assignee?.name, priority: json.priority };\n}\n// Gmail Messages  \nelse if (json.threadId || json.labelIds) {\n  platform = 'gmail';\n  id = json.id || json.threadId;\n  title = json.subject || '';\n  status = json.labelIds?.includes('UNREAD') ? 'unread' : 'read';\n}\n// Google Calendar\nelse if (json.start?.dateTime) {\n  platform = 'calendar';\n  title = json.summary || '';\n  metadata = { start: json.start, end: json.end, attendees: json.attendees };\n}\n// Notion\nelse if (json.properties || json.object === 'page') {\n  platform = 'notion';\n  title = json.properties?.Name?.title?.[0]?.plain_text || '';\n}\n// Slack\nelse if (json.ts || json.channel) {\n  platform = 'slack';\n  title = json.text?.substring(0, 100) || '';\n}\n```\n\n### Conflict Resolution Strategies\n```javascript\nconst strategy = config.conflictResolutionStrategy; // 'latest_wins' | 'merge' | 'manual_review'\n\nif (strategy === 'latest_wins') {\n  resolved.data = sourceTimestamp > targetTimestamp ? sourceData : targetData;\n} else if (strategy === 'merge') {\n  resolved.data = { ...targetData };\n  for (const [key, value] of Object.entries(sourceData)) {\n    if (!targetData[key]) resolved.data[key] = value;\n    else if (JSON.stringify(targetData[key]) !== JSON.stringify(value)) {\n      resolved.needsManualReview = true;\n      resolved.conflictDetails.push({ field: key, sourceValue: value, targetValue: targetData[key] });\n    }\n  }\n}\n```\n\n### State Management\n```javascript\n// Save Sync State\n{\n  timestamp: $now.toISO(),\n  platforms: { linear, gmail, calendar, notion, slack },\n  lastSync: $now.toISO(),\n  syncStatus: 'completed'\n}\n```"}
{"instruction": "Bygg ett AI-driven newsletter system med Google Drive trigger, AI-generering, approval workflow och automated reminders", "output": "## AI Newsletter Generation System\n\n### Workflow Overview\n1. **Trigger**: Nya filer i Google Drive-mapp\n2. **Process**: Extrahera text, generera med AI\n3. **Store**: Spara draft i PostgreSQL\n4. **Approve**: Webhook för godkännande\n5. **Send**: Skicka via Gmail\n6. **Remind**: Veckovis påminnelse om tomt innehåll\n\n### Implementation\n```javascript\n// Workflow Configuration\n{\n  distributionList: \"team@company.com,all-hands@company.com\",\n  reminderRecipients: \"content-team@company.com\",\n  databaseTable: \"newsletters\"\n}\n\n// Check If Already Processed\n\"SELECT COUNT(*) as count FROM processed_files WHERE file_id = $1\"\n\n// Generate Newsletter with AI\n{\n  model: \"gpt-4o\",\n  messages: [\n    {\n      role: \"system\",\n      content: \"You are a professional newsletter writer. Create well-formatted HTML...\"\n    },\n    { role: \"user\", content: \"={{ $json.text }}\" }\n  ],\n  temperature: 0.7\n}\n\n// Save Newsletter Draft\nINSERT INTO newsletters (id, html_content, status, file_id, created_at)\nVALUES ($1, $2, 'pending', $3, NOW())\n```\n\n### Approval Flow\n```\nPOST /webhook/newsletter-approval\n  → Get Newsletter WHERE status='pending'\n  → Send via Gmail\n  → UPDATE status='sent', sent_at=NOW()\n  → Respond { success: true }\n```\n\n### Weekly Reminder\n```javascript\n// Check Content Last Week\n\"SELECT COUNT(*) FROM processed_files WHERE processed_at >= NOW() - INTERVAL '7 days'\"\n\n// If count = 0, send reminder\nif ($json.count === 0) {\n  sendEmail({\n    to: config.reminderRecipients,\n    subject: \"Reminder: No Newsletter Content This Week\"\n  });\n}\n```\n\n### Database Schema\n```sql\nCREATE TABLE newsletters (\n  id TEXT PRIMARY KEY,\n  html_content TEXT,\n  status TEXT DEFAULT 'pending',\n  file_id TEXT,\n  created_at TIMESTAMP,\n  sent_at TIMESTAMP\n);\n\nCREATE TABLE processed_files (\n  file_id TEXT PRIMARY KEY,\n  file_name TEXT,\n  processed_at TIMESTAMP DEFAULT NOW()\n);\n```"}
{"instruction": "Hur hanterar jag errors och edge cases i n8n workflows?", "output": "## Error Handling i n8n Workflows\n\n### 1. Server Health Check Pattern\n```javascript\n// Kolla server status innan main flow\n{\n  url: \"{{ $('Config').json.statusCheckUrl }}\",\n  options: { timeout: 5000 }\n}\n\n// If-nod: statusCode === 200\n// True → Continue\n// False → Send Error Alert\n```\n\n### 2. Idempotency Pattern\n```sql\n-- Kolla om redan processad\nSELECT COUNT(*) as count FROM processed_items WHERE id = $1\n\n-- If count > 0 → Skip\n-- Else → Process och INSERT\n```\n\n### 3. HTTP Request Error Handling\n```javascript\n{\n  options: {\n    response: {\n      response: {\n        neverError: true  // Returnera error som data istället för att faila\n      }\n    }\n  }\n}\n```\n\n### 4. Fallback Values\n```javascript\n// Säkra expressions med fallbacks\n$json.value || $json.defaultValue || 'fallback'\n$json.nested?.deeply?.value ?? 'default'\n```\n\n### 5. Error Webhook Alerts\n```javascript\n// Send Error Alert nod\n{\n  method: \"POST\",\n  url: \"{{ config.alertWebhookUrl }}\",\n  body: {\n    timestamp: \"{{ $now.toISO() }}\",\n    error: \"{{ $json.error }}\",\n    workflow: \"Workflow Name\",\n    node: \"{{ $node.name }}\"\n  }\n}\n```\n\n### 6. Retry Logic\n```javascript\n// Använd n8n's inbyggda retry\n{\n  retryOnFail: true,\n  maxTries: 3,\n  waitBetweenTries: 1000\n}\n```\n\n### 7. Data Validation\n```javascript\n// Code Node validation\nconst required = ['email', 'name', 'amount'];\nconst missing = required.filter(field => !$json[field]);\n\nif (missing.length > 0) {\n  throw new Error(`Missing fields: ${missing.join(', ')}`);\n}\n```\n\n### 8. Graceful Degradation\n```javascript\n// Switch med fallback output\n{\n  rules: [...],\n  options: {\n    fallbackOutput: 0  // Default route om ingen regel matchar\n  }\n}\n```"}
{"instruction": "Hur optimerar jag n8n workflows för prestanda?", "output": "## n8n Performance Optimization\n\n### 1. Batch Processing\n```javascript\n// Använd SplitInBatches för stora datasets\n{\n  batchSize: 100,\n  options: { reset: false }\n}\n\n// Eller Loop Over Items\nfor (const item of $input.all()) {\n  // Process\n}\n```\n\n### 2. Parallel Execution\n```javascript\n// Merge med combineByPosition för parallella branches\n{\n  mode: \"combine\",\n  combineBy: \"combineByPosition\",\n  numberInputs: 5\n}\n```\n\n### 3. Caching\n```javascript\n// Spara resultat i workflow static data\nconst cache = $getWorkflowStaticData('global');\nconst cacheKey = `api_${$json.id}`;\n\nif (cache[cacheKey] && Date.now() - cache[cacheKey].timestamp < 3600000) {\n  return cache[cacheKey].data;\n}\n\n// Fetch and cache\nconst result = await fetch(...);\ncache[cacheKey] = { data: result, timestamp: Date.now() };\n```\n\n### 4. Incremental Sync\n```javascript\n// Spara last sync timestamp\nconst lastSync = $getWorkflowStaticData('global').lastSync || '1970-01-01';\n\n// Query endast nya items\n{\n  filter: `modified_at > '${lastSync}'`\n}\n\n// Uppdatera efter sync\n$getWorkflowStaticData('global').lastSync = $now.toISO();\n```\n\n### 5. Minimize API Calls\n```javascript\n// Batch API requests\nconst ids = items.map(i => i.json.id);\nconst response = await fetch(`/api/items?ids=${ids.join(',')}`);\n\n// Istället för\nfor (const item of items) {\n  await fetch(`/api/items/${item.json.id}`); // Långsamt!\n}\n```\n\n### 6. Response Size\n```javascript\n// Begränsa returnerade fält\n{\n  options: {\n    fields: \"id,name,status\"  // Bara nödvändiga fält\n  }\n}\n```\n\n### 7. Timeout Configuration\n```javascript\n// Sätt rimliga timeouts\n{\n  options: {\n    timeout: 30000  // 30 sekunder\n  }\n}\n```\n\n### 8. Memory Management\n```javascript\n// Rensa stora objekt efter användning\ndelete $json._original;\ndelete $json.largePayload;\n\n// Använd streaming för stora filer\n{ responseFormat: \"file\" }\n```"}
{"instruction": "Skapa webhooks med authentication och response handling i n8n", "output": "## n8n Webhook Patterns\n\n### 1. Basic Authenticated Webhook\n```javascript\n{\n  httpMethod: \"POST\",\n  path: \"my-webhook\",\n  authentication: \"headerAuth\",  // Kräver credentials\n  responseMode: \"responseNode\",  // Manuell response\n  options: {}\n}\n```\n\n### 2. Response Modes\n```javascript\n// responseMode options:\n// - \"onReceived\": Svara direkt med 200\n// - \"lastNode\": Svara efter sista noden\n// - \"responseNode\": Använd Respond to Webhook nod\n\n// Respond to Webhook\n{\n  respondWith: \"json\",\n  responseBody: \"={{ $json }}\",\n  options: {\n    responseCode: 200,\n    responseHeaders: {\n      \"X-Custom-Header\": \"value\"\n    }\n  }\n}\n```\n\n### 3. Request Parsing\n```javascript\n// Webhook data locations\n$json.body       // POST body\n$json.query      // Query parameters\n$json.headers    // Request headers\n$json.params     // URL parameters\n\n// Multipart form data\n$json.body.field_name\n$binary.file     // Uploaded file\n```\n\n### 4. Validation Pattern\n```javascript\n// Code Node efter webhook\nconst required = ['action', 'data'];\nconst body = $json.body;\n\nfor (const field of required) {\n  if (!body[field]) {\n    return {\n      json: {\n        error: `Missing required field: ${field}`,\n        statusCode: 400\n      }\n    };\n  }\n}\n\nreturn { json: { ...body, validated: true } };\n```\n\n### 5. Webhook URL Format\n```\nProduction: https://your-n8n.com/webhook/path-name\nTest: https://your-n8n.com/webhook-test/path-name\n```\n\n### 6. Error Response\n```javascript\n// Respond to Webhook för errors\n{\n  respondWith: \"json\",\n  responseBody: {\n    success: false,\n    error: \"{{ $json.errorMessage }}\",\n    code: \"{{ $json.errorCode }}\"\n  },\n  options: {\n    responseCode: 400\n  }\n}\n```\n\n### 7. Async Processing\n```javascript\n// Svara direkt, processa i bakgrunden\n// 1. Webhook med responseMode: \"onReceived\"\n// 2. Async processing continues\n// 3. Callback webhook när klart\n```"}
{"instruction": "Create a rate limiter middleware for FastAPI that limits requests per IP to 100 per minute", "output": "Here's a production-ready rate limiter middleware:\n\n```python\nfrom fastapi import FastAPI, Request, HTTPException\nfrom collections import defaultdict\nimport time\nimport asyncio\n\nclass RateLimiter:\n    def __init__(self, requests_per_minute: int = 100):\n        self.requests_per_minute = requests_per_minute\n        self.requests = defaultdict(list)\n        self._cleanup_task = None\n    \n    async def start_cleanup(self):\n        while True:\n            await asyncio.sleep(60)\n            cutoff = time.time() - 60\n            for ip in list(self.requests.keys()):\n                self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n                if not self.requests[ip]:\n                    del self.requests[ip]\n    \n    def is_allowed(self, ip: str) -> bool:\n        now = time.time()\n        cutoff = now - 60\n        self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n        \n        if len(self.requests[ip]) >= self.requests_per_minute:\n            return False\n        \n        self.requests[ip].append(now)\n        return True\n\nrate_limiter = RateLimiter(requests_per_minute=100)\n\n@app.middleware('http')\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n    \n    if not rate_limiter.is_allowed(client_ip):\n        raise HTTPException(\n            status_code=429,\n            detail='Rate limit exceeded. Max 100 requests per minute.'\n        )\n    \n    return await call_next(request)\n```\n\nKey features:\n1. In-memory storage with automatic cleanup\n2. Sliding window algorithm (more accurate than fixed windows)\n3. Per-IP tracking\n4. Async-compatible cleanup task\n5. Returns 429 with clear error message"}
{"instruction": "Write a Python decorator that retries failed async functions with exponential backoff", "output": "Here's a robust retry decorator with exponential backoff:\n\n```python\nimport asyncio\nimport functools\nimport logging\nfrom typing import Type, Tuple, Callable, Any\n\nlogger = logging.getLogger(__name__)\n\ndef async_retry(\n    max_attempts: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n) -> Callable:\n    \"\"\"Retry decorator with exponential backoff for async functions.\n    \n    Args:\n        max_attempts: Maximum number of retry attempts\n        base_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay cap\n        exponential_base: Base for exponential calculation\n        exceptions: Tuple of exceptions to catch and retry\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> Any:\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_attempts - 1:\n                        logger.error(f'{func.__name__} failed after {max_attempts} attempts: {e}')\n                        raise\n                    \n                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n                    logger.warning(f'{func.__name__} attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s')\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage example:\n@async_retry(max_attempts=5, base_delay=0.5, exceptions=(ConnectionError, TimeoutError))\nasync def fetch_data(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, timeout=10)\n        return response.json()\n```\n\nFeatures:\n1. Configurable retry count and delays\n2. Exponential backoff with max cap\n3. Selective exception handling\n4. Logging for debugging\n5. Preserves function metadata with functools.wraps"}
{"instruction": "Implement a simple pub/sub system in Python using asyncio", "output": "Here's a lightweight async pub/sub implementation:\n\n```python\nimport asyncio\nfrom collections import defaultdict\nfrom typing import Callable, Any, Dict, Set\nfrom dataclasses import dataclass, field\nimport uuid\n\n@dataclass\nclass Subscription:\n    id: str\n    topic: str\n    callback: Callable\n\nclass PubSub:\n    def __init__(self):\n        self._subscribers: Dict[str, Set[Subscription]] = defaultdict(set)\n        self._lock = asyncio.Lock()\n    \n    async def subscribe(self, topic: str, callback: Callable) -> str:\n        \"\"\"Subscribe to a topic. Returns subscription ID for unsubscribing.\"\"\"\n        sub_id = str(uuid.uuid4())\n        subscription = Subscription(id=sub_id, topic=topic, callback=callback)\n        \n        async with self._lock:\n            self._subscribers[topic].add(subscription)\n        \n        return sub_id\n    \n    async def unsubscribe(self, sub_id: str) -> bool:\n        \"\"\"Unsubscribe using subscription ID.\"\"\"\n        async with self._lock:\n            for topic, subs in self._subscribers.items():\n                for sub in subs:\n                    if sub.id == sub_id:\n                        subs.remove(sub)\n                        return True\n        return False\n    \n    async def publish(self, topic: str, message: Any) -> int:\n        \"\"\"Publish message to topic. Returns number of subscribers notified.\"\"\"\n        async with self._lock:\n            subscribers = list(self._subscribers.get(topic, []))\n        \n        if not subscribers:\n            return 0\n        \n        tasks = []\n        for sub in subscribers:\n            if asyncio.iscoroutinefunction(sub.callback):\n                tasks.append(sub.callback(message))\n            else:\n                tasks.append(asyncio.to_thread(sub.callback, message))\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n        return len(subscribers)\n    \n    async def publish_pattern(self, pattern: str, message: Any) -> int:\n        \"\"\"Publish to all topics matching pattern (simple * wildcard).\"\"\"\n        import fnmatch\n        count = 0\n        \n        async with self._lock:\n            matching_topics = [t for t in self._subscribers.keys() if fnmatch.fnmatch(t, pattern)]\n        \n        for topic in matching_topics:\n            count += await self.publish(topic, message)\n        \n        return count\n\n# Usage:\npubsub = PubSub()\n\nasync def handler(msg):\n    print(f'Received: {msg}')\n\nasync def main():\n    sub_id = await pubsub.subscribe('events.user.*', handler)\n    await pubsub.publish('events.user.login', {'user': 'simon'})\n    await pubsub.unsubscribe(sub_id)\n```\n\nFeatures:\n1. Async-safe with locks\n2. Supports both sync and async callbacks\n3. Subscription IDs for clean unsubscribe\n4. Wildcard pattern matching\n5. Thread-safe for sync callbacks"}
{"instruction": "Create a WebSocket connection manager for FastAPI that handles multiple rooms/channels", "output": "Here's a full-featured WebSocket manager with room support:\n\n```python\nfrom fastapi import WebSocket\nfrom typing import Dict, Set, Optional, Any\nimport json\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass Connection:\n    websocket: WebSocket\n    user_id: str\n    rooms: Set[str] = field(default_factory=set)\n    connected_at: datetime = field(default_factory=datetime.utcnow)\n\nclass ConnectionManager:\n    def __init__(self):\n        self._connections: Dict[str, Connection] = {}  # user_id -> Connection\n        self._rooms: Dict[str, Set[str]] = {}  # room_name -> set of user_ids\n        self._lock = asyncio.Lock()\n    \n    async def connect(self, websocket: WebSocket, user_id: str) -> Connection:\n        await websocket.accept()\n        conn = Connection(websocket=websocket, user_id=user_id)\n        \n        async with self._lock:\n            # Disconnect existing connection for same user\n            if user_id in self._connections:\n                await self._disconnect_internal(user_id)\n            self._connections[user_id] = conn\n        \n        return conn\n    \n    async def disconnect(self, user_id: str):\n        async with self._lock:\n            await self._disconnect_internal(user_id)\n    \n    async def _disconnect_internal(self, user_id: str):\n        if user_id not in self._connections:\n            return\n        \n        conn = self._connections[user_id]\n        \n        # Remove from all rooms\n        for room in conn.rooms.copy():\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n        \n        del self._connections[user_id]\n        \n        try:\n            await conn.websocket.close()\n        except:\n            pass\n    \n    async def join_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id not in self._connections:\n                return\n            \n            self._connections[user_id].rooms.add(room)\n            if room not in self._rooms:\n                self._rooms[room] = set()\n            self._rooms[room].add(user_id)\n    \n    async def leave_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id in self._connections:\n                self._connections[user_id].rooms.discard(room)\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n    \n    async def send_personal(self, user_id: str, message: Any):\n        if user_id not in self._connections:\n            return False\n        \n        try:\n            data = json.dumps(message) if not isinstance(message, str) else message\n            await self._connections[user_id].websocket.send_text(data)\n            return True\n        except:\n            await self.disconnect(user_id)\n            return False\n    \n    async def broadcast_room(self, room: str, message: Any, exclude: Optional[str] = None):\n        if room not in self._rooms:\n            return 0\n        \n        tasks = []\n        for user_id in self._rooms[room]:\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    async def broadcast_all(self, message: Any, exclude: Optional[str] = None):\n        tasks = []\n        for user_id in list(self._connections.keys()):\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    def get_room_users(self, room: str) -> Set[str]:\n        return self._rooms.get(room, set()).copy()\n    \n    def get_user_rooms(self, user_id: str) -> Set[str]:\n        if user_id in self._connections:\n            return self._connections[user_id].rooms.copy()\n        return set()\n\n# Usage in FastAPI:\nmanager = ConnectionManager()\n\n@app.websocket('/ws/{user_id}')\nasync def websocket_endpoint(websocket: WebSocket, user_id: str):\n    conn = await manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_json()\n            \n            if data.get('action') == 'join':\n                await manager.join_room(user_id, data['room'])\n            elif data.get('action') == 'leave':\n                await manager.leave_room(user_id, data['room'])\n            elif data.get('action') == 'message':\n                await manager.broadcast_room(data['room'], {\n                    'from': user_id,\n                    'content': data['content']\n                })\n    except:\n        await manager.disconnect(user_id)\n```\n\nFeatures:\n1. Multi-room support with join/leave\n2. Personal and broadcast messaging\n3. Automatic cleanup on disconnect\n4. Thread-safe with async locks\n5. Handles reconnection (kicks old connection)"}
{"instruction": "Write a Python script that monitors a directory for new files and processes them", "output": "Here's a robust file watcher with processing queue:\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler, FileCreatedEvent\nimport queue\nimport threading\nimport logging\nfrom typing import Callable, Optional\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass FileProcessor:\n    def __init__(\n        self,\n        watch_dir: str,\n        process_func: Callable[[Path], None],\n        patterns: Optional[list] = None,\n        debounce_seconds: float = 1.0\n    ):\n        self.watch_dir = Path(watch_dir)\n        self.process_func = process_func\n        self.patterns = patterns or ['*']\n        self.debounce_seconds = debounce_seconds\n        \n        self._queue = queue.Queue()\n        self._seen = {}  # path -> last_seen_time\n        self._running = False\n        self._observer = None\n        self._worker_thread = None\n    \n    def _matches_pattern(self, path: Path) -> bool:\n        import fnmatch\n        return any(fnmatch.fnmatch(path.name, p) for p in self.patterns)\n    \n    def _on_created(self, event):\n        if event.is_directory:\n            return\n        \n        path = Path(event.src_path)\n        if not self._matches_pattern(path):\n            return\n        \n        # Debounce: skip if we've seen this file recently\n        now = datetime.now().timestamp()\n        if path in self._seen:\n            if now - self._seen[path] < self.debounce_seconds:\n                return\n        \n        self._seen[path] = now\n        self._queue.put(path)\n        logger.info(f'Queued: {path.name}')\n    \n    def _worker(self):\n        while self._running:\n            try:\n                path = self._queue.get(timeout=1.0)\n            except queue.Empty:\n                continue\n            \n            # Wait for file to be fully written\n            asyncio.run(self._wait_for_stable(path))\n            \n            if not path.exists():\n                logger.warning(f'File disappeared: {path}')\n                continue\n            \n            try:\n                logger.info(f'Processing: {path.name}')\n                self.process_func(path)\n                logger.info(f'Completed: {path.name}')\n            except Exception as e:\n                logger.error(f'Error processing {path.name}: {e}')\n            finally:\n                self._queue.task_done()\n    \n    async def _wait_for_stable(self, path: Path, checks: int = 3, interval: float = 0.5):\n        \"\"\"Wait until file size stops changing.\"\"\"\n        prev_size = -1\n        stable_count = 0\n        \n        while stable_count < checks:\n            if not path.exists():\n                return\n            \n            size = path.stat().st_size\n            if size == prev_size:\n                stable_count += 1\n            else:\n                stable_count = 0\n                prev_size = size\n            \n            await asyncio.sleep(interval)\n    \n    def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        \n        # Start worker thread\n        self._worker_thread = threading.Thread(target=self._worker, daemon=True)\n        self._worker_thread.start()\n        \n        # Setup watchdog\n        handler = FileSystemEventHandler()\n        handler.on_created = self._on_created\n        \n        self._observer = Observer()\n        self._observer.schedule(handler, str(self.watch_dir), recursive=False)\n        self._observer.start()\n        \n        logger.info(f'Watching: {self.watch_dir}')\n    \n    def stop(self):\n        self._running = False\n        \n        if self._observer:\n            self._observer.stop()\n            self._observer.join()\n        \n        if self._worker_thread:\n            self._worker_thread.join(timeout=5.0)\n        \n        logger.info('Stopped watching')\n    \n    def process_existing(self):\n        \"\"\"Process files that already exist in the directory.\"\"\"\n        for path in self.watch_dir.iterdir():\n            if path.is_file() and self._matches_pattern(path):\n                self._queue.put(path)\n\n# Example usage:\ndef process_file(path: Path):\n    \"\"\"Your processing logic here.\"\"\"\n    content = path.read_text()\n    print(f'File {path.name} has {len(content)} characters')\n    # Move to processed folder, upload somewhere, etc.\n\nif __name__ == '__main__':\n    processor = FileProcessor(\n        watch_dir='/path/to/watch',\n        process_func=process_file,\n        patterns=['*.json', '*.csv'],\n        debounce_seconds=2.0\n    )\n    \n    processor.start()\n    processor.process_existing()  # Handle files already there\n    \n    try:\n        while True:\n            asyncio.run(asyncio.sleep(1))\n    except KeyboardInterrupt:\n        processor.stop()\n```\n\nFeatures:\n1. Pattern-based file filtering\n2. Debouncing to avoid duplicate processing\n3. Waits for files to be fully written before processing\n4. Queue-based processing with worker thread\n5. Can process existing files on startup\n6. Graceful shutdown"}
{"instruction": "Create a caching decorator that supports both sync and async functions with TTL", "output": "Here's a universal caching decorator with TTL support:\n\n```python\nimport asyncio\nimport functools\nimport time\nimport hashlib\nimport json\nfrom typing import Any, Callable, Optional, Union\nfrom collections import OrderedDict\nimport threading\n\nclass Cache:\n    def __init__(self, maxsize: int = 1000):\n        self._cache: OrderedDict = OrderedDict()\n        self._maxsize = maxsize\n        self._lock = threading.Lock()\n    \n    def get(self, key: str) -> tuple[bool, Any]:\n        with self._lock:\n            if key not in self._cache:\n                return False, None\n            \n            value, expiry = self._cache[key]\n            if expiry and time.time() > expiry:\n                del self._cache[key]\n                return False, None\n            \n            # Move to end (LRU)\n            self._cache.move_to_end(key)\n            return True, value\n    \n    def set(self, key: str, value: Any, ttl: Optional[float] = None):\n        with self._lock:\n            expiry = time.time() + ttl if ttl else None\n            \n            if key in self._cache:\n                del self._cache[key]\n            \n            self._cache[key] = (value, expiry)\n            \n            # Evict oldest if over maxsize\n            while len(self._cache) > self._maxsize:\n                self._cache.popitem(last=False)\n    \n    def delete(self, key: str):\n        with self._lock:\n            self._cache.pop(key, None)\n    \n    def clear(self):\n        with self._lock:\n            self._cache.clear()\n\n# Global cache instance\n_default_cache = Cache()\n\ndef _make_key(func: Callable, args: tuple, kwargs: dict) -> str:\n    \"\"\"Create a hashable cache key from function and arguments.\"\"\"\n    key_parts = [\n        func.__module__,\n        func.__qualname__,\n        str(args),\n        str(sorted(kwargs.items()))\n    ]\n    key_string = '|'.join(key_parts)\n    return hashlib.md5(key_string.encode()).hexdigest()\n\ndef cached(\n    ttl: Optional[float] = None,\n    maxsize: int = 1000,\n    key_func: Optional[Callable] = None,\n    cache: Optional[Cache] = None\n) -> Callable:\n    \"\"\"Universal caching decorator for sync and async functions.\n    \n    Args:\n        ttl: Time to live in seconds. None = no expiry.\n        maxsize: Maximum cache entries.\n        key_func: Custom function to generate cache key.\n        cache: Custom Cache instance. Uses global cache if None.\n    \"\"\"\n    _cache = cache or _default_cache\n    \n    def decorator(func: Callable) -> Callable:\n        is_async = asyncio.iscoroutinefunction(func)\n        \n        if is_async:\n            @functools.wraps(func)\n            async def async_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = await func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            async_wrapper.cache_clear = _cache.clear\n            async_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return async_wrapper\n        else:\n            @functools.wraps(func)\n            def sync_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            sync_wrapper.cache_clear = _cache.clear\n            sync_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return sync_wrapper\n    \n    return decorator\n\n# Usage examples:\n\n@cached(ttl=60)  # Cache for 60 seconds\ndef get_user(user_id: int) -> dict:\n    # Expensive database query\n    return {'id': user_id, 'name': 'Simon'}\n\n@cached(ttl=300, key_func=lambda url, **kw: url)  # Custom key\nasync def fetch_url(url: str, headers: dict = None) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        return response.text\n\n# Manual cache control\nget_user.cache_clear()  # Clear all\nget_user.cache_delete(42)  # Delete specific key\n```\n\nFeatures:\n1. Works with both sync and async functions\n2. TTL (time-to-live) support\n3. LRU eviction when maxsize reached\n4. Custom key functions\n5. Thread-safe\n6. Manual cache control methods\n7. Shared or isolated cache instances"}
{"instruction": "Build a simple task queue with priority support using asyncio", "output": "Here's a priority task queue implementation:\n\n```python\nimport asyncio\nimport heapq\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Coroutine, Optional\nfrom datetime import datetime\nimport uuid\nimport logging\nfrom enum import IntEnum\n\nlogger = logging.getLogger(__name__)\n\nclass Priority(IntEnum):\n    LOW = 3\n    NORMAL = 2\n    HIGH = 1\n    CRITICAL = 0\n\n@dataclass(order=True)\nclass Task:\n    priority: int\n    created_at: float = field(compare=True)\n    id: str = field(default_factory=lambda: str(uuid.uuid4()), compare=False)\n    func: Callable = field(compare=False)\n    args: tuple = field(default=(), compare=False)\n    kwargs: dict = field(default_factory=dict, compare=False)\n    result: Any = field(default=None, compare=False)\n    error: Optional[Exception] = field(default=None, compare=False)\n    completed: bool = field(default=False, compare=False)\n\nclass TaskQueue:\n    def __init__(self, num_workers: int = 3):\n        self._heap: list[Task] = []\n        self._num_workers = num_workers\n        self._workers: list[asyncio.Task] = []\n        self._running = False\n        self._task_added = asyncio.Event()\n        self._results: dict[str, Task] = {}\n        self._lock = asyncio.Lock()\n    \n    async def add(\n        self,\n        func: Callable[..., Coroutine],\n        *args,\n        priority: Priority = Priority.NORMAL,\n        **kwargs\n    ) -> str:\n        \"\"\"Add a task to the queue. Returns task ID.\"\"\"\n        task = Task(\n            priority=priority,\n            created_at=datetime.now().timestamp(),\n            func=func,\n            args=args,\n            kwargs=kwargs\n        )\n        \n        async with self._lock:\n            heapq.heappush(self._heap, task)\n            self._results[task.id] = task\n        \n        self._task_added.set()\n        logger.debug(f'Added task {task.id} with priority {priority.name}')\n        return task.id\n    \n    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Wait for task completion and return result.\"\"\"\n        start = asyncio.get_event_loop().time()\n        \n        while True:\n            if task_id in self._results:\n                task = self._results[task_id]\n                if task.completed:\n                    if task.error:\n                        raise task.error\n                    return task.result\n            \n            if timeout:\n                elapsed = asyncio.get_event_loop().time() - start\n                if elapsed >= timeout:\n                    raise TimeoutError(f'Task {task_id} did not complete in {timeout}s')\n            \n            await asyncio.sleep(0.1)\n    \n    async def _worker(self, worker_id: int):\n        logger.info(f'Worker {worker_id} started')\n        \n        while self._running:\n            # Wait for tasks\n            task = None\n            async with self._lock:\n                if self._heap:\n                    task = heapq.heappop(self._heap)\n            \n            if not task:\n                self._task_added.clear()\n                try:\n                    await asyncio.wait_for(self._task_added.wait(), timeout=1.0)\n                except asyncio.TimeoutError:\n                    pass\n                continue\n            \n            # Execute task\n            logger.debug(f'Worker {worker_id} executing task {task.id}')\n            try:\n                if asyncio.iscoroutinefunction(task.func):\n                    task.result = await task.func(*task.args, **task.kwargs)\n                else:\n                    task.result = await asyncio.to_thread(task.func, *task.args, **task.kwargs)\n            except Exception as e:\n                task.error = e\n                logger.error(f'Task {task.id} failed: {e}')\n            finally:\n                task.completed = True\n        \n        logger.info(f'Worker {worker_id} stopped')\n    \n    async def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        self._workers = [\n            asyncio.create_task(self._worker(i))\n            for i in range(self._num_workers)\n        ]\n        logger.info(f'TaskQueue started with {self._num_workers} workers')\n    \n    async def stop(self, wait_for_completion: bool = True):\n        if wait_for_completion:\n            # Wait for queue to drain\n            while self._heap:\n                await asyncio.sleep(0.1)\n        \n        self._running = False\n        self._task_added.set()  # Wake up workers\n        \n        await asyncio.gather(*self._workers, return_exceptions=True)\n        logger.info('TaskQueue stopped')\n    \n    @property\n    def pending_count(self) -> int:\n        return len(self._heap)\n    \n    def get_status(self, task_id: str) -> dict:\n        if task_id not in self._results:\n            return {'status': 'not_found'}\n        \n        task = self._results[task_id]\n        return {\n            'id': task.id,\n            'status': 'completed' if task.completed else 'pending',\n            'priority': Priority(task.priority).name,\n            'error': str(task.error) if task.error else None\n        }\n\n# Example usage:\nasync def slow_task(name: str, delay: float) -> str:\n    await asyncio.sleep(delay)\n    return f'Task {name} completed after {delay}s'\n\nasync def main():\n    queue = TaskQueue(num_workers=2)\n    await queue.start()\n    \n    # Add tasks with different priorities\n    id1 = await queue.add(slow_task, 'low', 2.0, priority=Priority.LOW)\n    id2 = await queue.add(slow_task, 'critical', 1.0, priority=Priority.CRITICAL)\n    id3 = await queue.add(slow_task, 'normal', 1.5, priority=Priority.NORMAL)\n    \n    # Critical will run first despite being added second\n    result = await queue.get_result(id2)\n    print(result)  # 'Task critical completed after 1.0s'\n    \n    await queue.stop()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nFeatures:\n1. Priority levels (CRITICAL, HIGH, NORMAL, LOW)\n2. Multiple worker support\n3. Async and sync function support\n4. Result retrieval with timeout\n5. Graceful shutdown\n6. Task status tracking\n7. Thread-safe operations"}
{"instruction": "Implement a circuit breaker pattern for external API calls", "output": "Here's a robust circuit breaker implementation:\n\n```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional, Any, Type\nimport functools\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'      # Normal operation\n    OPEN = 'open'          # Failing, reject calls\n    HALF_OPEN = 'half_open'  # Testing recovery\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    last_success_time: Optional[float] = None\n    consecutive_failures: int = 0\n    consecutive_successes: int = 0\n\nclass CircuitBreakerOpen(Exception):\n    \"\"\"Raised when circuit breaker is open.\"\"\"\n    def __init__(self, name: str, retry_after: float):\n        self.name = name\n        self.retry_after = retry_after\n        super().__init__(f'Circuit {name} is open. Retry after {retry_after:.1f}s')\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        name: str,\n        failure_threshold: int = 5,\n        success_threshold: int = 3,\n        timeout: float = 30.0,\n        exceptions: tuple[Type[Exception], ...] = (Exception,),\n        exclude: tuple[Type[Exception], ...] = ()\n    ):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.success_threshold = success_threshold\n        self.timeout = timeout\n        self.exceptions = exceptions\n        self.exclude = exclude\n        \n        self._state = CircuitState.CLOSED\n        self._stats = CircuitStats()\n        self._opened_at: Optional[float] = None\n        self._lock = asyncio.Lock()\n    \n    @property\n    def state(self) -> CircuitState:\n        if self._state == CircuitState.OPEN:\n            if time.time() - self._opened_at >= self.timeout:\n                return CircuitState.HALF_OPEN\n        return self._state\n    \n    async def _on_success(self):\n        async with self._lock:\n            self._stats.successes += 1\n            self._stats.last_success_time = time.time()\n            self._stats.consecutive_successes += 1\n            self._stats.consecutive_failures = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                if self._stats.consecutive_successes >= self.success_threshold:\n                    logger.info(f'Circuit {self.name}: HALF_OPEN -> CLOSED')\n                    self._state = CircuitState.CLOSED\n                    self._stats.consecutive_successes = 0\n    \n    async def _on_failure(self, error: Exception):\n        async with self._lock:\n            self._stats.failures += 1\n            self._stats.last_failure_time = time.time()\n            self._stats.consecutive_failures += 1\n            self._stats.consecutive_successes = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                logger.warning(f'Circuit {self.name}: HALF_OPEN -> OPEN (test failed)')\n                self._state = CircuitState.OPEN\n                self._opened_at = time.time()\n            \n            elif self._state == CircuitState.CLOSED:\n                if self._stats.consecutive_failures >= self.failure_threshold:\n                    logger.warning(f'Circuit {self.name}: CLOSED -> OPEN (threshold reached)')\n                    self._state = CircuitState.OPEN\n                    self._opened_at = time.time()\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        state = self.state  # Check with timeout logic\n        \n        if state == CircuitState.OPEN:\n            retry_after = self.timeout - (time.time() - self._opened_at)\n            raise CircuitBreakerOpen(self.name, retry_after)\n        \n        try:\n            if asyncio.iscoroutinefunction(func):\n                result = await func(*args, **kwargs)\n            else:\n                result = await asyncio.to_thread(func, *args, **kwargs)\n            \n            await self._on_success()\n            return result\n        \n        except self.exclude:\n            # These exceptions don't count as failures\n            raise\n        except self.exceptions as e:\n            await self._on_failure(e)\n            raise\n    \n    def get_stats(self) -> dict:\n        return {\n            'name': self.name,\n            'state': self.state.value,\n            'failures': self._stats.failures,\n            'successes': self._stats.successes,\n            'consecutive_failures': self._stats.consecutive_failures,\n            'consecutive_successes': self._stats.consecutive_successes\n        }\n    \n    async def reset(self):\n        async with self._lock:\n            self._state = CircuitState.CLOSED\n            self._stats = CircuitStats()\n            self._opened_at = None\n\n# Decorator version\ndef circuit_breaker(\n    name: str,\n    failure_threshold: int = 5,\n    success_threshold: int = 3,\n    timeout: float = 30.0,\n    exceptions: tuple = (Exception,)\n) -> Callable:\n    breaker = CircuitBreaker(\n        name=name,\n        failure_threshold=failure_threshold,\n        success_threshold=success_threshold,\n        timeout=timeout,\n        exceptions=exceptions\n    )\n    \n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await breaker.call(func, *args, **kwargs)\n        \n        wrapper.circuit_breaker = breaker\n        return wrapper\n    \n    return decorator\n\n# Usage examples:\n\n# As decorator\n@circuit_breaker('external_api', failure_threshold=3, timeout=60)\nasync def call_external_api(endpoint: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f'https://api.example.com/{endpoint}')\n        response.raise_for_status()\n        return response.json()\n\n# Manual usage\napi_breaker = CircuitBreaker('payment_gateway', failure_threshold=2, timeout=120)\n\nasync def process_payment(amount: float):\n    try:\n        result = await api_breaker.call(payment_api.charge, amount)\n        return result\n    except CircuitBreakerOpen as e:\n        # Handle gracefully - maybe queue for later\n        logger.warning(f'Payment service unavailable: {e}')\n        return {'status': 'queued', 'retry_after': e.retry_after}\n\n# Check stats\nprint(call_external_api.circuit_breaker.get_stats())\n```\n\nFeatures:\n1. Three states: CLOSED (normal), OPEN (failing), HALF_OPEN (testing)\n2. Configurable failure/success thresholds\n3. Automatic timeout and recovery testing\n4. Exception filtering (track specific errors)\n5. Both decorator and manual usage patterns\n6. Statistics tracking\n7. Async-safe with locks\n8. Clear error messages with retry timing"}
{"instruction": "Write a function that fetches user data from an API", "output": "Here's a properly structured implementation:\n\n```python\nfrom typing import Optional\nfrom dataclasses import dataclass\nimport httpx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass User:\n    \"\"\"User data model.\"\"\"\n    id: int\n    name: str\n    email: str\n    is_active: bool = True\n\nclass UserAPIError(Exception):\n    \"\"\"Raised when user API request fails.\"\"\"\n    pass\n\nclass UserNotFoundError(UserAPIError):\n    \"\"\"Raised when user doesn't exist.\"\"\"\n    pass\n\nasync def fetch_user(user_id: int, base_url: str = 'https://api.example.com') -> User:\n    \"\"\"\n    Fetch user data from API.\n    \n    Args:\n        user_id: The user's unique identifier\n        base_url: API base URL\n    \n    Returns:\n        User object with fetched data\n    \n    Raises:\n        UserNotFoundError: If user doesn't exist\n        UserAPIError: If API request fails\n    \"\"\"\n    if user_id <= 0:\n        raise ValueError('user_id must be positive')\n    \n    url = f'{base_url}/users/{user_id}'\n    \n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(url)\n            \n            if response.status_code == 404:\n                raise UserNotFoundError(f'User {user_id} not found')\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            return User(\n                id=data['id'],\n                name=data['name'],\n                email=data['email'],\n                is_active=data.get('is_active', True)\n            )\n    \n    except httpx.TimeoutException:\n        logger.error(f'Timeout fetching user {user_id}')\n        raise UserAPIError('API request timed out')\n    \n    except httpx.HTTPStatusError as e:\n        logger.error(f'HTTP error fetching user {user_id}: {e}')\n        raise UserAPIError(f'API returned status {e.response.status_code}')\n```\n\nKey quality aspects:\n1. **Type hints** on all parameters and return values\n2. **Dataclass** for structured data instead of raw dict\n3. **Custom exceptions** for different error types\n4. **Docstring** with Args, Returns, Raises\n5. **Input validation** before making request\n6. **Specific error handling** for different failure modes\n7. **Logging** for debugging\n8. **Timeout** to prevent hanging"}
{"instruction": "Create a configuration loader", "output": "Here's a production-ready configuration system:\n\n```python\nfrom typing import Any, Optional, TypeVar, Type\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport os\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\nclass ConfigError(Exception):\n    \"\"\"Configuration loading or validation error.\"\"\"\n    pass\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection settings.\"\"\"\n    host: str = 'localhost'\n    port: int = 5432\n    name: str = 'app_db'\n    user: str = 'postgres'\n    password: str = field(default='', repr=False)  # Hide password in logs\n    \n    @property\n    def url(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.name}'\n\n@dataclass\nclass APIConfig:\n    \"\"\"API server settings.\"\"\"\n    host: str = '0.0.0.0'\n    port: int = 8000\n    debug: bool = False\n    cors_origins: list[str] = field(default_factory=lambda: ['http://localhost:3000'])\n\n@dataclass\nclass AppConfig:\n    \"\"\"Main application configuration.\"\"\"\n    app_name: str = 'MyApp'\n    environment: str = 'development'\n    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n    api: APIConfig = field(default_factory=APIConfig)\n    \n    @property\n    def is_production(self) -> bool:\n        return self.environment == 'production'\n\nclass ConfigLoader:\n    \"\"\"\n    Load configuration from multiple sources with priority:\n    1. Environment variables (highest)\n    2. Config file\n    3. Defaults (lowest)\n    \"\"\"\n    \n    def __init__(self, config_path: Optional[Path] = None):\n        self._config_path = config_path\n        self._file_config: dict = {}\n        \n        if config_path and config_path.exists():\n            self._load_file(config_path)\n    \n    def _load_file(self, path: Path) -> None:\n        try:\n            with open(path) as f:\n                self._file_config = json.load(f)\n            logger.info(f'Loaded config from {path}')\n        except json.JSONDecodeError as e:\n            raise ConfigError(f'Invalid JSON in {path}: {e}')\n        except IOError as e:\n            raise ConfigError(f'Cannot read {path}: {e}')\n    \n    def get(self, key: str, default: T = None, required: bool = False) -> T:\n        \"\"\"\n        Get config value with environment override.\n        \n        Environment variables use uppercase with underscores:\n        'database.host' -> DATABASE_HOST\n        \"\"\"\n        # Try environment variable first\n        env_key = key.upper().replace('.', '_')\n        env_value = os.environ.get(env_key)\n        \n        if env_value is not None:\n            return self._convert_type(env_value, type(default) if default else str)\n        \n        # Try config file\n        value = self._get_nested(self._file_config, key)\n        if value is not None:\n            return value\n        \n        # Use default or raise\n        if required and default is None:\n            raise ConfigError(f'Required config missing: {key}')\n        \n        return default\n    \n    def _get_nested(self, data: dict, key: str) -> Any:\n        \"\"\"Get nested value using dot notation.\"\"\"\n        keys = key.split('.')\n        value = data\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return None\n        \n        return value\n    \n    def _convert_type(self, value: str, target_type: Type[T]) -> T:\n        \"\"\"Convert string to target type.\"\"\"\n        if target_type == bool:\n            return value.lower() in ('true', '1', 'yes')\n        if target_type == int:\n            return int(value)\n        if target_type == float:\n            return float(value)\n        if target_type == list:\n            return value.split(',')\n        return value\n    \n    def load_app_config(self) -> AppConfig:\n        \"\"\"Load complete application configuration.\"\"\"\n        return AppConfig(\n            app_name=self.get('app_name', 'MyApp'),\n            environment=self.get('environment', 'development'),\n            database=DatabaseConfig(\n                host=self.get('database.host', 'localhost'),\n                port=self.get('database.port', 5432),\n                name=self.get('database.name', 'app_db'),\n                user=self.get('database.user', 'postgres'),\n                password=self.get('database.password', '', required=False),\n            ),\n            api=APIConfig(\n                host=self.get('api.host', '0.0.0.0'),\n                port=self.get('api.port', 8000),\n                debug=self.get('api.debug', False),\n            )\n        )\n\n# Usage:\nconfig_loader = ConfigLoader(Path('config.json'))\nconfig = config_loader.load_app_config()\n\nprint(config.database.url)  # Uses env var if DATABASE_PASSWORD is set\nprint(config.is_production)  # Property for common checks\n```\n\nQuality aspects:\n1. **Layered config** - env vars override file override defaults\n2. **Type safety** with dataclasses and type hints\n3. **Sensitive data handling** - password hidden from repr\n4. **Dot notation** for nested access\n5. **Automatic type conversion** from env vars\n6. **Required vs optional** distinction\n7. **Custom exceptions** for clear error messages\n8. **Properties** for derived values"}
{"instruction": "Write code to process a list of items", "output": "Here's how to properly structure list processing:\n\n```python\nfrom typing import TypeVar, Callable, Iterator, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\nR = TypeVar('R')\n\nclass ProcessingStatus(Enum):\n    SUCCESS = 'success'\n    FAILED = 'failed'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass ProcessingResult(Generic[T, R]):\n    \"\"\"Result of processing a single item.\"\"\"\n    item: T\n    status: ProcessingStatus\n    result: Optional[R] = None\n    error: Optional[str] = None\n\n@dataclass\nclass BatchResult(Generic[T, R]):\n    \"\"\"Summary of batch processing.\"\"\"\n    total: int\n    successful: int\n    failed: int\n    skipped: int\n    results: list[ProcessingResult[T, R]]\n    \n    @property\n    def success_rate(self) -> float:\n        return self.successful / self.total if self.total > 0 else 0.0\n\ndef process_items(\n    items: list[T],\n    processor: Callable[[T], R],\n    *,\n    skip_condition: Optional[Callable[[T], bool]] = None,\n    on_error: str = 'continue',  # 'continue', 'raise', 'collect'\n    max_workers: int = 1,\n    progress_callback: Optional[Callable[[int, int], None]] = None\n) -> BatchResult[T, R]:\n    \"\"\"\n    Process a list of items with error handling and progress tracking.\n    \n    Args:\n        items: Items to process\n        processor: Function to apply to each item\n        skip_condition: Optional function to skip items\n        on_error: How to handle errors - continue, raise, or collect\n        max_workers: Parallel workers (1 = sequential)\n        progress_callback: Called with (completed, total) after each item\n    \n    Returns:\n        BatchResult with detailed processing summary\n    \"\"\"\n    results: list[ProcessingResult[T, R]] = []\n    total = len(items)\n    \n    def process_single(item: T, index: int) -> ProcessingResult[T, R]:\n        # Check skip condition\n        if skip_condition and skip_condition(item):\n            logger.debug(f'Skipping item {index}: {item}')\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SKIPPED\n            )\n        \n        # Process item\n        try:\n            result = processor(item)\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SUCCESS,\n                result=result\n            )\n        except Exception as e:\n            logger.warning(f'Error processing item {index}: {e}')\n            \n            if on_error == 'raise':\n                raise\n            \n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.FAILED,\n                error=str(e)\n            )\n    \n    # Sequential processing\n    if max_workers == 1:\n        for i, item in enumerate(items):\n            result = process_single(item, i)\n            results.append(result)\n            \n            if progress_callback:\n                progress_callback(i + 1, total)\n    \n    # Parallel processing\n    else:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {\n                executor.submit(process_single, item, i): i\n                for i, item in enumerate(items)\n            }\n            \n            completed = 0\n            for future in as_completed(futures):\n                results.append(future.result())\n                completed += 1\n                \n                if progress_callback:\n                    progress_callback(completed, total)\n    \n    # Build summary\n    successful = sum(1 for r in results if r.status == ProcessingStatus.SUCCESS)\n    failed = sum(1 for r in results if r.status == ProcessingStatus.FAILED)\n    skipped = sum(1 for r in results if r.status == ProcessingStatus.SKIPPED)\n    \n    return BatchResult(\n        total=total,\n        successful=successful,\n        failed=failed,\n        skipped=skipped,\n        results=results\n    )\n\n# Usage example:\norders = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': -50}, {'id': 3, 'amount': 200}]\n\ndef process_order(order: dict) -> dict:\n    if order['amount'] <= 0:\n        raise ValueError('Invalid amount')\n    return {'order_id': order['id'], 'processed': True}\n\nresult = process_items(\n    orders,\n    process_order,\n    skip_condition=lambda o: o.get('skip', False),\n    on_error='continue',\n    progress_callback=lambda done, total: print(f'{done}/{total}')\n)\n\nprint(f'Success rate: {result.success_rate:.1%}')\nfor r in result.results:\n    if r.status == ProcessingStatus.FAILED:\n        print(f'Failed: {r.item} - {r.error}')\n```\n\nQuality aspects:\n1. **Generic types** for reusability\n2. **Dataclasses** for structured results\n3. **Enum** for status instead of magic strings\n4. **Configurable error handling** - don't assume one strategy\n5. **Progress tracking** callback\n6. **Optional parallelization**\n7. **Detailed summary** with success rate\n8. **Docstring** explaining all options"}
{"instruction": "Create a simple database class", "output": "Here's a properly structured database wrapper:\n\n```python\nfrom typing import Optional, Any, TypeVar, Type, Generic\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nfrom abc import ABC, abstractmethod\nimport asyncpg\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection configuration.\"\"\"\n    host: str\n    port: int\n    database: str\n    user: str\n    password: str\n    min_connections: int = 5\n    max_connections: int = 20\n    \n    @property\n    def dsn(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}'\n\nclass DatabaseError(Exception):\n    \"\"\"Base database error.\"\"\"\n    pass\n\nclass ConnectionError(DatabaseError):\n    \"\"\"Failed to connect to database.\"\"\"\n    pass\n\nclass QueryError(DatabaseError):\n    \"\"\"Query execution failed.\"\"\"\n    pass\n\nclass Database:\n    \"\"\"\n    Async PostgreSQL database wrapper with connection pooling.\n    \n    Usage:\n        db = Database(config)\n        await db.connect()\n        \n        users = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\n        \n        async with db.transaction():\n            await db.execute('UPDATE users SET active = $1 WHERE id = $2', False, 123)\n        \n        await db.disconnect()\n    \"\"\"\n    \n    def __init__(self, config: DatabaseConfig):\n        self._config = config\n        self._pool: Optional[asyncpg.Pool] = None\n    \n    @property\n    def is_connected(self) -> bool:\n        return self._pool is not None\n    \n    async def connect(self) -> None:\n        \"\"\"Initialize connection pool.\"\"\"\n        if self._pool is not None:\n            logger.warning('Database already connected')\n            return\n        \n        try:\n            self._pool = await asyncpg.create_pool(\n                dsn=self._config.dsn,\n                min_size=self._config.min_connections,\n                max_size=self._config.max_connections\n            )\n            logger.info(f'Connected to {self._config.host}:{self._config.port}/{self._config.database}')\n        \n        except asyncpg.InvalidCatalogNameError:\n            raise ConnectionError(f'Database {self._config.database} does not exist')\n        \n        except asyncpg.InvalidPasswordError:\n            raise ConnectionError('Invalid database credentials')\n        \n        except OSError as e:\n            raise ConnectionError(f'Cannot reach database server: {e}')\n    \n    async def disconnect(self) -> None:\n        \"\"\"Close all connections.\"\"\"\n        if self._pool is None:\n            return\n        \n        await self._pool.close()\n        self._pool = None\n        logger.info('Database disconnected')\n    \n    def _ensure_connected(self) -> asyncpg.Pool:\n        if self._pool is None:\n            raise DatabaseError('Database not connected. Call connect() first.')\n        return self._pool\n    \n    async def execute(self, query: str, *args: Any) -> str:\n        \"\"\"\n        Execute a query that doesn't return rows.\n        \n        Returns:\n            Status string (e.g., 'UPDATE 5')\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.execute(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_one(self, query: str, *args: Any) -> Optional[dict]:\n        \"\"\"Fetch a single row as dict, or None if not found.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            row = await pool.fetchrow(query, *args)\n            return dict(row) if row else None\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_all(self, query: str, *args: Any) -> list[dict]:\n        \"\"\"Fetch all rows as list of dicts.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            rows = await pool.fetch(query, *args)\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_value(self, query: str, *args: Any) -> Any:\n        \"\"\"Fetch a single value (first column of first row).\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.fetchval(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    @asynccontextmanager\n    async def transaction(self):\n        \"\"\"\n        Transaction context manager with automatic rollback on error.\n        \n        Usage:\n            async with db.transaction():\n                await db.execute(...)\n                await db.execute(...)\n                # Commits if no error, rolls back otherwise\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        async with pool.acquire() as conn:\n            async with conn.transaction():\n                # Temporarily replace pool operations to use this connection\n                original_pool = self._pool\n                \n                class SingleConnectionPool:\n                    async def execute(self, *a): return await conn.execute(*a)\n                    async def fetch(self, *a): return await conn.fetch(*a)\n                    async def fetchrow(self, *a): return await conn.fetchrow(*a)\n                    async def fetchval(self, *a): return await conn.fetchval(*a)\n                \n                self._pool = SingleConnectionPool()\n                try:\n                    yield\n                finally:\n                    self._pool = original_pool\n    \n    async def table_exists(self, table_name: str) -> bool:\n        \"\"\"Check if a table exists.\"\"\"\n        result = await self.fetch_value(\n            \"SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_name = $1)\",\n            table_name\n        )\n        return result\n\n# Usage:\nconfig = DatabaseConfig(\n    host='localhost',\n    port=5432,\n    database='myapp',\n    user='postgres',\n    password='secret'\n)\n\ndb = Database(config)\nawait db.connect()\n\n# Simple queries\nusers = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\ncount = await db.fetch_value('SELECT COUNT(*) FROM users')\n\n# Transactions\nasync with db.transaction():\n    await db.execute('INSERT INTO audit_log (action) VALUES ($1)', 'user_created')\n    await db.execute('INSERT INTO users (name) VALUES ($1)', 'Simon')\n\nawait db.disconnect()\n```\n\nQuality aspects:\n1. **Connection pooling** for performance\n2. **Custom exceptions** hierarchy\n3. **Context manager** for transactions\n4. **Multiple fetch methods** for different use cases\n5. **Connection state checking**\n6. **Detailed logging**\n7. **DSN property** for clean URL building\n8. **Type hints** throughout"}
{"instruction": "Write a function to validate user input", "output": "Here's a comprehensive validation system:\n\n```python\nfrom typing import Any, Callable, Optional, Union, TypeVar\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport re\nfrom enum import Enum\n\nT = TypeVar('T')\n\nclass ValidationSeverity(Enum):\n    ERROR = 'error'      # Blocks submission\n    WARNING = 'warning'  # Allows submission with notice\n\n@dataclass\nclass ValidationError:\n    \"\"\"Single validation error.\"\"\"\n    field: str\n    message: str\n    code: str\n    severity: ValidationSeverity = ValidationSeverity.ERROR\n    value: Any = None\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Complete validation result.\"\"\"\n    is_valid: bool\n    errors: list[ValidationError] = field(default_factory=list)\n    warnings: list[ValidationError] = field(default_factory=list)\n    \n    def add_error(self, field: str, message: str, code: str, value: Any = None):\n        self.errors.append(ValidationError(field, message, code, ValidationSeverity.ERROR, value))\n        self.is_valid = False\n    \n    def add_warning(self, field: str, message: str, code: str, value: Any = None):\n        self.warnings.append(ValidationError(field, message, code, ValidationSeverity.WARNING, value))\n    \n    def merge(self, other: 'ValidationResult') -> 'ValidationResult':\n        \"\"\"Combine two validation results.\"\"\"\n        return ValidationResult(\n            is_valid=self.is_valid and other.is_valid,\n            errors=self.errors + other.errors,\n            warnings=self.warnings + other.warnings\n        )\n\nclass Validator(ABC):\n    \"\"\"Base validator class.\"\"\"\n    \n    @abstractmethod\n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        pass\n\nclass Required(Validator):\n    \"\"\"Validates that value is present and not empty.\"\"\"\n    \n    def __init__(self, message: str = 'This field is required'):\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None or value == '' or (isinstance(value, (list, dict)) and len(value) == 0):\n            result.add_error(field_name, self.message, 'required')\n        \n        return result\n\nclass MinLength(Validator):\n    \"\"\"Validates minimum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be at least {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) < self.length:\n            result.add_error(field_name, self.message, 'min_length', value)\n        \n        return result\n\nclass MaxLength(Validator):\n    \"\"\"Validates maximum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be no more than {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) > self.length:\n            result.add_error(field_name, self.message, 'max_length', value)\n        \n        return result\n\nclass Pattern(Validator):\n    \"\"\"Validates against regex pattern.\"\"\"\n    \n    def __init__(self, pattern: str, message: str = 'Invalid format'):\n        self.pattern = re.compile(pattern)\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.pattern.match(str(value)):\n            result.add_error(field_name, self.message, 'pattern', value)\n        \n        return result\n\nclass Email(Pattern):\n    \"\"\"Validates email format.\"\"\"\n    \n    def __init__(self, message: str = 'Invalid email address'):\n        super().__init__(\n            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n            message\n        )\n\nclass Range(Validator):\n    \"\"\"Validates numeric range.\"\"\"\n    \n    def __init__(self, min_val: Optional[float] = None, max_val: Optional[float] = None):\n        self.min_val = min_val\n        self.max_val = max_val\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None:\n            return result\n        \n        try:\n            num = float(value)\n            \n            if self.min_val is not None and num < self.min_val:\n                result.add_error(field_name, f'Must be at least {self.min_val}', 'min_value', value)\n            \n            if self.max_val is not None and num > self.max_val:\n                result.add_error(field_name, f'Must be no more than {self.max_val}', 'max_value', value)\n        \n        except (TypeError, ValueError):\n            result.add_error(field_name, 'Must be a number', 'not_numeric', value)\n        \n        return result\n\nclass Custom(Validator):\n    \"\"\"Custom validation function.\"\"\"\n    \n    def __init__(self, func: Callable[[Any], bool], message: str, code: str = 'custom'):\n        self.func = func\n        self.message = message\n        self.code = code\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.func(value):\n            result.add_error(field_name, self.message, self.code, value)\n        \n        return result\n\n@dataclass\nclass Field:\n    \"\"\"Field definition with validators.\"\"\"\n    name: str\n    validators: list[Validator] = field(default_factory=list)\n    \n    def validate(self, value: Any) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for validator in self.validators:\n            result = result.merge(validator.validate(value, self.name))\n        \n        return result\n\nclass Schema:\n    \"\"\"Validation schema for objects.\"\"\"\n    \n    def __init__(self, fields: list[Field]):\n        self._fields = {f.name: f for f in fields}\n    \n    def validate(self, data: dict) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for field_name, field in self._fields.items():\n            value = data.get(field_name)\n            result = result.merge(field.validate(value))\n        \n        return result\n\n# Usage example:\nuser_schema = Schema([\n    Field('email', [Required(), Email()]),\n    Field('password', [Required(), MinLength(8), MaxLength(100)]),\n    Field('age', [Range(min_val=18, max_val=120)]),\n    Field('username', [\n        Required(),\n        MinLength(3),\n        MaxLength(20),\n        Pattern(r'^[a-zA-Z0-9_]+$', 'Only letters, numbers, and underscores allowed'),\n        Custom(\n            lambda x: x.lower() not in ['admin', 'root', 'system'],\n            'This username is reserved',\n            'reserved_username'\n        )\n    ])\n])\n\n# Validate\ndata = {\n    'email': 'invalid-email',\n    'password': '123',\n    'age': 15,\n    'username': 'admin'\n}\n\nresult = user_schema.validate(data)\n\nif not result.is_valid:\n    for error in result.errors:\n        print(f'{error.field}: {error.message} (code: {error.code})')\n\n# Output:\n# email: Invalid email address (code: pattern)\n# password: Must be at least 8 characters (code: min_length)\n# age: Must be at least 18 (code: min_value)\n# username: This username is reserved (code: reserved_username)\n```\n\nQuality aspects:\n1. **Composable validators** - mix and match\n2. **Custom validators** - easy extension\n3. **Error codes** for programmatic handling\n4. **Severity levels** - errors vs warnings\n5. **Schema-based** validation for objects\n6. **Detailed error info** including invalid value\n7. **Immutable results** with merge support\n8. **Pre-built common validators** (email, range, pattern)"}
{"instruction": "Write a logging setup for an application", "output": "Here's a production-ready logging configuration:\n\n```python\nimport logging\nimport logging.handlers\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport traceback\n\nclass LogLevel(Enum):\n    DEBUG = 'DEBUG'\n    INFO = 'INFO'\n    WARNING = 'WARNING'\n    ERROR = 'ERROR'\n    CRITICAL = 'CRITICAL'\n\n@dataclass\nclass LogConfig:\n    \"\"\"Logging configuration.\"\"\"\n    app_name: str\n    level: LogLevel = LogLevel.INFO\n    log_dir: Optional[Path] = None\n    max_file_size_mb: int = 10\n    backup_count: int = 5\n    json_format: bool = False\n    include_hostname: bool = False\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"\n    JSON log formatter for structured logging.\n    Ideal for log aggregation systems (ELK, Datadog, etc.)\n    \"\"\"\n    \n    def __init__(self, app_name: str, include_hostname: bool = False):\n        super().__init__()\n        self.app_name = app_name\n        self.include_hostname = include_hostname\n        \n        if include_hostname:\n            import socket\n            self.hostname = socket.gethostname()\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_data = {\n            'timestamp': datetime.utcnow().isoformat() + 'Z',\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'app': self.app_name,\n        }\n        \n        # Add location info\n        if record.levelno >= logging.WARNING:\n            log_data['location'] = {\n                'file': record.filename,\n                'line': record.lineno,\n                'function': record.funcName\n            }\n        \n        # Add hostname if configured\n        if self.include_hostname:\n            log_data['hostname'] = self.hostname\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_data['exception'] = {\n                'type': record.exc_info[0].__name__ if record.exc_info[0] else None,\n                'message': str(record.exc_info[1]) if record.exc_info[1] else None,\n                'traceback': traceback.format_exception(*record.exc_info)\n            }\n        \n        # Add extra fields\n        for key, value in record.__dict__.items():\n            if key not in [\n                'name', 'msg', 'args', 'created', 'filename', 'funcName',\n                'levelname', 'levelno', 'lineno', 'module', 'msecs',\n                'pathname', 'process', 'processName', 'relativeCreated',\n                'stack_info', 'exc_info', 'exc_text', 'thread', 'threadName',\n                'message', 'taskName'\n            ]:\n                log_data[key] = value\n        \n        return json.dumps(log_data, default=str)\n\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Colored console output for development.\"\"\"\n    \n    COLORS = {\n        'DEBUG': '\\033[36m',     # Cyan\n        'INFO': '\\033[32m',      # Green\n        'WARNING': '\\033[33m',   # Yellow\n        'ERROR': '\\033[31m',     # Red\n        'CRITICAL': '\\033[35m',  # Magenta\n    }\n    RESET = '\\033[0m'\n    \n    def format(self, record: logging.LogRecord) -> str:\n        color = self.COLORS.get(record.levelname, '')\n        \n        # Format: [TIME] LEVEL logger: message\n        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')\n        \n        formatted = f'{color}[{timestamp}] {record.levelname:8}{self.RESET} '\n        formatted += f'{record.name}: {record.getMessage()}'\n        \n        if record.exc_info:\n            formatted += '\\n' + self.formatException(record.exc_info)\n        \n        return formatted\n\ndef setup_logging(config: LogConfig) -> logging.Logger:\n    \"\"\"\n    Configure application logging.\n    \n    Sets up:\n    - Console handler (colored in dev, JSON optional)\n    - File handler with rotation (if log_dir specified)\n    - Separate error log file\n    \n    Returns:\n        Root logger for the application\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(config.app_name)\n    logger.setLevel(getattr(logging, config.level.value))\n    \n    # Remove existing handlers\n    logger.handlers.clear()\n    \n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    \n    if config.json_format:\n        console_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n    else:\n        console_handler.setFormatter(ColoredFormatter())\n    \n    logger.addHandler(console_handler)\n    \n    # File handlers (if log directory specified)\n    if config.log_dir:\n        config.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Main log file (rotating)\n        main_log = config.log_dir / f'{config.app_name}.log'\n        file_handler = logging.handlers.RotatingFileHandler(\n            main_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(file_handler)\n        \n        # Error log file (errors and above only)\n        error_log = config.log_dir / f'{config.app_name}.error.log'\n        error_handler = logging.handlers.RotatingFileHandler(\n            error_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        error_handler.setLevel(logging.ERROR)\n        error_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(error_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    logger.info(f'Logging initialized', extra={'config': config.__dict__})\n    \n    return logger\n\nclass LoggerMixin:\n    \"\"\"Mixin to add logger to any class.\"\"\"\n    \n    @property\n    def logger(self) -> logging.Logger:\n        if not hasattr(self, '_logger'):\n            self._logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')\n        return self._logger\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a child logger.\"\"\"\n    return logging.getLogger(name)\n\n# Context manager for adding extra fields\nclass LogContext:\n    \"\"\"Add extra fields to all logs within context.\"\"\"\n    \n    def __init__(self, logger: logging.Logger, **extra: Any):\n        self.logger = logger\n        self.extra = extra\n        self.old_factory = None\n    \n    def __enter__(self):\n        self.old_factory = logging.getLogRecordFactory()\n        extra = self.extra\n        \n        def factory(*args, **kwargs):\n            record = self.old_factory(*args, **kwargs)\n            for key, value in extra.items():\n                setattr(record, key, value)\n            return record\n        \n        logging.setLogRecordFactory(factory)\n        return self\n    \n    def __exit__(self, *args):\n        logging.setLogRecordFactory(self.old_factory)\n\n# Usage:\nconfig = LogConfig(\n    app_name='myapp',\n    level=LogLevel.DEBUG,\n    log_dir=Path('./logs'),\n    json_format=False  # Set True for production\n)\n\nlogger = setup_logging(config)\n\n# Basic logging\nlogger.info('Application started')\nlogger.warning('Low memory', extra={'available_mb': 256})\n\n# With context\nwith LogContext(logger, request_id='abc-123', user_id=42):\n    logger.info('Processing request')  # Includes request_id and user_id\n\n# In classes\nclass UserService(LoggerMixin):\n    def create_user(self, name: str):\n        self.logger.info(f'Creating user: {name}')\n\ntry:\n    raise ValueError('Something went wrong')\nexcept Exception:\n    logger.exception('Failed to process')  # Includes full traceback\n```\n\nQuality aspects:\n1. **Structured JSON logging** for production\n2. **Colored output** for development\n3. **Log rotation** to prevent disk fill\n4. **Separate error log** for easy monitoring\n5. **Context manager** for request tracing\n6. **LoggerMixin** for easy class integration\n7. **Extra fields** support\n8. **Configurable** via dataclass"}
{"instruction": "Create a health check endpoint", "output": "Here's a comprehensive health check system:\n\n```python\nfrom typing import Optional, Callable, Any\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nimport asyncio\nimport time\nimport logging\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\nclass HealthStatus(str, Enum):\n    HEALTHY = 'healthy'\n    DEGRADED = 'degraded'  # Working but with issues\n    UNHEALTHY = 'unhealthy'\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"Health status of a single component.\"\"\"\n    name: str\n    status: HealthStatus\n    message: Optional[str] = None\n    response_time_ms: Optional[float] = None\n    last_check: Optional[datetime] = None\n    details: dict = field(default_factory=dict)\n\nclass HealthCheckResponse(BaseModel):\n    \"\"\"API response model.\"\"\"\n    status: HealthStatus\n    timestamp: str\n    version: str\n    uptime_seconds: float\n    components: dict[str, dict]\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Individual health check definition.\"\"\"\n    name: str\n    check_func: Callable[[], Any]  # Returns True/False or raises\n    timeout_seconds: float = 5.0\n    critical: bool = True  # If False, failure only causes DEGRADED\n    \n    async def run(self) -> ComponentHealth:\n        \"\"\"Execute health check with timeout.\"\"\"\n        start = time.time()\n        \n        try:\n            if asyncio.iscoroutinefunction(self.check_func):\n                result = await asyncio.wait_for(\n                    self.check_func(),\n                    timeout=self.timeout_seconds\n                )\n            else:\n                result = await asyncio.wait_for(\n                    asyncio.to_thread(self.check_func),\n                    timeout=self.timeout_seconds\n                )\n            \n            elapsed_ms = (time.time() - start) * 1000\n            \n            if result is True or result is None:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n            elif isinstance(result, dict):\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow(),\n                    details=result\n                )\n            else:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n        \n        except asyncio.TimeoutError:\n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=f'Timeout after {self.timeout_seconds}s',\n                response_time_ms=self.timeout_seconds * 1000,\n                last_check=datetime.utcnow()\n            )\n        \n        except Exception as e:\n            elapsed_ms = (time.time() - start) * 1000\n            logger.warning(f'Health check {self.name} failed: {e}')\n            \n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=str(e),\n                response_time_ms=elapsed_ms,\n                last_check=datetime.utcnow()\n            )\n\nclass HealthChecker:\n    \"\"\"\n    Manages health checks for an application.\n    \n    Usage:\n        health = HealthChecker(app_version='1.0.0')\n        \n        health.add_check('database', check_database, critical=True)\n        health.add_check('cache', check_redis, critical=False)\n        \n        result = await health.check_all()\n    \"\"\"\n    \n    def __init__(self, app_version: str = '1.0.0'):\n        self._checks: list[HealthCheck] = []\n        self._start_time = datetime.utcnow()\n        self._version = app_version\n        self._last_result: Optional[HealthCheckResponse] = None\n        self._cache_ttl = timedelta(seconds=5)  # Don't check more than every 5s\n        self._last_check_time: Optional[datetime] = None\n    \n    def add_check(\n        self,\n        name: str,\n        check_func: Callable,\n        timeout: float = 5.0,\n        critical: bool = True\n    ) -> 'HealthChecker':\n        \"\"\"Add a health check. Returns self for chaining.\"\"\"\n        self._checks.append(HealthCheck(\n            name=name,\n            check_func=check_func,\n            timeout_seconds=timeout,\n            critical=critical\n        ))\n        return self\n    \n    async def check_all(self, use_cache: bool = True) -> HealthCheckResponse:\n        \"\"\"\n        Run all health checks.\n        \n        Args:\n            use_cache: Return cached result if within TTL\n        \"\"\"\n        # Return cached result if fresh\n        if use_cache and self._last_result and self._last_check_time:\n            if datetime.utcnow() - self._last_check_time < self._cache_ttl:\n                return self._last_result\n        \n        # Run all checks in parallel\n        results = await asyncio.gather(\n            *[check.run() for check in self._checks],\n            return_exceptions=True\n        )\n        \n        # Process results\n        components = {}\n        overall_status = HealthStatus.HEALTHY\n        \n        for check, result in zip(self._checks, results):\n            if isinstance(result, Exception):\n                result = ComponentHealth(\n                    name=check.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    last_check=datetime.utcnow()\n                )\n            \n            components[result.name] = {\n                'status': result.status.value,\n                'message': result.message,\n                'response_time_ms': result.response_time_ms,\n                'details': result.details\n            }\n            \n            # Update overall status\n            if result.status == HealthStatus.UNHEALTHY:\n                if check.critical:\n                    overall_status = HealthStatus.UNHEALTHY\n                elif overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n            elif result.status == HealthStatus.DEGRADED:\n                if overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n        \n        uptime = (datetime.utcnow() - self._start_time).total_seconds()\n        \n        response = HealthCheckResponse(\n            status=overall_status,\n            timestamp=datetime.utcnow().isoformat() + 'Z',\n            version=self._version,\n            uptime_seconds=uptime,\n            components=components\n        )\n        \n        self._last_result = response\n        self._last_check_time = datetime.utcnow()\n        \n        return response\n    \n    async def is_healthy(self) -> bool:\n        \"\"\"Quick check if system is healthy.\"\"\"\n        result = await self.check_all()\n        return result.status == HealthStatus.HEALTHY\n\n# FastAPI integration\ndef setup_health_routes(app: FastAPI, health_checker: HealthChecker):\n    \"\"\"Add health check routes to FastAPI app.\"\"\"\n    \n    @app.get('/health', response_model=HealthCheckResponse)\n    async def health_check(response: Response):\n        \"\"\"Full health check with component details.\"\"\"\n        result = await health_checker.check_all()\n        \n        if result.status == HealthStatus.UNHEALTHY:\n            response.status_code = 503\n        elif result.status == HealthStatus.DEGRADED:\n            response.status_code = 200  # Still serving requests\n        \n        return result\n    \n    @app.get('/health/live')\n    async def liveness():\n        \"\"\"Kubernetes liveness probe - is the app running?\"\"\"\n        return {'status': 'alive'}\n    \n    @app.get('/health/ready')\n    async def readiness(response: Response):\n        \"\"\"Kubernetes readiness probe - can it serve traffic?\"\"\"\n        is_ready = await health_checker.is_healthy()\n        \n        if not is_ready:\n            response.status_code = 503\n            return {'status': 'not_ready'}\n        \n        return {'status': 'ready'}\n\n# Example check functions:\nasync def check_database(db) -> bool:\n    await db.execute('SELECT 1')\n    return True\n\nasync def check_redis(redis) -> dict:\n    info = await redis.info()\n    return {\n        'connected_clients': info['connected_clients'],\n        'used_memory_mb': info['used_memory'] / 1024 / 1024\n    }\n\ndef check_disk_space() -> bool:\n    import shutil\n    usage = shutil.disk_usage('/')\n    free_percent = usage.free / usage.total * 100\n    \n    if free_percent < 10:\n        raise Exception(f'Low disk space: {free_percent:.1f}% free')\n    \n    return True\n\n# Usage:\napp = FastAPI()\n\nhealth = HealthChecker(app_version='2.1.0')\nhealth.add_check('database', lambda: check_database(db), critical=True)\nhealth.add_check('redis', lambda: check_redis(redis), critical=False)\nhealth.add_check('disk', check_disk_space, timeout=2.0, critical=True)\n\nsetup_health_routes(app, health)\n```\n\nQuality aspects:\n1. **Three status levels** - healthy, degraded, unhealthy\n2. **Critical vs non-critical** - degraded mode support\n3. **Kubernetes-ready** - liveness and readiness probes\n4. **Response caching** - don't overload checks\n5. **Parallel execution** - fast even with many checks\n6. **Detailed component info** - timing, messages, details\n7. **Timeout handling** - prevent hanging checks\n8. **Proper HTTP status codes** - 503 when unhealthy"}
