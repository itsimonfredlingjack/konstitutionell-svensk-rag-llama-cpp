{"instruction": "Write a function that fetches user data from an API", "output": "Here's a properly structured implementation:\n\n```python\nfrom typing import Optional\nfrom dataclasses import dataclass\nimport httpx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass User:\n    \"\"\"User data model.\"\"\"\n    id: int\n    name: str\n    email: str\n    is_active: bool = True\n\nclass UserAPIError(Exception):\n    \"\"\"Raised when user API request fails.\"\"\"\n    pass\n\nclass UserNotFoundError(UserAPIError):\n    \"\"\"Raised when user doesn't exist.\"\"\"\n    pass\n\nasync def fetch_user(user_id: int, base_url: str = 'https://api.example.com') -> User:\n    \"\"\"\n    Fetch user data from API.\n    \n    Args:\n        user_id: The user's unique identifier\n        base_url: API base URL\n    \n    Returns:\n        User object with fetched data\n    \n    Raises:\n        UserNotFoundError: If user doesn't exist\n        UserAPIError: If API request fails\n    \"\"\"\n    if user_id <= 0:\n        raise ValueError('user_id must be positive')\n    \n    url = f'{base_url}/users/{user_id}'\n    \n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(url)\n            \n            if response.status_code == 404:\n                raise UserNotFoundError(f'User {user_id} not found')\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            return User(\n                id=data['id'],\n                name=data['name'],\n                email=data['email'],\n                is_active=data.get('is_active', True)\n            )\n    \n    except httpx.TimeoutException:\n        logger.error(f'Timeout fetching user {user_id}')\n        raise UserAPIError('API request timed out')\n    \n    except httpx.HTTPStatusError as e:\n        logger.error(f'HTTP error fetching user {user_id}: {e}')\n        raise UserAPIError(f'API returned status {e.response.status_code}')\n```\n\nKey quality aspects:\n1. **Type hints** on all parameters and return values\n2. **Dataclass** for structured data instead of raw dict\n3. **Custom exceptions** for different error types\n4. **Docstring** with Args, Returns, Raises\n5. **Input validation** before making request\n6. **Specific error handling** for different failure modes\n7. **Logging** for debugging\n8. **Timeout** to prevent hanging"}
{"instruction": "Create a configuration loader", "output": "Here's a production-ready configuration system:\n\n```python\nfrom typing import Any, Optional, TypeVar, Type\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport os\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\nclass ConfigError(Exception):\n    \"\"\"Configuration loading or validation error.\"\"\"\n    pass\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection settings.\"\"\"\n    host: str = 'localhost'\n    port: int = 5432\n    name: str = 'app_db'\n    user: str = 'postgres'\n    password: str = field(default='', repr=False)  # Hide password in logs\n    \n    @property\n    def url(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.name}'\n\n@dataclass\nclass APIConfig:\n    \"\"\"API server settings.\"\"\"\n    host: str = '0.0.0.0'\n    port: int = 8000\n    debug: bool = False\n    cors_origins: list[str] = field(default_factory=lambda: ['http://localhost:3000'])\n\n@dataclass\nclass AppConfig:\n    \"\"\"Main application configuration.\"\"\"\n    app_name: str = 'MyApp'\n    environment: str = 'development'\n    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n    api: APIConfig = field(default_factory=APIConfig)\n    \n    @property\n    def is_production(self) -> bool:\n        return self.environment == 'production'\n\nclass ConfigLoader:\n    \"\"\"\n    Load configuration from multiple sources with priority:\n    1. Environment variables (highest)\n    2. Config file\n    3. Defaults (lowest)\n    \"\"\"\n    \n    def __init__(self, config_path: Optional[Path] = None):\n        self._config_path = config_path\n        self._file_config: dict = {}\n        \n        if config_path and config_path.exists():\n            self._load_file(config_path)\n    \n    def _load_file(self, path: Path) -> None:\n        try:\n            with open(path) as f:\n                self._file_config = json.load(f)\n            logger.info(f'Loaded config from {path}')\n        except json.JSONDecodeError as e:\n            raise ConfigError(f'Invalid JSON in {path}: {e}')\n        except IOError as e:\n            raise ConfigError(f'Cannot read {path}: {e}')\n    \n    def get(self, key: str, default: T = None, required: bool = False) -> T:\n        \"\"\"\n        Get config value with environment override.\n        \n        Environment variables use uppercase with underscores:\n        'database.host' -> DATABASE_HOST\n        \"\"\"\n        # Try environment variable first\n        env_key = key.upper().replace('.', '_')\n        env_value = os.environ.get(env_key)\n        \n        if env_value is not None:\n            return self._convert_type(env_value, type(default) if default else str)\n        \n        # Try config file\n        value = self._get_nested(self._file_config, key)\n        if value is not None:\n            return value\n        \n        # Use default or raise\n        if required and default is None:\n            raise ConfigError(f'Required config missing: {key}')\n        \n        return default\n    \n    def _get_nested(self, data: dict, key: str) -> Any:\n        \"\"\"Get nested value using dot notation.\"\"\"\n        keys = key.split('.')\n        value = data\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return None\n        \n        return value\n    \n    def _convert_type(self, value: str, target_type: Type[T]) -> T:\n        \"\"\"Convert string to target type.\"\"\"\n        if target_type == bool:\n            return value.lower() in ('true', '1', 'yes')\n        if target_type == int:\n            return int(value)\n        if target_type == float:\n            return float(value)\n        if target_type == list:\n            return value.split(',')\n        return value\n    \n    def load_app_config(self) -> AppConfig:\n        \"\"\"Load complete application configuration.\"\"\"\n        return AppConfig(\n            app_name=self.get('app_name', 'MyApp'),\n            environment=self.get('environment', 'development'),\n            database=DatabaseConfig(\n                host=self.get('database.host', 'localhost'),\n                port=self.get('database.port', 5432),\n                name=self.get('database.name', 'app_db'),\n                user=self.get('database.user', 'postgres'),\n                password=self.get('database.password', '', required=False),\n            ),\n            api=APIConfig(\n                host=self.get('api.host', '0.0.0.0'),\n                port=self.get('api.port', 8000),\n                debug=self.get('api.debug', False),\n            )\n        )\n\n# Usage:\nconfig_loader = ConfigLoader(Path('config.json'))\nconfig = config_loader.load_app_config()\n\nprint(config.database.url)  # Uses env var if DATABASE_PASSWORD is set\nprint(config.is_production)  # Property for common checks\n```\n\nQuality aspects:\n1. **Layered config** - env vars override file override defaults\n2. **Type safety** with dataclasses and type hints\n3. **Sensitive data handling** - password hidden from repr\n4. **Dot notation** for nested access\n5. **Automatic type conversion** from env vars\n6. **Required vs optional** distinction\n7. **Custom exceptions** for clear error messages\n8. **Properties** for derived values"}
{"instruction": "Write code to process a list of items", "output": "Here's how to properly structure list processing:\n\n```python\nfrom typing import TypeVar, Callable, Iterator, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\nR = TypeVar('R')\n\nclass ProcessingStatus(Enum):\n    SUCCESS = 'success'\n    FAILED = 'failed'\n    SKIPPED = 'skipped'\n\n@dataclass\nclass ProcessingResult(Generic[T, R]):\n    \"\"\"Result of processing a single item.\"\"\"\n    item: T\n    status: ProcessingStatus\n    result: Optional[R] = None\n    error: Optional[str] = None\n\n@dataclass\nclass BatchResult(Generic[T, R]):\n    \"\"\"Summary of batch processing.\"\"\"\n    total: int\n    successful: int\n    failed: int\n    skipped: int\n    results: list[ProcessingResult[T, R]]\n    \n    @property\n    def success_rate(self) -> float:\n        return self.successful / self.total if self.total > 0 else 0.0\n\ndef process_items(\n    items: list[T],\n    processor: Callable[[T], R],\n    *,\n    skip_condition: Optional[Callable[[T], bool]] = None,\n    on_error: str = 'continue',  # 'continue', 'raise', 'collect'\n    max_workers: int = 1,\n    progress_callback: Optional[Callable[[int, int], None]] = None\n) -> BatchResult[T, R]:\n    \"\"\"\n    Process a list of items with error handling and progress tracking.\n    \n    Args:\n        items: Items to process\n        processor: Function to apply to each item\n        skip_condition: Optional function to skip items\n        on_error: How to handle errors - continue, raise, or collect\n        max_workers: Parallel workers (1 = sequential)\n        progress_callback: Called with (completed, total) after each item\n    \n    Returns:\n        BatchResult with detailed processing summary\n    \"\"\"\n    results: list[ProcessingResult[T, R]] = []\n    total = len(items)\n    \n    def process_single(item: T, index: int) -> ProcessingResult[T, R]:\n        # Check skip condition\n        if skip_condition and skip_condition(item):\n            logger.debug(f'Skipping item {index}: {item}')\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SKIPPED\n            )\n        \n        # Process item\n        try:\n            result = processor(item)\n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.SUCCESS,\n                result=result\n            )\n        except Exception as e:\n            logger.warning(f'Error processing item {index}: {e}')\n            \n            if on_error == 'raise':\n                raise\n            \n            return ProcessingResult(\n                item=item,\n                status=ProcessingStatus.FAILED,\n                error=str(e)\n            )\n    \n    # Sequential processing\n    if max_workers == 1:\n        for i, item in enumerate(items):\n            result = process_single(item, i)\n            results.append(result)\n            \n            if progress_callback:\n                progress_callback(i + 1, total)\n    \n    # Parallel processing\n    else:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {\n                executor.submit(process_single, item, i): i\n                for i, item in enumerate(items)\n            }\n            \n            completed = 0\n            for future in as_completed(futures):\n                results.append(future.result())\n                completed += 1\n                \n                if progress_callback:\n                    progress_callback(completed, total)\n    \n    # Build summary\n    successful = sum(1 for r in results if r.status == ProcessingStatus.SUCCESS)\n    failed = sum(1 for r in results if r.status == ProcessingStatus.FAILED)\n    skipped = sum(1 for r in results if r.status == ProcessingStatus.SKIPPED)\n    \n    return BatchResult(\n        total=total,\n        successful=successful,\n        failed=failed,\n        skipped=skipped,\n        results=results\n    )\n\n# Usage example:\norders = [{'id': 1, 'amount': 100}, {'id': 2, 'amount': -50}, {'id': 3, 'amount': 200}]\n\ndef process_order(order: dict) -> dict:\n    if order['amount'] <= 0:\n        raise ValueError('Invalid amount')\n    return {'order_id': order['id'], 'processed': True}\n\nresult = process_items(\n    orders,\n    process_order,\n    skip_condition=lambda o: o.get('skip', False),\n    on_error='continue',\n    progress_callback=lambda done, total: print(f'{done}/{total}')\n)\n\nprint(f'Success rate: {result.success_rate:.1%}')\nfor r in result.results:\n    if r.status == ProcessingStatus.FAILED:\n        print(f'Failed: {r.item} - {r.error}')\n```\n\nQuality aspects:\n1. **Generic types** for reusability\n2. **Dataclasses** for structured results\n3. **Enum** for status instead of magic strings\n4. **Configurable error handling** - don't assume one strategy\n5. **Progress tracking** callback\n6. **Optional parallelization**\n7. **Detailed summary** with success rate\n8. **Docstring** explaining all options"}
{"instruction": "Create a simple database class", "output": "Here's a properly structured database wrapper:\n\n```python\nfrom typing import Optional, Any, TypeVar, Type, Generic\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nfrom abc import ABC, abstractmethod\nimport asyncpg\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\n@dataclass\nclass DatabaseConfig:\n    \"\"\"Database connection configuration.\"\"\"\n    host: str\n    port: int\n    database: str\n    user: str\n    password: str\n    min_connections: int = 5\n    max_connections: int = 20\n    \n    @property\n    def dsn(self) -> str:\n        return f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}'\n\nclass DatabaseError(Exception):\n    \"\"\"Base database error.\"\"\"\n    pass\n\nclass ConnectionError(DatabaseError):\n    \"\"\"Failed to connect to database.\"\"\"\n    pass\n\nclass QueryError(DatabaseError):\n    \"\"\"Query execution failed.\"\"\"\n    pass\n\nclass Database:\n    \"\"\"\n    Async PostgreSQL database wrapper with connection pooling.\n    \n    Usage:\n        db = Database(config)\n        await db.connect()\n        \n        users = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\n        \n        async with db.transaction():\n            await db.execute('UPDATE users SET active = $1 WHERE id = $2', False, 123)\n        \n        await db.disconnect()\n    \"\"\"\n    \n    def __init__(self, config: DatabaseConfig):\n        self._config = config\n        self._pool: Optional[asyncpg.Pool] = None\n    \n    @property\n    def is_connected(self) -> bool:\n        return self._pool is not None\n    \n    async def connect(self) -> None:\n        \"\"\"Initialize connection pool.\"\"\"\n        if self._pool is not None:\n            logger.warning('Database already connected')\n            return\n        \n        try:\n            self._pool = await asyncpg.create_pool(\n                dsn=self._config.dsn,\n                min_size=self._config.min_connections,\n                max_size=self._config.max_connections\n            )\n            logger.info(f'Connected to {self._config.host}:{self._config.port}/{self._config.database}')\n        \n        except asyncpg.InvalidCatalogNameError:\n            raise ConnectionError(f'Database {self._config.database} does not exist')\n        \n        except asyncpg.InvalidPasswordError:\n            raise ConnectionError('Invalid database credentials')\n        \n        except OSError as e:\n            raise ConnectionError(f'Cannot reach database server: {e}')\n    \n    async def disconnect(self) -> None:\n        \"\"\"Close all connections.\"\"\"\n        if self._pool is None:\n            return\n        \n        await self._pool.close()\n        self._pool = None\n        logger.info('Database disconnected')\n    \n    def _ensure_connected(self) -> asyncpg.Pool:\n        if self._pool is None:\n            raise DatabaseError('Database not connected. Call connect() first.')\n        return self._pool\n    \n    async def execute(self, query: str, *args: Any) -> str:\n        \"\"\"\n        Execute a query that doesn't return rows.\n        \n        Returns:\n            Status string (e.g., 'UPDATE 5')\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.execute(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_one(self, query: str, *args: Any) -> Optional[dict]:\n        \"\"\"Fetch a single row as dict, or None if not found.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            row = await pool.fetchrow(query, *args)\n            return dict(row) if row else None\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_all(self, query: str, *args: Any) -> list[dict]:\n        \"\"\"Fetch all rows as list of dicts.\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            rows = await pool.fetch(query, *args)\n            return [dict(row) for row in rows]\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    async def fetch_value(self, query: str, *args: Any) -> Any:\n        \"\"\"Fetch a single value (first column of first row).\"\"\"\n        pool = self._ensure_connected()\n        \n        try:\n            return await pool.fetchval(query, *args)\n        except asyncpg.PostgresError as e:\n            logger.error(f'Query failed: {query[:100]}... Error: {e}')\n            raise QueryError(f'Query failed: {e}')\n    \n    @asynccontextmanager\n    async def transaction(self):\n        \"\"\"\n        Transaction context manager with automatic rollback on error.\n        \n        Usage:\n            async with db.transaction():\n                await db.execute(...)\n                await db.execute(...)\n                # Commits if no error, rolls back otherwise\n        \"\"\"\n        pool = self._ensure_connected()\n        \n        async with pool.acquire() as conn:\n            async with conn.transaction():\n                # Temporarily replace pool operations to use this connection\n                original_pool = self._pool\n                \n                class SingleConnectionPool:\n                    async def execute(self, *a): return await conn.execute(*a)\n                    async def fetch(self, *a): return await conn.fetch(*a)\n                    async def fetchrow(self, *a): return await conn.fetchrow(*a)\n                    async def fetchval(self, *a): return await conn.fetchval(*a)\n                \n                self._pool = SingleConnectionPool()\n                try:\n                    yield\n                finally:\n                    self._pool = original_pool\n    \n    async def table_exists(self, table_name: str) -> bool:\n        \"\"\"Check if a table exists.\"\"\"\n        result = await self.fetch_value(\n            \"SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_name = $1)\",\n            table_name\n        )\n        return result\n\n# Usage:\nconfig = DatabaseConfig(\n    host='localhost',\n    port=5432,\n    database='myapp',\n    user='postgres',\n    password='secret'\n)\n\ndb = Database(config)\nawait db.connect()\n\n# Simple queries\nusers = await db.fetch_all('SELECT * FROM users WHERE active = $1', True)\ncount = await db.fetch_value('SELECT COUNT(*) FROM users')\n\n# Transactions\nasync with db.transaction():\n    await db.execute('INSERT INTO audit_log (action) VALUES ($1)', 'user_created')\n    await db.execute('INSERT INTO users (name) VALUES ($1)', 'Simon')\n\nawait db.disconnect()\n```\n\nQuality aspects:\n1. **Connection pooling** for performance\n2. **Custom exceptions** hierarchy\n3. **Context manager** for transactions\n4. **Multiple fetch methods** for different use cases\n5. **Connection state checking**\n6. **Detailed logging**\n7. **DSN property** for clean URL building\n8. **Type hints** throughout"}
{"instruction": "Write a function to validate user input", "output": "Here's a comprehensive validation system:\n\n```python\nfrom typing import Any, Callable, Optional, Union, TypeVar\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport re\nfrom enum import Enum\n\nT = TypeVar('T')\n\nclass ValidationSeverity(Enum):\n    ERROR = 'error'      # Blocks submission\n    WARNING = 'warning'  # Allows submission with notice\n\n@dataclass\nclass ValidationError:\n    \"\"\"Single validation error.\"\"\"\n    field: str\n    message: str\n    code: str\n    severity: ValidationSeverity = ValidationSeverity.ERROR\n    value: Any = None\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Complete validation result.\"\"\"\n    is_valid: bool\n    errors: list[ValidationError] = field(default_factory=list)\n    warnings: list[ValidationError] = field(default_factory=list)\n    \n    def add_error(self, field: str, message: str, code: str, value: Any = None):\n        self.errors.append(ValidationError(field, message, code, ValidationSeverity.ERROR, value))\n        self.is_valid = False\n    \n    def add_warning(self, field: str, message: str, code: str, value: Any = None):\n        self.warnings.append(ValidationError(field, message, code, ValidationSeverity.WARNING, value))\n    \n    def merge(self, other: 'ValidationResult') -> 'ValidationResult':\n        \"\"\"Combine two validation results.\"\"\"\n        return ValidationResult(\n            is_valid=self.is_valid and other.is_valid,\n            errors=self.errors + other.errors,\n            warnings=self.warnings + other.warnings\n        )\n\nclass Validator(ABC):\n    \"\"\"Base validator class.\"\"\"\n    \n    @abstractmethod\n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        pass\n\nclass Required(Validator):\n    \"\"\"Validates that value is present and not empty.\"\"\"\n    \n    def __init__(self, message: str = 'This field is required'):\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None or value == '' or (isinstance(value, (list, dict)) and len(value) == 0):\n            result.add_error(field_name, self.message, 'required')\n        \n        return result\n\nclass MinLength(Validator):\n    \"\"\"Validates minimum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be at least {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) < self.length:\n            result.add_error(field_name, self.message, 'min_length', value)\n        \n        return result\n\nclass MaxLength(Validator):\n    \"\"\"Validates maximum string length.\"\"\"\n    \n    def __init__(self, length: int, message: Optional[str] = None):\n        self.length = length\n        self.message = message or f'Must be no more than {length} characters'\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and len(str(value)) > self.length:\n            result.add_error(field_name, self.message, 'max_length', value)\n        \n        return result\n\nclass Pattern(Validator):\n    \"\"\"Validates against regex pattern.\"\"\"\n    \n    def __init__(self, pattern: str, message: str = 'Invalid format'):\n        self.pattern = re.compile(pattern)\n        self.message = message\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.pattern.match(str(value)):\n            result.add_error(field_name, self.message, 'pattern', value)\n        \n        return result\n\nclass Email(Pattern):\n    \"\"\"Validates email format.\"\"\"\n    \n    def __init__(self, message: str = 'Invalid email address'):\n        super().__init__(\n            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n            message\n        )\n\nclass Range(Validator):\n    \"\"\"Validates numeric range.\"\"\"\n    \n    def __init__(self, min_val: Optional[float] = None, max_val: Optional[float] = None):\n        self.min_val = min_val\n        self.max_val = max_val\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is None:\n            return result\n        \n        try:\n            num = float(value)\n            \n            if self.min_val is not None and num < self.min_val:\n                result.add_error(field_name, f'Must be at least {self.min_val}', 'min_value', value)\n            \n            if self.max_val is not None and num > self.max_val:\n                result.add_error(field_name, f'Must be no more than {self.max_val}', 'max_value', value)\n        \n        except (TypeError, ValueError):\n            result.add_error(field_name, 'Must be a number', 'not_numeric', value)\n        \n        return result\n\nclass Custom(Validator):\n    \"\"\"Custom validation function.\"\"\"\n    \n    def __init__(self, func: Callable[[Any], bool], message: str, code: str = 'custom'):\n        self.func = func\n        self.message = message\n        self.code = code\n    \n    def validate(self, value: Any, field_name: str) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        if value is not None and not self.func(value):\n            result.add_error(field_name, self.message, self.code, value)\n        \n        return result\n\n@dataclass\nclass Field:\n    \"\"\"Field definition with validators.\"\"\"\n    name: str\n    validators: list[Validator] = field(default_factory=list)\n    \n    def validate(self, value: Any) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for validator in self.validators:\n            result = result.merge(validator.validate(value, self.name))\n        \n        return result\n\nclass Schema:\n    \"\"\"Validation schema for objects.\"\"\"\n    \n    def __init__(self, fields: list[Field]):\n        self._fields = {f.name: f for f in fields}\n    \n    def validate(self, data: dict) -> ValidationResult:\n        result = ValidationResult(is_valid=True)\n        \n        for field_name, field in self._fields.items():\n            value = data.get(field_name)\n            result = result.merge(field.validate(value))\n        \n        return result\n\n# Usage example:\nuser_schema = Schema([\n    Field('email', [Required(), Email()]),\n    Field('password', [Required(), MinLength(8), MaxLength(100)]),\n    Field('age', [Range(min_val=18, max_val=120)]),\n    Field('username', [\n        Required(),\n        MinLength(3),\n        MaxLength(20),\n        Pattern(r'^[a-zA-Z0-9_]+$', 'Only letters, numbers, and underscores allowed'),\n        Custom(\n            lambda x: x.lower() not in ['admin', 'root', 'system'],\n            'This username is reserved',\n            'reserved_username'\n        )\n    ])\n])\n\n# Validate\ndata = {\n    'email': 'invalid-email',\n    'password': '123',\n    'age': 15,\n    'username': 'admin'\n}\n\nresult = user_schema.validate(data)\n\nif not result.is_valid:\n    for error in result.errors:\n        print(f'{error.field}: {error.message} (code: {error.code})')\n\n# Output:\n# email: Invalid email address (code: pattern)\n# password: Must be at least 8 characters (code: min_length)\n# age: Must be at least 18 (code: min_value)\n# username: This username is reserved (code: reserved_username)\n```\n\nQuality aspects:\n1. **Composable validators** - mix and match\n2. **Custom validators** - easy extension\n3. **Error codes** for programmatic handling\n4. **Severity levels** - errors vs warnings\n5. **Schema-based** validation for objects\n6. **Detailed error info** including invalid value\n7. **Immutable results** with merge support\n8. **Pre-built common validators** (email, range, pattern)"}
{"instruction": "Write a logging setup for an application", "output": "Here's a production-ready logging configuration:\n\n```python\nimport logging\nimport logging.handlers\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport traceback\n\nclass LogLevel(Enum):\n    DEBUG = 'DEBUG'\n    INFO = 'INFO'\n    WARNING = 'WARNING'\n    ERROR = 'ERROR'\n    CRITICAL = 'CRITICAL'\n\n@dataclass\nclass LogConfig:\n    \"\"\"Logging configuration.\"\"\"\n    app_name: str\n    level: LogLevel = LogLevel.INFO\n    log_dir: Optional[Path] = None\n    max_file_size_mb: int = 10\n    backup_count: int = 5\n    json_format: bool = False\n    include_hostname: bool = False\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"\n    JSON log formatter for structured logging.\n    Ideal for log aggregation systems (ELK, Datadog, etc.)\n    \"\"\"\n    \n    def __init__(self, app_name: str, include_hostname: bool = False):\n        super().__init__()\n        self.app_name = app_name\n        self.include_hostname = include_hostname\n        \n        if include_hostname:\n            import socket\n            self.hostname = socket.gethostname()\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_data = {\n            'timestamp': datetime.utcnow().isoformat() + 'Z',\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'app': self.app_name,\n        }\n        \n        # Add location info\n        if record.levelno >= logging.WARNING:\n            log_data['location'] = {\n                'file': record.filename,\n                'line': record.lineno,\n                'function': record.funcName\n            }\n        \n        # Add hostname if configured\n        if self.include_hostname:\n            log_data['hostname'] = self.hostname\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_data['exception'] = {\n                'type': record.exc_info[0].__name__ if record.exc_info[0] else None,\n                'message': str(record.exc_info[1]) if record.exc_info[1] else None,\n                'traceback': traceback.format_exception(*record.exc_info)\n            }\n        \n        # Add extra fields\n        for key, value in record.__dict__.items():\n            if key not in [\n                'name', 'msg', 'args', 'created', 'filename', 'funcName',\n                'levelname', 'levelno', 'lineno', 'module', 'msecs',\n                'pathname', 'process', 'processName', 'relativeCreated',\n                'stack_info', 'exc_info', 'exc_text', 'thread', 'threadName',\n                'message', 'taskName'\n            ]:\n                log_data[key] = value\n        \n        return json.dumps(log_data, default=str)\n\nclass ColoredFormatter(logging.Formatter):\n    \"\"\"Colored console output for development.\"\"\"\n    \n    COLORS = {\n        'DEBUG': '\\033[36m',     # Cyan\n        'INFO': '\\033[32m',      # Green\n        'WARNING': '\\033[33m',   # Yellow\n        'ERROR': '\\033[31m',     # Red\n        'CRITICAL': '\\033[35m',  # Magenta\n    }\n    RESET = '\\033[0m'\n    \n    def format(self, record: logging.LogRecord) -> str:\n        color = self.COLORS.get(record.levelname, '')\n        \n        # Format: [TIME] LEVEL logger: message\n        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')\n        \n        formatted = f'{color}[{timestamp}] {record.levelname:8}{self.RESET} '\n        formatted += f'{record.name}: {record.getMessage()}'\n        \n        if record.exc_info:\n            formatted += '\\n' + self.formatException(record.exc_info)\n        \n        return formatted\n\ndef setup_logging(config: LogConfig) -> logging.Logger:\n    \"\"\"\n    Configure application logging.\n    \n    Sets up:\n    - Console handler (colored in dev, JSON optional)\n    - File handler with rotation (if log_dir specified)\n    - Separate error log file\n    \n    Returns:\n        Root logger for the application\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(config.app_name)\n    logger.setLevel(getattr(logging, config.level.value))\n    \n    # Remove existing handlers\n    logger.handlers.clear()\n    \n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n    \n    if config.json_format:\n        console_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n    else:\n        console_handler.setFormatter(ColoredFormatter())\n    \n    logger.addHandler(console_handler)\n    \n    # File handlers (if log directory specified)\n    if config.log_dir:\n        config.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Main log file (rotating)\n        main_log = config.log_dir / f'{config.app_name}.log'\n        file_handler = logging.handlers.RotatingFileHandler(\n            main_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(file_handler)\n        \n        # Error log file (errors and above only)\n        error_log = config.log_dir / f'{config.app_name}.error.log'\n        error_handler = logging.handlers.RotatingFileHandler(\n            error_log,\n            maxBytes=config.max_file_size_mb * 1024 * 1024,\n            backupCount=config.backup_count\n        )\n        error_handler.setLevel(logging.ERROR)\n        error_handler.setFormatter(JsonFormatter(config.app_name, config.include_hostname))\n        logger.addHandler(error_handler)\n    \n    # Prevent propagation to root logger\n    logger.propagate = False\n    \n    logger.info(f'Logging initialized', extra={'config': config.__dict__})\n    \n    return logger\n\nclass LoggerMixin:\n    \"\"\"Mixin to add logger to any class.\"\"\"\n    \n    @property\n    def logger(self) -> logging.Logger:\n        if not hasattr(self, '_logger'):\n            self._logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')\n        return self._logger\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a child logger.\"\"\"\n    return logging.getLogger(name)\n\n# Context manager for adding extra fields\nclass LogContext:\n    \"\"\"Add extra fields to all logs within context.\"\"\"\n    \n    def __init__(self, logger: logging.Logger, **extra: Any):\n        self.logger = logger\n        self.extra = extra\n        self.old_factory = None\n    \n    def __enter__(self):\n        self.old_factory = logging.getLogRecordFactory()\n        extra = self.extra\n        \n        def factory(*args, **kwargs):\n            record = self.old_factory(*args, **kwargs)\n            for key, value in extra.items():\n                setattr(record, key, value)\n            return record\n        \n        logging.setLogRecordFactory(factory)\n        return self\n    \n    def __exit__(self, *args):\n        logging.setLogRecordFactory(self.old_factory)\n\n# Usage:\nconfig = LogConfig(\n    app_name='myapp',\n    level=LogLevel.DEBUG,\n    log_dir=Path('./logs'),\n    json_format=False  # Set True for production\n)\n\nlogger = setup_logging(config)\n\n# Basic logging\nlogger.info('Application started')\nlogger.warning('Low memory', extra={'available_mb': 256})\n\n# With context\nwith LogContext(logger, request_id='abc-123', user_id=42):\n    logger.info('Processing request')  # Includes request_id and user_id\n\n# In classes\nclass UserService(LoggerMixin):\n    def create_user(self, name: str):\n        self.logger.info(f'Creating user: {name}')\n\ntry:\n    raise ValueError('Something went wrong')\nexcept Exception:\n    logger.exception('Failed to process')  # Includes full traceback\n```\n\nQuality aspects:\n1. **Structured JSON logging** for production\n2. **Colored output** for development\n3. **Log rotation** to prevent disk fill\n4. **Separate error log** for easy monitoring\n5. **Context manager** for request tracing\n6. **LoggerMixin** for easy class integration\n7. **Extra fields** support\n8. **Configurable** via dataclass"}
{"instruction": "Create a health check endpoint", "output": "Here's a comprehensive health check system:\n\n```python\nfrom typing import Optional, Callable, Any\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nimport asyncio\nimport time\nimport logging\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\n\nclass HealthStatus(str, Enum):\n    HEALTHY = 'healthy'\n    DEGRADED = 'degraded'  # Working but with issues\n    UNHEALTHY = 'unhealthy'\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"Health status of a single component.\"\"\"\n    name: str\n    status: HealthStatus\n    message: Optional[str] = None\n    response_time_ms: Optional[float] = None\n    last_check: Optional[datetime] = None\n    details: dict = field(default_factory=dict)\n\nclass HealthCheckResponse(BaseModel):\n    \"\"\"API response model.\"\"\"\n    status: HealthStatus\n    timestamp: str\n    version: str\n    uptime_seconds: float\n    components: dict[str, dict]\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Individual health check definition.\"\"\"\n    name: str\n    check_func: Callable[[], Any]  # Returns True/False or raises\n    timeout_seconds: float = 5.0\n    critical: bool = True  # If False, failure only causes DEGRADED\n    \n    async def run(self) -> ComponentHealth:\n        \"\"\"Execute health check with timeout.\"\"\"\n        start = time.time()\n        \n        try:\n            if asyncio.iscoroutinefunction(self.check_func):\n                result = await asyncio.wait_for(\n                    self.check_func(),\n                    timeout=self.timeout_seconds\n                )\n            else:\n                result = await asyncio.wait_for(\n                    asyncio.to_thread(self.check_func),\n                    timeout=self.timeout_seconds\n                )\n            \n            elapsed_ms = (time.time() - start) * 1000\n            \n            if result is True or result is None:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n            elif isinstance(result, dict):\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.HEALTHY,\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow(),\n                    details=result\n                )\n            else:\n                return ComponentHealth(\n                    name=self.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    response_time_ms=elapsed_ms,\n                    last_check=datetime.utcnow()\n                )\n        \n        except asyncio.TimeoutError:\n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=f'Timeout after {self.timeout_seconds}s',\n                response_time_ms=self.timeout_seconds * 1000,\n                last_check=datetime.utcnow()\n            )\n        \n        except Exception as e:\n            elapsed_ms = (time.time() - start) * 1000\n            logger.warning(f'Health check {self.name} failed: {e}')\n            \n            return ComponentHealth(\n                name=self.name,\n                status=HealthStatus.UNHEALTHY,\n                message=str(e),\n                response_time_ms=elapsed_ms,\n                last_check=datetime.utcnow()\n            )\n\nclass HealthChecker:\n    \"\"\"\n    Manages health checks for an application.\n    \n    Usage:\n        health = HealthChecker(app_version='1.0.0')\n        \n        health.add_check('database', check_database, critical=True)\n        health.add_check('cache', check_redis, critical=False)\n        \n        result = await health.check_all()\n    \"\"\"\n    \n    def __init__(self, app_version: str = '1.0.0'):\n        self._checks: list[HealthCheck] = []\n        self._start_time = datetime.utcnow()\n        self._version = app_version\n        self._last_result: Optional[HealthCheckResponse] = None\n        self._cache_ttl = timedelta(seconds=5)  # Don't check more than every 5s\n        self._last_check_time: Optional[datetime] = None\n    \n    def add_check(\n        self,\n        name: str,\n        check_func: Callable,\n        timeout: float = 5.0,\n        critical: bool = True\n    ) -> 'HealthChecker':\n        \"\"\"Add a health check. Returns self for chaining.\"\"\"\n        self._checks.append(HealthCheck(\n            name=name,\n            check_func=check_func,\n            timeout_seconds=timeout,\n            critical=critical\n        ))\n        return self\n    \n    async def check_all(self, use_cache: bool = True) -> HealthCheckResponse:\n        \"\"\"\n        Run all health checks.\n        \n        Args:\n            use_cache: Return cached result if within TTL\n        \"\"\"\n        # Return cached result if fresh\n        if use_cache and self._last_result and self._last_check_time:\n            if datetime.utcnow() - self._last_check_time < self._cache_ttl:\n                return self._last_result\n        \n        # Run all checks in parallel\n        results = await asyncio.gather(\n            *[check.run() for check in self._checks],\n            return_exceptions=True\n        )\n        \n        # Process results\n        components = {}\n        overall_status = HealthStatus.HEALTHY\n        \n        for check, result in zip(self._checks, results):\n            if isinstance(result, Exception):\n                result = ComponentHealth(\n                    name=check.name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=str(result),\n                    last_check=datetime.utcnow()\n                )\n            \n            components[result.name] = {\n                'status': result.status.value,\n                'message': result.message,\n                'response_time_ms': result.response_time_ms,\n                'details': result.details\n            }\n            \n            # Update overall status\n            if result.status == HealthStatus.UNHEALTHY:\n                if check.critical:\n                    overall_status = HealthStatus.UNHEALTHY\n                elif overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n            elif result.status == HealthStatus.DEGRADED:\n                if overall_status == HealthStatus.HEALTHY:\n                    overall_status = HealthStatus.DEGRADED\n        \n        uptime = (datetime.utcnow() - self._start_time).total_seconds()\n        \n        response = HealthCheckResponse(\n            status=overall_status,\n            timestamp=datetime.utcnow().isoformat() + 'Z',\n            version=self._version,\n            uptime_seconds=uptime,\n            components=components\n        )\n        \n        self._last_result = response\n        self._last_check_time = datetime.utcnow()\n        \n        return response\n    \n    async def is_healthy(self) -> bool:\n        \"\"\"Quick check if system is healthy.\"\"\"\n        result = await self.check_all()\n        return result.status == HealthStatus.HEALTHY\n\n# FastAPI integration\ndef setup_health_routes(app: FastAPI, health_checker: HealthChecker):\n    \"\"\"Add health check routes to FastAPI app.\"\"\"\n    \n    @app.get('/health', response_model=HealthCheckResponse)\n    async def health_check(response: Response):\n        \"\"\"Full health check with component details.\"\"\"\n        result = await health_checker.check_all()\n        \n        if result.status == HealthStatus.UNHEALTHY:\n            response.status_code = 503\n        elif result.status == HealthStatus.DEGRADED:\n            response.status_code = 200  # Still serving requests\n        \n        return result\n    \n    @app.get('/health/live')\n    async def liveness():\n        \"\"\"Kubernetes liveness probe - is the app running?\"\"\"\n        return {'status': 'alive'}\n    \n    @app.get('/health/ready')\n    async def readiness(response: Response):\n        \"\"\"Kubernetes readiness probe - can it serve traffic?\"\"\"\n        is_ready = await health_checker.is_healthy()\n        \n        if not is_ready:\n            response.status_code = 503\n            return {'status': 'not_ready'}\n        \n        return {'status': 'ready'}\n\n# Example check functions:\nasync def check_database(db) -> bool:\n    await db.execute('SELECT 1')\n    return True\n\nasync def check_redis(redis) -> dict:\n    info = await redis.info()\n    return {\n        'connected_clients': info['connected_clients'],\n        'used_memory_mb': info['used_memory'] / 1024 / 1024\n    }\n\ndef check_disk_space() -> bool:\n    import shutil\n    usage = shutil.disk_usage('/')\n    free_percent = usage.free / usage.total * 100\n    \n    if free_percent < 10:\n        raise Exception(f'Low disk space: {free_percent:.1f}% free')\n    \n    return True\n\n# Usage:\napp = FastAPI()\n\nhealth = HealthChecker(app_version='2.1.0')\nhealth.add_check('database', lambda: check_database(db), critical=True)\nhealth.add_check('redis', lambda: check_redis(redis), critical=False)\nhealth.add_check('disk', check_disk_space, timeout=2.0, critical=True)\n\nsetup_health_routes(app, health)\n```\n\nQuality aspects:\n1. **Three status levels** - healthy, degraded, unhealthy\n2. **Critical vs non-critical** - degraded mode support\n3. **Kubernetes-ready** - liveness and readiness probes\n4. **Response caching** - don't overload checks\n5. **Parallel execution** - fast even with many checks\n6. **Detailed component info** - timing, messages, details\n7. **Timeout handling** - prevent hanging checks\n8. **Proper HTTP status codes** - 503 when unhealthy"}
