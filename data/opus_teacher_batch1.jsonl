{"instruction": "Skapa en FastAPI endpoint som streamar AI-svar via Server-Sent Events", "output": "```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\napp = FastAPI()\n\nasync def generate_stream(prompt: str):\n    \"\"\"Generator som streamar AI-svar chunk f√∂r chunk.\"\"\"\n    async for chunk in ollama_client.chat_stream(prompt):\n        yield f\"data: {chunk}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n@app.get(\"/api/stream\")\nasync def stream_response(prompt: str):\n    return StreamingResponse(\n        generate_stream(prompt),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n```\n\nDenna endpoint anv√§nder Server-Sent Events f√∂r att streama AI-svar i realtid. Klienten kan l√§sa str√∂mmen med EventSource API."}
{"instruction": "Implementera en React hook f√∂r WebSocket-anslutning med automatisk reconnect", "output": "```typescript\nimport { useState, useEffect, useCallback, useRef } from 'react';\n\ninterface UseWebSocketOptions {\n  url: string;\n  onMessage: (data: any) => void;\n  reconnectInterval?: number;\n}\n\nexport function useWebSocket({ url, onMessage, reconnectInterval = 3000 }: UseWebSocketOptions) {\n  const [isConnected, setIsConnected] = useState(false);\n  const wsRef = useRef<WebSocket | null>(null);\n  const reconnectTimeoutRef = useRef<NodeJS.Timeout>();\n\n  const connect = useCallback(() => {\n    const ws = new WebSocket(url);\n    \n    ws.onopen = () => {\n      setIsConnected(true);\n      console.log('WebSocket connected');\n    };\n    \n    ws.onmessage = (event) => {\n      const data = JSON.parse(event.data);\n      onMessage(data);\n    };\n    \n    ws.onclose = () => {\n      setIsConnected(false);\n      reconnectTimeoutRef.current = setTimeout(connect, reconnectInterval);\n    };\n    \n    wsRef.current = ws;\n  }, [url, onMessage, reconnectInterval]);\n\n  useEffect(() => {\n    connect();\n    return () => {\n      clearTimeout(reconnectTimeoutRef.current);\n      wsRef.current?.close();\n    };\n  }, [connect]);\n\n  const send = useCallback((data: any) => {\n    wsRef.current?.send(JSON.stringify(data));\n  }, []);\n\n  return { isConnected, send };\n}\n```"}
{"instruction": "Skapa en Python async context manager f√∂r Ollama model loading med timeout", "output": "```python\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import Optional\nimport httpx\n\nclass OllamaModelLoader:\n    def __init__(self, base_url: str = \"http://localhost:11434\"):\n        self.base_url = base_url\n        self.client = httpx.AsyncClient(timeout=60.0)\n    \n    @asynccontextmanager\n    async def load_model(self, model_name: str, timeout: float = 120.0):\n        \"\"\"Load model with timeout, unload on exit.\"\"\"\n        try:\n            # Ladda modellen\n            async with asyncio.timeout(timeout):\n                await self.client.post(\n                    f\"{self.base_url}/api/generate\",\n                    json={\"model\": model_name, \"prompt\": \"\", \"keep_alive\": \"5m\"}\n                )\n            yield model_name\n        finally:\n            # Unload model f√∂r att frig√∂ra VRAM\n            await self.client.post(\n                f\"{self.base_url}/api/generate\",\n                json={\"model\": model_name, \"keep_alive\": \"0\"}\n            )\n\n# Anv√§ndning:\nasync with loader.load_model(\"qwen2.5-coder:14b\") as model:\n    response = await ollama.chat(model=model, messages=[...])\n```"}
{"instruction": "Implementera en Zustand store f√∂r chat-historik med persistens", "output": "```typescript\nimport { create } from 'zustand';\nimport { persist } from 'zustand/middleware';\n\ninterface Message {\n  id: string;\n  role: 'user' | 'assistant';\n  content: string;\n  timestamp: number;\n}\n\ninterface ChatStore {\n  messages: Message[];\n  isLoading: boolean;\n  addMessage: (message: Omit<Message, 'id' | 'timestamp'>) => void;\n  updateMessage: (id: string, content: string) => void;\n  clearHistory: () => void;\n  setLoading: (loading: boolean) => void;\n}\n\nexport const useChatStore = create<ChatStore>()(\n  persist(\n    (set) => ({\n      messages: [],\n      isLoading: false,\n      \n      addMessage: (message) => set((state) => ({\n        messages: [...state.messages, {\n          ...message,\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n        }]\n      })),\n      \n      updateMessage: (id, content) => set((state) => ({\n        messages: state.messages.map(m => \n          m.id === id ? { ...m, content } : m\n        )\n      })),\n      \n      clearHistory: () => set({ messages: [] }),\n      setLoading: (loading) => set({ isLoading: loading }),\n    }),\n    { name: 'chat-history' }\n  )\n);\n```"}
{"instruction": "Skapa en Python decorator f√∂r rate limiting med sliding window", "output": "```python\nimport asyncio\nimport time\nfrom functools import wraps\nfrom collections import deque\n\ndef rate_limit(calls: int, period: float):\n    \"\"\"Sliding window rate limiter decorator.\"\"\"\n    def decorator(func):\n        call_times = deque()\n        lock = asyncio.Lock()\n        \n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            async with lock:\n                now = time.monotonic()\n                \n                # Ta bort gamla anrop utanf√∂r f√∂nstret\n                while call_times and call_times[0] < now - period:\n                    call_times.popleft()\n                \n                # Kolla om vi √∂verskrider gr√§nsen\n                if len(call_times) >= calls:\n                    wait_time = call_times[0] + period - now\n                    await asyncio.sleep(wait_time)\n                    call_times.popleft()\n                \n                call_times.append(time.monotonic())\n            \n            return await func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# Anv√§ndning:\n@rate_limit(calls=10, period=60.0)  # Max 10 anrop per minut\nasync def call_ollama(prompt: str):\n    return await ollama.chat(prompt=prompt)\n```"}
{"instruction": "Implementera error boundary komponent i React med fallback UI", "output": "```typescript\nimport React, { Component, ErrorInfo, ReactNode } from 'react';\n\ninterface Props {\n  children: ReactNode;\n  fallback?: ReactNode;\n}\n\ninterface State {\n  hasError: boolean;\n  error: Error | null;\n}\n\nexport class ErrorBoundary extends Component<Props, State> {\n  constructor(props: Props) {\n    super(props);\n    this.state = { hasError: false, error: null };\n  }\n\n  static getDerivedStateFromError(error: Error): State {\n    return { hasError: true, error };\n  }\n\n  componentDidCatch(error: Error, errorInfo: ErrorInfo) {\n    console.error('ErrorBoundary caught:', error, errorInfo);\n  }\n\n  render() {\n    if (this.state.hasError) {\n      return this.props.fallback || (\n        <div className=\"p-4 bg-red-500/20 rounded-lg border border-red-500\">\n          <h2 className=\"text-red-400 font-bold\">N√•got gick fel</h2>\n          <p className=\"text-red-300 text-sm mt-2\">\n            {this.state.error?.message}\n          </p>\n          <button\n            onClick={() => this.setState({ hasError: false, error: null })}\n            className=\"mt-4 px-4 py-2 bg-red-500 rounded hover:bg-red-600\"\n          >\n            F√∂rs√∂k igen\n          </button>\n        </div>\n      );\n    }\n\n    return this.props.children;\n  }\n}\n```"}
{"instruction": "Skapa en Python service f√∂r GPU monitoring med NVIDIA SMI", "output": "```python\nimport subprocess\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass GPUStats:\n    name: str\n    memory_used: int  # MiB\n    memory_total: int  # MiB\n    utilization: int  # %\n    temperature: int  # Celsius\n    \n    @property\n    def memory_percent(self) -> float:\n        return (self.memory_used / self.memory_total) * 100\n    \n    @property\n    def is_memory_critical(self) -> bool:\n        return self.memory_percent > 90\n\nclass GPUMonitor:\n    async def get_stats(self) -> Optional[GPUStats]:\n        \"\"\"H√§mta GPU-statistik via nvidia-smi.\"\"\"\n        try:\n            result = await asyncio.create_subprocess_exec(\n                \"nvidia-smi\",\n                \"--query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu\",\n                \"--format=csv,noheader,nounits\",\n                stdout=asyncio.subprocess.PIPE\n            )\n            stdout, _ = await result.communicate()\n            \n            parts = stdout.decode().strip().split(\", \")\n            return GPUStats(\n                name=parts[0],\n                memory_used=int(parts[1]),\n                memory_total=int(parts[2]),\n                utilization=int(parts[3]),\n                temperature=int(parts[4])\n            )\n        except Exception as e:\n            print(f\"GPU monitoring error: {e}\")\n            return None\n\n# Anv√§ndning:\nmonitor = GPUMonitor()\nstats = await monitor.get_stats()\nif stats and stats.is_memory_critical:\n    print(f\"VARNING: GPU minne p√• {stats.memory_percent:.1f}%\")\n```"}
{"instruction": "Implementera en multi-agent cascade pipeline med hot-swap model loading", "output": "```python\nimport asyncio\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import AsyncIterator\n\nclass CascadePhase(Enum):\n    PLANNING = \"planning\"\n    CODING = \"coding\"\n    REVIEWING = \"reviewing\"\n    COMPLETE = \"complete\"\n\n@dataclass\nclass CascadeResult:\n    phase: CascadePhase\n    content: str\n    model: str\n\nclass CascadeOrchestrator:\n    MODELS = {\n        CascadePhase.PLANNING: \"devstral-openhands:latest\",\n        CascadePhase.CODING: \"qwen-sonnet:latest\",\n        CascadePhase.REVIEWING: \"qwen-openhands:latest\",\n    }\n    \n    async def execute_cascade(self, task: str) -> AsyncIterator[CascadeResult]:\n        \"\"\"K√∂r multi-agent cascade med hot-swap.\"\"\"\n        context = {\"task\": task}\n        \n        for phase in [CascadePhase.PLANNING, CascadePhase.CODING, CascadePhase.REVIEWING]:\n            model = self.MODELS[phase]\n            \n            # Hot-swap: unload previous, load new\n            await self._switch_model(model)\n            \n            # Execute phase\n            prompt = self._build_prompt(phase, context)\n            async for chunk in self.ollama.chat_stream(model, prompt):\n                yield CascadeResult(phase=phase, content=chunk, model=model)\n                context[phase.value] = context.get(phase.value, \"\") + chunk\n        \n        yield CascadeResult(phase=CascadePhase.COMPLETE, content=\"\", model=\"\")\n    \n    async def _switch_model(self, new_model: str):\n        \"\"\"Unload current model, load new one.\"\"\"\n        await self.ollama.unload_all()\n        await self.ollama.load(new_model)\n```"}
{"instruction": "Skapa en TypeScript utility f√∂r att debounce async funktioner", "output": "```typescript\nexport function debounceAsync<T extends (...args: any[]) => Promise<any>>(\n  fn: T,\n  delay: number\n): (...args: Parameters<T>) => Promise<ReturnType<T>> {\n  let timeoutId: NodeJS.Timeout | null = null;\n  let pendingPromise: Promise<ReturnType<T>> | null = null;\n  let resolvePromise: ((value: ReturnType<T>) => void) | null = null;\n\n  return (...args: Parameters<T>): Promise<ReturnType<T>> => {\n    // Cancel previous timeout\n    if (timeoutId) {\n      clearTimeout(timeoutId);\n    }\n\n    // Create new promise if none pending\n    if (!pendingPromise) {\n      pendingPromise = new Promise((resolve) => {\n        resolvePromise = resolve;\n      });\n    }\n\n    timeoutId = setTimeout(async () => {\n      try {\n        const result = await fn(...args);\n        resolvePromise?.(result);\n      } finally {\n        pendingPromise = null;\n        resolvePromise = null;\n      }\n    }, delay);\n\n    return pendingPromise;\n  };\n}\n\n// Anv√§ndning:\nconst debouncedSearch = debounceAsync(\n  async (query: string) => {\n    const response = await fetch(`/api/search?q=${query}`);\n    return response.json();\n  },\n  300\n);\n```"}
{"instruction": "Implementera en Python background task manager med graceful shutdown", "output": "```python\nimport asyncio\nimport signal\nfrom typing import Callable, Awaitable, Set\nfrom contextlib import asynccontextmanager\n\nclass BackgroundTaskManager:\n    def __init__(self):\n        self._tasks: Set[asyncio.Task] = set()\n        self._shutdown = asyncio.Event()\n    \n    def start_task(self, coro: Awaitable, name: str = None):\n        \"\"\"Starta en bakgrundsuppgift.\"\"\"\n        task = asyncio.create_task(coro, name=name)\n        self._tasks.add(task)\n        task.add_done_callback(self._tasks.discard)\n        return task\n    \n    async def shutdown(self, timeout: float = 10.0):\n        \"\"\"Graceful shutdown av alla tasks.\"\"\"\n        self._shutdown.set()\n        \n        if not self._tasks:\n            return\n        \n        # V√§nta p√• att tasks avslutas\n        done, pending = await asyncio.wait(\n            self._tasks,\n            timeout=timeout\n        )\n        \n        # Avbryt kvarvarande tasks\n        for task in pending:\n            task.cancel()\n        \n        if pending:\n            await asyncio.gather(*pending, return_exceptions=True)\n    \n    @property\n    def should_stop(self) -> bool:\n        return self._shutdown.is_set()\n\n# FastAPI integration:\ntask_manager = BackgroundTaskManager()\n\n@asynccontextmanager\nasync def lifespan(app):\n    # Starta bakgrundsuppgifter\n    task_manager.start_task(periodic_cleanup(), name=\"cleanup\")\n    yield\n    # Graceful shutdown\n    await task_manager.shutdown()\n```"}
{"instruction": "Skapa en React komponent f√∂r infinite scroll med virtualisering", "output": "```typescript\nimport { useRef, useEffect, useState, useCallback } from 'react';\n\ninterface VirtualListProps<T> {\n  items: T[];\n  itemHeight: number;\n  containerHeight: number;\n  renderItem: (item: T, index: number) => React.ReactNode;\n  onLoadMore?: () => void;\n  hasMore?: boolean;\n}\n\nexport function VirtualList<T>({\n  items,\n  itemHeight,\n  containerHeight,\n  renderItem,\n  onLoadMore,\n  hasMore = false,\n}: VirtualListProps<T>) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const [scrollTop, setScrollTop] = useState(0);\n\n  // Ber√§kna synliga items\n  const startIndex = Math.floor(scrollTop / itemHeight);\n  const endIndex = Math.min(\n    startIndex + Math.ceil(containerHeight / itemHeight) + 1,\n    items.length\n  );\n  const visibleItems = items.slice(startIndex, endIndex);\n\n  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {\n    const { scrollTop, scrollHeight, clientHeight } = e.currentTarget;\n    setScrollTop(scrollTop);\n\n    // Ladda mer n√§r n√§ra botten\n    if (hasMore && scrollHeight - scrollTop - clientHeight < 100) {\n      onLoadMore?.();\n    }\n  }, [hasMore, onLoadMore]);\n\n  return (\n    <div\n      ref={containerRef}\n      onScroll={handleScroll}\n      style={{ height: containerHeight, overflow: 'auto' }}\n    >\n      <div style={{ height: items.length * itemHeight, position: 'relative' }}>\n        {visibleItems.map((item, i) => (\n          <div\n            key={startIndex + i}\n            style={{\n              position: 'absolute',\n              top: (startIndex + i) * itemHeight,\n              height: itemHeight,\n              width: '100%',\n            }}\n          >\n            {renderItem(item, startIndex + i)}\n          </div>\n        ))}\n      </div>\n    </div>\n  );\n}\n```"}
{"instruction": "Implementera en Python retry decorator med exponential backoff", "output": "```python\nimport asyncio\nimport random\nfrom functools import wraps\nfrom typing import Type, Tuple, Optional\n\ndef retry_with_backoff(\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n):\n    \"\"\"Retry decorator med exponential backoff och jitter.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            last_exception: Optional[Exception] = None\n            \n            for attempt in range(max_retries + 1):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_retries:\n                        raise\n                    \n                    # Ber√§kna delay med exponential backoff\n                    delay = min(\n                        base_delay * (exponential_base ** attempt),\n                        max_delay\n                    )\n                    \n                    # L√§gg till jitter f√∂r att undvika thundering herd\n                    if jitter:\n                        delay *= (0.5 + random.random())\n                    \n                    print(f\"Retry {attempt + 1}/{max_retries} efter {delay:.2f}s: {e}\")\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Anv√§ndning:\n@retry_with_backoff(max_retries=5, exceptions=(httpx.TimeoutException,))\nasync def call_api(url: str):\n    async with httpx.AsyncClient() as client:\n        return await client.get(url, timeout=10.0)\n```"}
{"instruction": "Skapa en n8n workflow f√∂r AI-driven email kategorisering och auto-svar", "output": "```json\n{\n  \"name\": \"AI Email Categorizer and Auto-Responder\",\n  \"nodes\": [\n    {\n      \"name\": \"Gmail Trigger\",\n      \"type\": \"n8n-nodes-base.gmailTrigger\",\n      \"parameters\": {\n        \"pollTimes\": { \"item\": [{ \"mode\": \"everyMinute\" }] },\n        \"filters\": { \"labelIds\": [\"INBOX\"], \"q\": \"is:unread\" }\n      }\n    },\n    {\n      \"name\": \"Categorize with Ollama\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Kategorisera email som: URGENT, SUPPORT, SALES, NEWSLETTER, SPAM. Svara med JSON: {\\\"category\\\": \\\"...\\\", \\\"needs_reply\\\": true/false, \\\"suggested_reply\\\": \\\"...\\\"}\"\n          },\n          {\n            \"role\": \"user\", \n            \"content\": \"√Ñmne: {{$json.subject}}\\n\\nInneh√•ll: {{$json.body}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Route by Category\",\n      \"type\": \"n8n-nodes-base.switch\",\n      \"parameters\": {\n        \"rules\": [\n          { \"value\": \"URGENT\", \"output\": 0 },\n          { \"value\": \"SUPPORT\", \"output\": 1 },\n          { \"value\": \"SPAM\", \"output\": 2 }\n        ]\n      }\n    },\n    {\n      \"name\": \"Send Auto-Reply\",\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"parameters\": {\n        \"operation\": \"reply\",\n        \"message\": \"{{$json.suggested_reply}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\n1. Triggar p√• nya ol√§sta email\n2. Anv√§nder Ollama f√∂r att kategorisera\n3. Routar baserat p√• kategori\n4. Skickar auto-svar f√∂r URGENT/SUPPORT"}
{"instruction": "Skapa en n8n workflow f√∂r multi-source data aggregation med konflikthantering", "output": "```json\n{\n  \"name\": \"Multi-Source Data Aggregator with Conflict Resolution\",\n  \"nodes\": [\n    {\n      \"name\": \"Schedule Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": { \"rule\": { \"interval\": [{ \"field\": \"hours\", \"hoursInterval\": 1 }] } }\n    },\n    {\n      \"name\": \"Fetch from Source A\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": { \"url\": \"https://api-a.example.com/data\" }\n    },\n    {\n      \"name\": \"Fetch from Source B\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": { \"url\": \"https://api-b.example.com/data\" }\n    },\n    {\n      \"name\": \"Merge Sources\",\n      \"type\": \"n8n-nodes-base.merge\",\n      \"parameters\": { \"mode\": \"combine\", \"mergeByFields\": { \"values\": [{ \"field1\": \"id\", \"field2\": \"id\" }] } }\n    },\n    {\n      \"name\": \"Resolve Conflicts\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const items = $input.all();\\nreturn items.map(item => {\\n  const a = item.json.source_a;\\n  const b = item.json.source_b;\\n  \\n  // Anv√§nd senaste uppdaterad\\n  const resolved = a.updated_at > b.updated_at ? a : b;\\n  \\n  // Logga konflikter\\n  if (JSON.stringify(a) !== JSON.stringify(b)) {\\n    resolved.had_conflict = true;\\n    resolved.conflict_sources = [a, b];\\n  }\\n  \\n  return { json: resolved };\\n});\"\n      }\n    },\n    {\n      \"name\": \"Store to Database\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": { \"operation\": \"upsert\", \"table\": \"aggregated_data\" }\n    }\n  ]\n}\n```\n\nWorkflow aggregerar data fr√•n flera k√§llor, identifierar konflikter baserat p√• timestamp, och sparar resultatet med konfliktloggning."}
{"instruction": "Skapa en n8n workflow f√∂r intelligent task routing baserat p√• team capacity", "output": "```json\n{\n  \"name\": \"Intelligent Task Router with Capacity Analysis\",\n  \"nodes\": [\n    {\n      \"name\": \"Webhook Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": { \"path\": \"new-task\", \"httpMethod\": \"POST\" }\n    },\n    {\n      \"name\": \"Get Team Workload\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": { \"operation\": \"getAll\", \"resource\": \"task\", \"filters\": { \"statuses\": [\"in progress\"] } }\n    },\n    {\n      \"name\": \"Analyze with AI\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en task router. Analysera teamets arbetsbelastning och v√§lj b√§sta tilldelning.\\n\\nRegler:\\n1. Max 5 aktiva tasks per person\\n2. Prioritera baserat p√• skills\\n3. Balansera workload\\n\\nSvara med JSON: {\\\"assignee\\\": \\\"namn\\\", \\\"reason\\\": \\\"motivering\\\", \\\"priority\\\": 1-5}\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Ny task: {{$node['Webhook Trigger'].json.title}}\\n\\nBeskrivning: {{$node['Webhook Trigger'].json.description}}\\n\\nTeam workload:\\n{{$node['Get Team Workload'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Create Task in ClickUp\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"resource\": \"task\",\n        \"assignees\": [\"={{$json.assignee}}\"],\n        \"priority\": \"={{$json.priority}}\"\n      }\n    },\n    {\n      \"name\": \"Notify on Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#task-assignments\",\n        \"text\": \"üéØ Ny task tilldelad {{$json.assignee}}\\n\\nMotivering: {{$json.reason}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Skapa en n8n workflow f√∂r automatisk dokumentgenerering fr√•n ClickUp tasks", "output": "```json\n{\n  \"name\": \"Auto-Generated Documentation from ClickUp\",\n  \"nodes\": [\n    {\n      \"name\": \"Daily Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": { \"rule\": { \"interval\": [{ \"field\": \"days\", \"daysInterval\": 1 }] } }\n    },\n    {\n      \"name\": \"Get Completed Tasks\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"getAll\",\n        \"resource\": \"task\",\n        \"filters\": { \"statuses\": [\"complete\"], \"date_updated_gt\": \"={{$now.minus({days: 1}).toISO()}}\" }\n      }\n    },\n    {\n      \"name\": \"Generate Documentation\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Generera teknisk dokumentation i Markdown-format baserat p√• avslutade tasks. Inkludera:\\n- Sammanfattning av √§ndringar\\n- Tekniska detaljer\\n- Breaking changes\\n- Migrationssteg om till√§mpligt\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Avslutade tasks senaste 24h:\\n{{$json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Create Google Doc\",\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"title\": \"Changelog {{$now.format('yyyy-MM-dd')}}\",\n        \"content\": \"={{$json.documentation}}\"\n      }\n    },\n    {\n      \"name\": \"Post to Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#releases\",\n        \"text\": \"üìö Daglig dokumentation genererad: {{$json.docUrl}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Hur √∂vervakar jag Ollama model memory usage i produktionsmilj√∂?", "output": "## Ollama Memory Monitoring i Produktion\n\n### 1. Grundl√§ggande monitoring script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-monitor.sh\n\nwhile true; do\n  # GPU memory\n  GPU_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits)\n  GPU_TOTAL=12288  # RTX 4070\n  GPU_PCT=$((GPU_MEM * 100 / GPU_TOTAL))\n  \n  # Ollama models\n  MODELS=$(ollama ps --format json | jq -r '.[] | \"\\(.name): \\(.size)\"')\n  \n  # Log\n  echo \"[$(date)] GPU: ${GPU_MEM}MB (${GPU_PCT}%) | Models: $MODELS\"\n  \n  # Alert om >90%\n  if [ $GPU_PCT -gt 90 ]; then\n    curl -X POST https://slack.webhook/... -d '{\"text\": \"‚ö†Ô∏è GPU memory critical: '$GPU_PCT'%\"}'\n  fi\n  \n  sleep 30\ndone\n```\n\n### 2. Systemd service f√∂r monitoring\n\n```ini\n# /etc/systemd/system/ollama-monitor.service\n[Unit]\nDescription=Ollama Memory Monitor\nAfter=ollama.service\n\n[Service]\nExecStart=/usr/local/bin/ollama-monitor.sh\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 3. Python monitoring med Prometheus metrics\n\n```python\nfrom prometheus_client import Gauge, start_http_server\nimport subprocess\nimport time\n\ngpu_memory = Gauge('ollama_gpu_memory_mb', 'GPU memory usage in MB')\nmodel_count = Gauge('ollama_loaded_models', 'Number of loaded models')\n\ndef collect_metrics():\n    # GPU memory\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n        capture_output=True, text=True\n    )\n    gpu_memory.set(int(result.stdout.strip()))\n    \n    # Model count\n    result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True)\n    model_count.set(len(result.stdout.strip().split('\\n')) - 1)\n\nif __name__ == '__main__':\n    start_http_server(9090)\n    while True:\n        collect_metrics()\n        time.sleep(15)\n```\n\n### 4. Automatisk VRAM-rensning vid kritisk niv√•\n\n```bash\n# L√§gg till i crontab\n*/5 * * * * GPU_PCT=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print int($1/122.88)}') && [ $GPU_PCT -gt 95 ] && curl -X POST http://localhost:11434/api/generate -d '{\"model\":\"dummy\",\"keep_alive\":\"0\"}' 2>/dev/null\n```"}
{"instruction": "Hur s√§tter jag upp automatisk backup av Ollama modeller och konfiguration?", "output": "## Automatisk Backup av Ollama Setup\n\n### 1. Backup script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-backup.sh\n\nBACKUP_DIR=\"/backup/ollama\"\nDATE=$(date +%Y%m%d)\nOLLAMA_HOME=\"/usr/share/ollama/.ollama\"\n\n# Skapa backup-katalog\nmkdir -p \"$BACKUP_DIR/$DATE\"\n\n# Backup Modelfiles (viktigast!)\necho \"Backing up Modelfiles...\"\ncp -r /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/ \"$BACKUP_DIR/$DATE/\"\n\n# Backup model manifest (inte weights - f√∂r stora)\necho \"Backing up model manifests...\"\nfind \"$OLLAMA_HOME/models/manifests\" -type f -exec cp --parents {} \"$BACKUP_DIR/$DATE/\" \\;\n\n# Lista installerade modeller\necho \"Saving model list...\"\nollama list > \"$BACKUP_DIR/$DATE/installed_models.txt\"\n\n# Backup custom configs\ncp /etc/systemd/system/ollama.service \"$BACKUP_DIR/$DATE/\" 2>/dev/null\n\n# Komprimera\ntar -czf \"$BACKUP_DIR/ollama-backup-$DATE.tar.gz\" -C \"$BACKUP_DIR\" \"$DATE\"\nrm -rf \"$BACKUP_DIR/$DATE\"\n\n# Beh√•ll senaste 7 backups\nls -t \"$BACKUP_DIR\"/*.tar.gz | tail -n +8 | xargs rm -f 2>/dev/null\n\necho \"Backup complete: $BACKUP_DIR/ollama-backup-$DATE.tar.gz\"\n```\n\n### 2. Restore script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/ollama-restore.sh\n\nBACKUP_FILE=$1\nif [ -z \"$BACKUP_FILE\" ]; then\n  echo \"Usage: $0 <backup-file.tar.gz>\"\n  exit 1\nfi\n\n# Extrahera\nTMP_DIR=$(mktemp -d)\ntar -xzf \"$BACKUP_FILE\" -C \"$TMP_DIR\"\nDATE_DIR=$(ls \"$TMP_DIR\")\n\n# √Öterst√§ll Modelfiles\ncp -r \"$TMP_DIR/$DATE_DIR/modelfiles/\"* /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/\n\n# Skapa modeller fr√•n Modelfiles\nfor mf in /home/ai-server/01_PROJECTS/01_AI-VIBE-WORLD/modelfiles/Modelfile.*; do\n  MODEL_NAME=$(basename \"$mf\" | sed 's/Modelfile.//')\n  echo \"Creating model: $MODEL_NAME\"\n  ollama create \"$MODEL_NAME\" -f \"$mf\"\ndone\n\nrm -rf \"$TMP_DIR\"\necho \"Restore complete!\"\n```\n\n### 3. Cron f√∂r daglig backup\n\n```bash\n# crontab -e\n0 3 * * * /usr/local/bin/ollama-backup.sh >> /var/log/ollama-backup.log 2>&1\n```\n\n### 4. Sync till remote (optional)\n\n```bash\n# L√§gg till i backup-scriptet\nrsync -avz \"$BACKUP_DIR/\" backup-server:/backups/ollama/\n```"}
{"instruction": "Hur fels√∂ker jag n√§r Ollama inte svarar eller √§r l√•ngsam?", "output": "## Ollama Fels√∂kningsguide\n\n### Steg 1: Kolla status\n\n```bash\n# Service status\nsystemctl status ollama\n\n# Process\nps aux | grep ollama\n\n# Lyssnar p√• port?\nlsof -i :11434\n\n# API health\ncurl http://localhost:11434/api/tags\n```\n\n### Steg 2: GPU-status\n\n```bash\n# GPU memory och utilization\nnvidia-smi\n\n# Laddade modeller\nollama ps\n\n# Om GPU full - unload alla modeller\nfor model in $(ollama ps -q); do\n  curl -X POST http://localhost:11434/api/generate \\\n    -d \"{\\\"model\\\":\\\"$model\\\",\\\"keep_alive\\\":\\\"0\\\"}\"\ndone\n```\n\n### Steg 3: Vanliga problem och l√∂sningar\n\n**Problem: \"connection refused\"**\n```bash\n# Starta om Ollama\nsudo systemctl restart ollama\n\n# V√§nta p√• start\nsleep 5 && curl http://localhost:11434/api/tags\n```\n\n**Problem: \"CUDA out of memory\"**\n```bash\n# Kolla vad som anv√§nder GPU\nnvidia-smi --query-compute-apps=pid,name,used_memory --format=csv\n\n# D√∂da zombie-processer\nfuser -k /dev/nvidia*\n\n# Starta om\nsudo systemctl restart ollama\n```\n\n**Problem: L√•ngsam inference**\n```bash\n# Kolla om modellen √§r laddad\nollama ps\n\n# Om inte - pre-load\ncurl -X POST http://localhost:11434/api/generate \\\n  -d '{\"model\":\"qwen2.5-coder:14b\",\"prompt\":\"\",\"keep_alive\":\"10m\"}'\n\n# Kolla context size (l√§gre = snabbare)\n# I Modelfile: PARAMETER num_ctx 4096\n```\n\n**Problem: Modell hittas inte**\n```bash\n# Lista installerade\nollama list\n\n# Ladda ner\nollama pull qwen2.5-coder:14b\n\n# Eller skapa fr√•n Modelfile\nollama create qwen-sonnet -f modelfiles/Modelfile.qwen-sonnet\n```\n\n### Steg 4: Loggar\n\n```bash\n# Service loggar\njournalctl -u ollama -f\n\n# Med debug\nOLLAMA_DEBUG=1 ollama serve\n```\n\n### Quick Fix Script\n\n```bash\n#!/bin/bash\n# /usr/local/bin/fix-ollama.sh\n\necho \"üîß Fixing Ollama...\"\n\n# Stop\nsudo systemctl stop ollama\nsleep 2\n\n# Clear GPU\nfuser -k /dev/nvidia* 2>/dev/null\n\n# Start\nsudo systemctl start ollama\nsleep 3\n\n# Verify\nif curl -s http://localhost:11434/api/tags > /dev/null; then\n  echo \"‚úÖ Ollama is running!\"\n  ollama list\nelse\n  echo \"‚ùå Ollama failed to start\"\n  journalctl -u ollama --since \"1 minute ago\"\nfi\n```"}
{"instruction": "Hur konfigurerar jag Ollama f√∂r optimal prestanda p√• RTX 4070 (12GB)?", "output": "## Optimal Ollama-konfiguration f√∂r RTX 4070 12GB\n\n### 1. Systemd service optimering\n\n```ini\n# /etc/systemd/system/ollama.service\n[Unit]\nDescription=Ollama Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=ollama\nGroup=ollama\n\n# Milj√∂variabler f√∂r prestanda\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=1\"\nEnvironment=\"CUDA_VISIBLE_DEVICES=0\"\n\nExecStart=/usr/local/bin/ollama serve\nRestart=always\nRestartSec=3\n\n# Resource limits\nLimitNOFILE=65535\nLimitNPROC=4096\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 2. Modelfile-optimeringar\n\n```dockerfile\n# Modelfile.qwen-optimized\nFROM qwen2.5-coder:14b\n\n# Context size - balans mellan kapacitet och VRAM\nPARAMETER num_ctx 8192\n\n# GPU layers - alla p√• GPU f√∂r 14B\nPARAMETER num_gpu 99\n\n# Batch size\nPARAMETER num_batch 512\n\n# Temperature f√∂r konsistens\nPARAMETER temperature 0.7\n\n# Stop tokens\nPARAMETER stop <|im_end|>\nPARAMETER stop <|endoftext|>\n```\n\n### 3. VRAM-budget f√∂r 12GB\n\n| Modell | VRAM | Context | Rekommendation |\n|--------|------|---------|----------------|\n| 7B Q4 | ~4GB | 8192 | ‚úÖ Kan k√∂ra 2 parallellt |\n| 14B Q4 | ~9GB | 8192 | ‚úÖ En √•t g√•ngen |\n| 14B Q4 | ~11GB | 16384 | ‚ö†Ô∏è Tight, minska context |\n| 32B Q4 | ~18GB | - | ‚ùå F√∂r stor |\n\n### 4. Hot-swap strategi f√∂r cascade\n\n```python\n# I cascade_orchestrator.py\nasync def switch_model(self, model: str):\n    # Unload current f√∂r att frig√∂ra VRAM\n    await self.unload_all()\n    \n    # Pre-load next model\n    await self.client.post(\n        f\"{self.base_url}/api/generate\",\n        json={\n            \"model\": model,\n            \"prompt\": \"\",\n            \"keep_alive\": \"5m\"  # 5 min timeout\n        }\n    )\n```\n\n### 5. Warmup script vid boot\n\n```bash\n#!/bin/bash\n# /usr/local/bin/warmup-models.sh\n\necho \"Warming up primary model...\"\ncurl -s -X POST http://localhost:11434/api/generate \\\n  -d '{\"model\":\"qwen2.5-coder:14b\",\"prompt\":\"Hello\",\"stream\":false}' \\\n  > /dev/null\n\necho \"Model loaded and ready!\"\nollama ps\n```\n\n### 6. Monitoring\n\n```bash\n# Alias f√∂r snabb koll\nalias gpu='nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv'\nalias models='ollama ps'\n```"}
{"instruction": "Skapa en TypeScript service f√∂r att hantera WebSocket connections med heartbeat", "output": "```typescript\nimport { WebSocket, WebSocketServer } from 'ws';\n\ninterface Client {\n  ws: WebSocket;\n  id: string;\n  isAlive: boolean;\n  lastPing: number;\n}\n\nexport class WebSocketManager {\n  private wss: WebSocketServer;\n  private clients: Map<string, Client> = new Map();\n  private heartbeatInterval: NodeJS.Timer;\n  private readonly HEARTBEAT_MS = 30000;\n  private readonly TIMEOUT_MS = 60000;\n\n  constructor(port: number) {\n    this.wss = new WebSocketServer({ port });\n    this.setupServer();\n    this.startHeartbeat();\n  }\n\n  private setupServer() {\n    this.wss.on('connection', (ws, req) => {\n      const clientId = this.generateId();\n      const client: Client = {\n        ws,\n        id: clientId,\n        isAlive: true,\n        lastPing: Date.now(),\n      };\n\n      this.clients.set(clientId, client);\n      console.log(`Client connected: ${clientId}`);\n\n      ws.on('pong', () => {\n        client.isAlive = true;\n        client.lastPing = Date.now();\n      });\n\n      ws.on('message', (data) => {\n        this.handleMessage(client, data.toString());\n      });\n\n      ws.on('close', () => {\n        this.clients.delete(clientId);\n        console.log(`Client disconnected: ${clientId}`);\n      });\n\n      // Send welcome message\n      ws.send(JSON.stringify({ type: 'connected', clientId }));\n    });\n  }\n\n  private startHeartbeat() {\n    this.heartbeatInterval = setInterval(() => {\n      const now = Date.now();\n\n      this.clients.forEach((client, id) => {\n        // Check timeout\n        if (now - client.lastPing > this.TIMEOUT_MS) {\n          console.log(`Client timed out: ${id}`);\n          client.ws.terminate();\n          this.clients.delete(id);\n          return;\n        }\n\n        // Send ping\n        if (client.isAlive) {\n          client.isAlive = false;\n          client.ws.ping();\n        }\n      });\n    }, this.HEARTBEAT_MS);\n  }\n\n  broadcast(message: object, excludeId?: string) {\n    const data = JSON.stringify(message);\n    this.clients.forEach((client, id) => {\n      if (id !== excludeId && client.ws.readyState === WebSocket.OPEN) {\n        client.ws.send(data);\n      }\n    });\n  }\n\n  send(clientId: string, message: object) {\n    const client = this.clients.get(clientId);\n    if (client?.ws.readyState === WebSocket.OPEN) {\n      client.ws.send(JSON.stringify(message));\n    }\n  }\n\n  private handleMessage(client: Client, data: string) {\n    try {\n      const message = JSON.parse(data);\n      // Handle different message types\n      console.log(`Message from ${client.id}:`, message);\n    } catch (e) {\n      console.error('Invalid message:', data);\n    }\n  }\n\n  private generateId(): string {\n    return `client_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  getConnectedClients(): number {\n    return this.clients.size;\n  }\n\n  shutdown() {\n    clearInterval(this.heartbeatInterval);\n    this.clients.forEach((client) => client.ws.close());\n    this.wss.close();\n  }\n}\n```"}
{"instruction": "Implementera en Python middleware f√∂r request logging och timing", "output": "```python\nimport time\nimport uuid\nimport logging\nfrom typing import Callable\nfrom fastapi import Request, Response\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.types import ASGIApp\n\nlogger = logging.getLogger(__name__)\n\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware f√∂r request logging, timing och request ID.\"\"\"\n    \n    def __init__(self, app: ASGIApp, exclude_paths: list[str] = None):\n        super().__init__(app)\n        self.exclude_paths = exclude_paths or [\"/health\", \"/metrics\"]\n    \n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n        # Skip excluded paths\n        if any(request.url.path.startswith(p) for p in self.exclude_paths):\n            return await call_next(request)\n        \n        # Generate request ID\n        request_id = str(uuid.uuid4())[:8]\n        request.state.request_id = request_id\n        \n        # Log request start\n        start_time = time.perf_counter()\n        logger.info(\n            f\"[{request_id}] {request.method} {request.url.path} started\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"client\": request.client.host if request.client else \"unknown\",\n            }\n        )\n        \n        # Process request\n        try:\n            response = await call_next(request)\n        except Exception as e:\n            duration_ms = (time.perf_counter() - start_time) * 1000\n            logger.error(\n                f\"[{request_id}] {request.method} {request.url.path} \"\n                f\"FAILED after {duration_ms:.2f}ms: {e}\"\n            )\n            raise\n        \n        # Calculate duration\n        duration_ms = (time.perf_counter() - start_time) * 1000\n        \n        # Add headers\n        response.headers[\"X-Request-ID\"] = request_id\n        response.headers[\"X-Response-Time\"] = f\"{duration_ms:.2f}ms\"\n        \n        # Log request completion\n        log_level = logging.WARNING if response.status_code >= 400 else logging.INFO\n        logger.log(\n            log_level,\n            f\"[{request_id}] {request.method} {request.url.path} \"\n            f\"{response.status_code} in {duration_ms:.2f}ms\",\n            extra={\n                \"request_id\": request_id,\n                \"method\": request.method,\n                \"path\": request.url.path,\n                \"status_code\": response.status_code,\n                \"duration_ms\": duration_ms,\n            }\n        )\n        \n        return response\n\n\n# Anv√§ndning i FastAPI:\nfrom fastapi import FastAPI\n\napp = FastAPI()\napp.add_middleware(\n    RequestLoggingMiddleware,\n    exclude_paths=[\"/health\", \"/metrics\", \"/favicon.ico\"]\n)\n```"}
{"instruction": "Skapa en React context f√∂r theme switching med system preference detection", "output": "```typescript\nimport React, { createContext, useContext, useEffect, useState } from 'react';\n\ntype Theme = 'light' | 'dark' | 'system';\ntype ResolvedTheme = 'light' | 'dark';\n\ninterface ThemeContextValue {\n  theme: Theme;\n  resolvedTheme: ResolvedTheme;\n  setTheme: (theme: Theme) => void;\n  toggleTheme: () => void;\n}\n\nconst ThemeContext = createContext<ThemeContextValue | null>(null);\n\nconst STORAGE_KEY = 'theme-preference';\n\nfunction getSystemTheme(): ResolvedTheme {\n  if (typeof window === 'undefined') return 'dark';\n  return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n}\n\nexport function ThemeProvider({ children }: { children: React.ReactNode }) {\n  const [theme, setThemeState] = useState<Theme>(() => {\n    if (typeof window === 'undefined') return 'system';\n    return (localStorage.getItem(STORAGE_KEY) as Theme) || 'system';\n  });\n\n  const [resolvedTheme, setResolvedTheme] = useState<ResolvedTheme>(() => {\n    if (theme === 'system') return getSystemTheme();\n    return theme;\n  });\n\n  // Listen for system theme changes\n  useEffect(() => {\n    const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');\n    \n    const handleChange = (e: MediaQueryListEvent) => {\n      if (theme === 'system') {\n        setResolvedTheme(e.matches ? 'dark' : 'light');\n      }\n    };\n\n    mediaQuery.addEventListener('change', handleChange);\n    return () => mediaQuery.removeEventListener('change', handleChange);\n  }, [theme]);\n\n  // Update resolved theme when theme changes\n  useEffect(() => {\n    const resolved = theme === 'system' ? getSystemTheme() : theme;\n    setResolvedTheme(resolved);\n  }, [theme]);\n\n  // Apply theme to document\n  useEffect(() => {\n    document.documentElement.classList.remove('light', 'dark');\n    document.documentElement.classList.add(resolvedTheme);\n    document.documentElement.style.colorScheme = resolvedTheme;\n  }, [resolvedTheme]);\n\n  const setTheme = (newTheme: Theme) => {\n    setThemeState(newTheme);\n    localStorage.setItem(STORAGE_KEY, newTheme);\n  };\n\n  const toggleTheme = () => {\n    const newTheme = resolvedTheme === 'dark' ? 'light' : 'dark';\n    setTheme(newTheme);\n  };\n\n  return (\n    <ThemeContext.Provider value={{ theme, resolvedTheme, setTheme, toggleTheme }}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}\n\nexport function useTheme() {\n  const context = useContext(ThemeContext);\n  if (!context) {\n    throw new Error('useTheme must be used within a ThemeProvider');\n  }\n  return context;\n}\n\n// Anv√§ndning:\n// <ThemeProvider>\n//   <App />\n// </ThemeProvider>\n//\n// const { theme, resolvedTheme, setTheme, toggleTheme } = useTheme();\n```"}
{"instruction": "Implementera en Python service f√∂r structured logging med JSON output", "output": "```python\nimport json\nimport logging\nimport sys\nfrom datetime import datetime\nfrom typing import Any, Dict, Optional\nimport traceback\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter f√∂r structured logging.\"\"\"\n    \n    def __init__(self, service_name: str = \"app\", environment: str = \"development\"):\n        super().__init__()\n        self.service_name = service_name\n        self.environment = environment\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_entry: Dict[str, Any] = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n            \"service\": self.service_name,\n            \"environment\": self.environment,\n        }\n        \n        # Add location info\n        log_entry[\"location\"] = {\n            \"file\": record.filename,\n            \"line\": record.lineno,\n            \"function\": record.funcName,\n        }\n        \n        # Add extra fields\n        if hasattr(record, \"__dict__\"):\n            for key, value in record.__dict__.items():\n                if key not in [\n                    \"name\", \"msg\", \"args\", \"created\", \"filename\",\n                    \"funcName\", \"levelname\", \"levelno\", \"lineno\",\n                    \"module\", \"msecs\", \"pathname\", \"process\",\n                    \"processName\", \"relativeCreated\", \"stack_info\",\n                    \"thread\", \"threadName\", \"exc_info\", \"exc_text\",\n                    \"message\", \"getMessage\"\n                ]:\n                    log_entry[key] = value\n        \n        # Add exception info\n        if record.exc_info:\n            log_entry[\"exception\"] = {\n                \"type\": record.exc_info[0].__name__ if record.exc_info[0] else None,\n                \"message\": str(record.exc_info[1]) if record.exc_info[1] else None,\n                \"traceback\": traceback.format_exception(*record.exc_info),\n            }\n        \n        return json.dumps(log_entry, default=str, ensure_ascii=False)\n\n\ndef setup_logging(\n    service_name: str = \"app\",\n    level: str = \"INFO\",\n    json_output: bool = True\n) -> logging.Logger:\n    \"\"\"Konfigurera structured logging.\"\"\"\n    \n    # Remove existing handlers\n    root = logging.getLogger()\n    for handler in root.handlers[:]:\n        root.removeHandler(handler)\n    \n    # Create handler\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(getattr(logging, level.upper()))\n    \n    if json_output:\n        handler.setFormatter(JSONFormatter(service_name=service_name))\n    else:\n        handler.setFormatter(logging.Formatter(\n            \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n        ))\n    \n    root.addHandler(handler)\n    root.setLevel(getattr(logging, level.upper()))\n    \n    return root\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"H√§mta en logger instance.\"\"\"\n    return logging.getLogger(name)\n\n\n# Anv√§ndning:\nsetup_logging(service_name=\"ai-vibe-backend\", level=\"INFO\", json_output=True)\n\nlogger = get_logger(__name__)\nlogger.info(\"Server started\", extra={\"port\": 8000, \"version\": \"1.0.0\"})\nlogger.warning(\"High memory usage\", extra={\"memory_percent\": 85})\n\n# Output (JSON):\n# {\"timestamp\": \"2025-12-03T19:00:00Z\", \"level\": \"INFO\", \"message\": \"Server started\", \"port\": 8000, ...}\n```"}
{"instruction": "Skapa en n8n workflow f√∂r competitive intelligence monitoring", "output": "```json\n{\n  \"name\": \"Competitive Intelligence Monitor\",\n  \"nodes\": [\n    {\n      \"name\": \"Schedule Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": {\n        \"rule\": { \"interval\": [{ \"field\": \"hours\", \"hoursInterval\": 6 }] }\n      }\n    },\n    {\n      \"name\": \"Define Competitors\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"parameters\": {\n        \"values\": {\n          \"string\": [\n            { \"name\": \"competitors\", \"value\": \"[{\\\"name\\\":\\\"Competitor A\\\",\\\"url\\\":\\\"https://competitor-a.com\\\"},{\\\"name\\\":\\\"Competitor B\\\",\\\"url\\\":\\\"https://competitor-b.com\\\"}]\" }\n          ]\n        }\n      }\n    },\n    {\n      \"name\": \"Split Competitors\",\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"parameters\": { \"batchSize\": 1 }\n    },\n    {\n      \"name\": \"Fetch Website\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"={{$json.url}}\",\n        \"options\": { \"timeout\": 30000 }\n      }\n    },\n    {\n      \"name\": \"Take Screenshot\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"https://api.screenshotmachine.com\",\n        \"qs\": { \"url\": \"={{$json.url}}\", \"dimension\": \"1920x1080\" }\n      }\n    },\n    {\n      \"name\": \"Analyze Changes\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Analysera konkurrenters webbplats och identifiera:\\n1. Nya produkter/features\\n2. Pris√§ndringar\\n3. Design/UX f√∂r√§ndringar\\n4. Nytt inneh√•ll\\n\\nSvara med JSON: {\\\"changes\\\": [], \\\"significance\\\": \\\"low/medium/high\\\", \\\"action_items\\\": []}\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analysera {{$json.name}}:\\n\\n{{$json.body}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Filter Significant\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"string\": [{ \"value1\": \"={{$json.significance}}\", \"operation\": \"notEqual\", \"value2\": \"low\" }]\n        }\n      }\n    },\n    {\n      \"name\": \"Store to Database\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"insert\",\n        \"table\": \"competitive_intel\",\n        \"columns\": \"competitor,changes,significance,timestamp\"\n      }\n    },\n    {\n      \"name\": \"Alert Team\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#competitive-intel\",\n        \"text\": \"üîç *Konkurrensintelligens Alert*\\n\\n*{{$json.name}}* har gjort {{$json.significance}} f√∂r√§ndringar:\\n\\n{{$json.changes}}\\n\\n*Action items:*\\n{{$json.action_items}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. K√∂r var 6:e timme\\n2. H√§mtar konkurrenters webbplatser\\n3. Tar screenshots f√∂r visuell j√§mf√∂relse\\n4. Analyserar med AI f√∂r att hitta f√∂r√§ndringar\\n5. Filtrerar bort low-significance changes\\n6. Sparar till databas och alertar teamet"}
{"instruction": "Skapa en n8n workflow f√∂r automated lead scoring baserat p√• beteendedata", "output": "```json\n{\n  \"name\": \"AI-Powered Lead Scoring Engine\",\n  \"nodes\": [\n    {\n      \"name\": \"Webhook Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": {\n        \"path\": \"lead-activity\",\n        \"httpMethod\": \"POST\",\n        \"responseMode\": \"onReceived\"\n      }\n    },\n    {\n      \"name\": \"Get Lead History\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"executeQuery\",\n        \"query\": \"SELECT * FROM lead_activities WHERE lead_id = $1 ORDER BY timestamp DESC LIMIT 50\",\n        \"values\": \"={{$json.lead_id}}\"\n      }\n    },\n    {\n      \"name\": \"Calculate Engagement Score\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const activities = $input.all();\\nconst weights = {\\n  'page_view': 1,\\n  'download': 5,\\n  'email_open': 2,\\n  'email_click': 4,\\n  'demo_request': 20,\\n  'pricing_view': 10,\\n  'contact_form': 15\\n};\\n\\nlet score = 0;\\nlet recentActivity = false;\\nconst now = new Date();\\n\\nfor (const activity of activities) {\\n  const weight = weights[activity.json.type] || 1;\\n  const daysAgo = (now - new Date(activity.json.timestamp)) / (1000 * 60 * 60 * 24);\\n  \\n  // Decay: recent = full points, old = less\\n  const decayMultiplier = Math.max(0.1, 1 - (daysAgo / 30));\\n  score += weight * decayMultiplier;\\n  \\n  if (daysAgo < 7) recentActivity = true;\\n}\\n\\nreturn [{\\n  json: {\\n    lead_id: $input.first().json.lead_id,\\n    engagement_score: Math.round(score),\\n    recent_activity: recentActivity,\\n    activity_count: activities.length\\n  }\\n}];\"\n      }\n    },\n    {\n      \"name\": \"AI Lead Analysis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en lead scoring-expert. Analysera beteendedata och ge:\\n1. Lead grade (A/B/C/D)\\n2. K√∂psannolikhet (0-100%)\\n3. Rekommenderad action\\n4. B√§sta tid att kontakta\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Lead data:\\nEngagement score: {{$json.engagement_score}}\\nRecent activity: {{$json.recent_activity}}\\nActivity count: {{$json.activity_count}}\\n\\nAktiviteter senaste 30 dagar:\\n{{$node['Get Lead History'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Update CRM\",\n      \"type\": \"n8n-nodes-base.hubspot\",\n      \"parameters\": {\n        \"operation\": \"update\",\n        \"resource\": \"contact\",\n        \"contactId\": \"={{$json.lead_id}}\",\n        \"additionalFields\": {\n          \"lead_score\": \"={{$json.engagement_score}}\",\n          \"lead_grade\": \"={{$json.grade}}\",\n          \"purchase_probability\": \"={{$json.purchase_probability}}\"\n        }\n      }\n    },\n    {\n      \"name\": \"Route Hot Leads\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"string\": [{ \"value1\": \"={{$json.grade}}\", \"value2\": \"A\" }]\n        }\n      }\n    },\n    {\n      \"name\": \"Create Sales Task\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"resource\": \"task\",\n        \"name\": \"üî• HOT LEAD: Contact {{$json.lead_name}}\",\n        \"priority\": 1,\n        \"dueDate\": \"={{$json.best_contact_time}}\"\n      }\n    },\n    {\n      \"name\": \"Notify Sales Rep\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#sales-alerts\",\n        \"text\": \"üéØ *Grade A Lead Alert!*\\n\\n*Lead:* {{$json.lead_name}}\\n*Score:* {{$json.engagement_score}}\\n*Purchase probability:* {{$json.purchase_probability}}%\\n*Recommended action:* {{$json.recommended_action}}\\n*Best contact time:* {{$json.best_contact_time}}\"\n      }\n    }\n  ]\n}\n```"}
{"instruction": "Hur skapar jag en custom Ollama model med specifikt beteende?", "output": "## Skapa Custom Ollama Model med Modelfile\n\n### 1. Grundl√§ggande Modelfile-struktur\n\n```dockerfile\n# modelfiles/Modelfile.my-assistant\n\n# Basmodell\nFROM qwen2.5-coder:14b\n\n# System prompt - definierar beteende\nSYSTEM \"\"\"\nDu √§r en svensk AI-assistent specialiserad p√• teknisk dokumentation.\n\n## Dina regler:\n1. Svara ALLTID p√• svenska\n2. Ge konkreta kodexempel\n3. F√∂rklara WHY, inte bara HOW\n4. FR√ÖGA ALDRIG - leverera direkt\n\n## Format:\n- Anv√§nd markdown\n- Kod i code blocks med spr√•k\n- Bullet points f√∂r listor\n\"\"\"\n\n# Parametrar\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nPARAMETER num_ctx 8192\nPARAMETER stop <|im_end|>\nPARAMETER stop <|endoftext|>\n\n# Template (optional - f√∂r chat format)\nTEMPLATE \"\"\"<|im_start|>system\n{{ .System }}<|im_end|>\n<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n\"\"\"\n```\n\n### 2. Skapa modellen\n\n```bash\n# Skapa fr√•n Modelfile\nollama create my-assistant -f modelfiles/Modelfile.my-assistant\n\n# Verifiera\nollama list | grep my-assistant\n\n# Testa\nollama run my-assistant \"F√∂rklara async/await i Python\"\n```\n\n### 3. Specialiserade exempel\n\n**Code Reviewer:**\n```dockerfile\nFROM qwen2.5-coder:14b\n\nSYSTEM \"\"\"\nDu √§r en senior code reviewer. F√∂r varje kod du granskar:\n\n## Struktur:\n1. **SAMMANFATTNING** - Kort bed√∂mning\n2. **BUGGAR** - Kritiska problem\n3. **F√ñRB√ÑTTRINGAR** - Suggestions\n4. **S√ÑKERHET** - Security issues\n5. **VERDICT** - APPROVED ‚úì eller NEEDS CHANGES ‚úó\n\n## Regler:\n- Var konstruktiv\n- Ge konkreta fix-f√∂rslag\n- Prioritera: Buggar > S√§kerhet > Performance > Style\n\"\"\"\n```\n\n**Planner:**\n```dockerfile\nFROM devstral:latest\n\nSYSTEM \"\"\"\nDu √§r en teknisk planerare. F√∂r varje uppgift:\n\n## Output format:\n### IMPLEMENTATION STEG\n1. [Steg] - [Tid]\n2. ...\n\n### FILER ATT √ÑNDRA\n- fil.py - [beskrivning]\n\n### POTENTIELLA UTMANINGAR\n- [Utmaning] - [L√∂sning]\n\n## Regler:\n- Bryt ner i 30-min steg\n- Var specifik med filnamn\n- Identifiera beroenden\n\"\"\"\n```\n\n### 4. Uppdatera befintlig modell\n\n```bash\n# Redigera Modelfile\nnano modelfiles/Modelfile.my-assistant\n\n# Skapa om (√∂verskriver)\nollama create my-assistant -f modelfiles/Modelfile.my-assistant\n\n# Testa igen\nollama run my-assistant \"Test prompt\"\n```\n\n### 5. Backup och dela\n\n```bash\n# Exportera Modelfile\nollama show my-assistant --modelfile > backup/Modelfile.my-assistant\n\n# Kopiera till annan maskin\nscp modelfiles/Modelfile.my-assistant user@server:~/modelfiles/\nssh user@server \"ollama create my-assistant -f ~/modelfiles/Modelfile.my-assistant\"\n```\n\n### 6. Tips f√∂r bra system prompts\n\n- **Var specifik** - Vaga instruktioner ger vaga svar\n- **Anv√§nd exempel** - Visa exakt format du vill ha\n- **Negativa instruktioner** - \"G√ñR INTE X\" funkar bra\n- **Rollspel** - \"Du √§r en...\" hj√§lper modellen\n- **Strukturera output** - Definiera rubriker och format"}
{"instruction": "Skapa en n8n workflow f√∂r self-healing infrastructure monitoring", "output": "```json\n{\n  \"name\": \"Self-Healing Infrastructure Monitor\",\n  \"nodes\": [\n    {\n      \"name\": \"Every 5 Minutes\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"parameters\": {\n        \"rule\": { \"interval\": [{ \"field\": \"minutes\", \"minutesInterval\": 5 }] }\n      }\n    },\n    {\n      \"name\": \"Health Checks\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"parameters\": {\n        \"jsCode\": \"const services = [\\n  { name: 'ollama', url: 'http://localhost:11434/api/tags', restartCmd: 'systemctl restart ollama' },\\n  { name: 'backend', url: 'http://localhost:8000/health', restartCmd: 'systemctl restart ai-vibe-backend' },\\n  { name: 'frontend', url: 'http://localhost:5173', restartCmd: 'cd /app/frontend && npm run dev' }\\n];\\n\\nconst results = [];\\nfor (const service of services) {\\n  try {\\n    const response = await fetch(service.url, { timeout: 5000 });\\n    results.push({ ...service, healthy: response.ok, status: response.status });\\n  } catch (e) {\\n    results.push({ ...service, healthy: false, error: e.message });\\n  }\\n}\\n\\nreturn results.map(r => ({ json: r }));\"\n      }\n    },\n    {\n      \"name\": \"Filter Unhealthy\",\n      \"type\": \"n8n-nodes-base.filter\",\n      \"parameters\": {\n        \"conditions\": {\n          \"boolean\": [{ \"value1\": \"={{$json.healthy}}\", \"value2\": false }]\n        }\n      }\n    },\n    {\n      \"name\": \"AI Diagnosis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Du √§r en DevOps-expert. Analysera felande service och ge:\\n1. Trolig orsak\\n2. Recommended fix\\n3. Risk level (low/medium/high)\\n4. Should auto-restart (true/false)\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Service: {{$json.name}}\\nError: {{$json.error}}\\nRestart command: {{$json.restartCmd}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Should Auto-Fix?\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"parameters\": {\n        \"conditions\": {\n          \"boolean\": [{ \"value1\": \"={{$json.should_auto_restart}}\", \"value2\": true }]\n        }\n      }\n    },\n    {\n      \"name\": \"Execute Fix\",\n      \"type\": \"n8n-nodes-base.executeCommand\",\n      \"parameters\": {\n        \"command\": \"={{$json.restartCmd}}\"\n      }\n    },\n    {\n      \"name\": \"Verify Fix\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"parameters\": {\n        \"url\": \"={{$json.url}}\",\n        \"options\": { \"timeout\": 10000 }\n      }\n    },\n    {\n      \"name\": \"Log Result\",\n      \"type\": \"n8n-nodes-base.postgres\",\n      \"parameters\": {\n        \"operation\": \"insert\",\n        \"table\": \"healing_events\",\n        \"columns\": \"service,issue,fix_applied,success,timestamp\"\n      }\n    },\n    {\n      \"name\": \"Alert if Failed\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#alerts\",\n        \"text\": \"üö® *Self-Healing Failed*\\n\\nService: {{$json.name}}\\nIssue: {{$json.error}}\\nDiagnosis: {{$json.diagnosis}}\\n\\n‚ö†Ô∏è Manual intervention required!\"\n      }\n    },\n    {\n      \"name\": \"Escalate High Risk\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#on-call\",\n        \"text\": \"üî¥ *HIGH RISK ISSUE*\\n\\nService: {{$json.name}}\\nRisk: HIGH\\nDiagnosis: {{$json.diagnosis}}\\n\\nAuto-restart disabled - awaiting manual approval.\\n\\nReact with ‚úÖ to approve restart.\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. K√∂r health checks var 5:e minut\\n2. Identifierar unhealthy services\\n3. Anv√§nder AI f√∂r att diagnostisera problem\\n4. Auto-fixar low/medium risk issues\\n5. Eskalerar high risk till on-call\\n6. Loggar alla healing events"}
{"instruction": "Implementera en Python service f√∂r semantic caching av LLM responses", "output": "```python\nimport hashlib\nimport json\nimport asyncio\nfrom typing import Optional, Dict, Any, List\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n@dataclass\nclass CacheEntry:\n    prompt_hash: str\n    prompt: str\n    response: str\n    embedding: List[float]\n    created_at: datetime\n    hits: int = 0\n    \n    @property\n    def age_hours(self) -> float:\n        return (datetime.now() - self.created_at).total_seconds() / 3600\n\nclass SemanticCache:\n    \"\"\"\n    Semantic cache f√∂r LLM responses.\n    Anv√§nder embeddings f√∂r att hitta semantiskt liknande prompts.\n    \"\"\"\n    \n    def __init__(\n        self,\n        embedding_model: str = \"nomic-embed-text\",\n        similarity_threshold: float = 0.92,\n        max_entries: int = 1000,\n        ttl_hours: float = 24.0\n    ):\n        self.embedding_model = embedding_model\n        self.similarity_threshold = similarity_threshold\n        self.max_entries = max_entries\n        self.ttl_hours = ttl_hours\n        self.cache: Dict[str, CacheEntry] = {}\n        self._lock = asyncio.Lock()\n    \n    async def get_embedding(self, text: str) -> List[float]:\n        \"\"\"H√§mta embedding via Ollama.\"\"\"\n        import httpx\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                \"http://localhost:11434/api/embeddings\",\n                json={\"model\": self.embedding_model, \"prompt\": text}\n            )\n            return response.json()[\"embedding\"]\n    \n    def cosine_similarity(self, a: List[float], b: List[float]) -> float:\n        \"\"\"Ber√§kna cosine similarity mellan tv√• vektorer.\"\"\"\n        a_np = np.array(a)\n        b_np = np.array(b)\n        return float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np)))\n    \n    async def get(self, prompt: str) -> Optional[str]:\n        \"\"\"S√∂k efter cached response f√∂r semantiskt liknande prompt.\"\"\"\n        async with self._lock:\n            # Rensa expired entries\n            self._cleanup_expired()\n            \n            # H√§mta embedding f√∂r query\n            query_embedding = await self.get_embedding(prompt)\n            \n            # S√∂k efter match\n            best_match: Optional[CacheEntry] = None\n            best_similarity = 0.0\n            \n            for entry in self.cache.values():\n                similarity = self.cosine_similarity(query_embedding, entry.embedding)\n                if similarity > best_similarity and similarity >= self.similarity_threshold:\n                    best_similarity = similarity\n                    best_match = entry\n            \n            if best_match:\n                best_match.hits += 1\n                return best_match.response\n            \n            return None\n    \n    async def set(self, prompt: str, response: str):\n        \"\"\"Cacha en prompt-response pair.\"\"\"\n        async with self._lock:\n            # Evict if at capacity\n            if len(self.cache) >= self.max_entries:\n                self._evict_lru()\n            \n            # H√§mta embedding\n            embedding = await self.get_embedding(prompt)\n            \n            # Skapa hash f√∂r exakt lookup\n            prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]\n            \n            self.cache[prompt_hash] = CacheEntry(\n                prompt_hash=prompt_hash,\n                prompt=prompt,\n                response=response,\n                embedding=embedding,\n                created_at=datetime.now()\n            )\n    \n    def _cleanup_expired(self):\n        \"\"\"Ta bort expired entries.\"\"\"\n        expired = [\n            key for key, entry in self.cache.items()\n            if entry.age_hours > self.ttl_hours\n        ]\n        for key in expired:\n            del self.cache[key]\n    \n    def _evict_lru(self):\n        \"\"\"Evict least recently used entry.\"\"\"\n        if not self.cache:\n            return\n        \n        # Sortera p√• hits (ascending) och age (descending)\n        lru_key = min(\n            self.cache.keys(),\n            key=lambda k: (self.cache[k].hits, -self.cache[k].age_hours)\n        )\n        del self.cache[lru_key]\n    \n    @property\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Cache statistics.\"\"\"\n        total_hits = sum(e.hits for e in self.cache.values())\n        return {\n            \"entries\": len(self.cache),\n            \"total_hits\": total_hits,\n            \"avg_age_hours\": np.mean([e.age_hours for e in self.cache.values()]) if self.cache else 0\n        }\n\n\n# Anv√§ndning:\ncache = SemanticCache(similarity_threshold=0.92)\n\nasync def chat_with_cache(prompt: str) -> str:\n    # Check cache\n    cached = await cache.get(prompt)\n    if cached:\n        return cached\n    \n    # Generate response\n    response = await ollama.chat(prompt=prompt)\n    \n    # Cache it\n    await cache.set(prompt, response)\n    \n    return response\n```"}
{"instruction": "Skapa en React hook f√∂r optimistic updates med rollback", "output": "```typescript\nimport { useState, useCallback, useRef } from 'react';\n\ninterface OptimisticState<T> {\n  data: T;\n  isUpdating: boolean;\n  error: Error | null;\n}\n\ninterface UseOptimisticUpdateOptions<T> {\n  onSuccess?: (newData: T) => void;\n  onError?: (error: Error, rollbackData: T) => void;\n  retryCount?: number;\n  retryDelay?: number;\n}\n\nexport function useOptimisticUpdate<T>(\n  initialData: T,\n  options: UseOptimisticUpdateOptions<T> = {}\n) {\n  const { onSuccess, onError, retryCount = 0, retryDelay = 1000 } = options;\n  \n  const [state, setState] = useState<OptimisticState<T>>({\n    data: initialData,\n    isUpdating: false,\n    error: null,\n  });\n  \n  const rollbackRef = useRef<T>(initialData);\n  const retryCountRef = useRef(0);\n\n  const update = useCallback(\n    async (\n      optimisticData: T | ((prev: T) => T),\n      asyncOperation: () => Promise<T>\n    ) => {\n      // Store rollback point\n      rollbackRef.current = state.data;\n      retryCountRef.current = 0;\n\n      // Apply optimistic update immediately\n      const newData = typeof optimisticData === 'function'\n        ? (optimisticData as (prev: T) => T)(state.data)\n        : optimisticData;\n\n      setState({\n        data: newData,\n        isUpdating: true,\n        error: null,\n      });\n\n      const attemptOperation = async (): Promise<void> => {\n        try {\n          // Perform actual async operation\n          const serverData = await asyncOperation();\n          \n          // Update with server-confirmed data\n          setState({\n            data: serverData,\n            isUpdating: false,\n            error: null,\n          });\n          \n          onSuccess?.(serverData);\n        } catch (error) {\n          const err = error instanceof Error ? error : new Error(String(error));\n          \n          // Retry logic\n          if (retryCountRef.current < retryCount) {\n            retryCountRef.current++;\n            await new Promise(resolve => setTimeout(resolve, retryDelay));\n            return attemptOperation();\n          }\n          \n          // Rollback on failure\n          setState({\n            data: rollbackRef.current,\n            isUpdating: false,\n            error: err,\n          });\n          \n          onError?.(err, rollbackRef.current);\n        }\n      };\n\n      await attemptOperation();\n    },\n    [state.data, onSuccess, onError, retryCount, retryDelay]\n  );\n\n  const reset = useCallback(() => {\n    setState({\n      data: rollbackRef.current,\n      isUpdating: false,\n      error: null,\n    });\n  }, []);\n\n  return {\n    data: state.data,\n    isUpdating: state.isUpdating,\n    error: state.error,\n    update,\n    reset,\n  };\n}\n\n// Anv√§ndning:\nfunction TodoList() {\n  const {\n    data: todos,\n    isUpdating,\n    error,\n    update,\n  } = useOptimisticUpdate<Todo[]>([], {\n    onError: (error) => toast.error(`Failed: ${error.message}`),\n    retryCount: 2,\n  });\n\n  const addTodo = async (text: string) => {\n    const optimisticTodo: Todo = {\n      id: `temp-${Date.now()}`,\n      text,\n      completed: false,\n    };\n\n    await update(\n      // Optimistic update\n      (prev) => [...prev, optimisticTodo],\n      // Actual API call\n      async () => {\n        const response = await fetch('/api/todos', {\n          method: 'POST',\n          body: JSON.stringify({ text }),\n        });\n        return response.json();\n      }\n    );\n  };\n\n  return (\n    <div className={isUpdating ? 'opacity-50' : ''}>\n      {todos.map(todo => (\n        <TodoItem key={todo.id} todo={todo} />\n      ))}\n      {error && <p className=\"text-red-500\">{error.message}</p>}\n    </div>\n  );\n}\n```"}
{"instruction": "Implementera en Python circuit breaker pattern f√∂r external API calls", "output": "```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom typing import Callable, TypeVar, Generic, Optional\nfrom dataclasses import dataclass, field\nfrom functools import wraps\n\nT = TypeVar('T')\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failing, reject calls\n    HALF_OPEN = \"half_open\"  # Testing if recovered\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5\n    success_threshold: int = 2\n    timeout_seconds: float = 30.0\n    half_open_max_calls: int = 3\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    half_open_calls: int = 0\n\nclass CircuitBreakerError(Exception):\n    \"\"\"Raised when circuit is open.\"\"\"\n    pass\n\nclass CircuitBreaker:\n    \"\"\"\n    Circuit breaker for protecting external service calls.\n    \n    States:\n    - CLOSED: Normal operation, tracking failures\n    - OPEN: Service unhealthy, rejecting calls\n    - HALF_OPEN: Testing if service recovered\n    \"\"\"\n    \n    def __init__(self, name: str, config: CircuitBreakerConfig = None):\n        self.name = name\n        self.config = config or CircuitBreakerConfig()\n        self.state = CircuitState.CLOSED\n        self.stats = CircuitStats()\n        self._lock = asyncio.Lock()\n    \n    async def call(self, func: Callable[..., T], *args, **kwargs) -> T:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        async with self._lock:\n            self._check_state_transition()\n            \n            if self.state == CircuitState.OPEN:\n                raise CircuitBreakerError(\n                    f\"Circuit '{self.name}' is OPEN. \"\n                    f\"Retry after {self._time_until_retry():.1f}s\"\n                )\n            \n            if self.state == CircuitState.HALF_OPEN:\n                if self.stats.half_open_calls >= self.config.half_open_max_calls:\n                    raise CircuitBreakerError(\n                        f\"Circuit '{self.name}' is HALF_OPEN, max test calls reached\"\n                    )\n                self.stats.half_open_calls += 1\n        \n        try:\n            result = await func(*args, **kwargs)\n            await self._on_success()\n            return result\n        except Exception as e:\n            await self._on_failure()\n            raise\n    \n    async def _on_success(self):\n        async with self._lock:\n            self.stats.successes += 1\n            \n            if self.state == CircuitState.HALF_OPEN:\n                if self.stats.successes >= self.config.success_threshold:\n                    self._close()\n            elif self.state == CircuitState.CLOSED:\n                # Reset failure count on success\n                self.stats.failures = 0\n    \n    async def _on_failure(self):\n        async with self._lock:\n            self.stats.failures += 1\n            self.stats.last_failure_time = time.monotonic()\n            \n            if self.state == CircuitState.HALF_OPEN:\n                self._open()\n            elif self.state == CircuitState.CLOSED:\n                if self.stats.failures >= self.config.failure_threshold:\n                    self._open()\n    \n    def _check_state_transition(self):\n        \"\"\"Check if we should transition from OPEN to HALF_OPEN.\"\"\"\n        if self.state == CircuitState.OPEN:\n            if self._time_until_retry() <= 0:\n                self._half_open()\n    \n    def _time_until_retry(self) -> float:\n        if self.stats.last_failure_time is None:\n            return 0\n        elapsed = time.monotonic() - self.stats.last_failure_time\n        return max(0, self.config.timeout_seconds - elapsed)\n    \n    def _open(self):\n        self.state = CircuitState.OPEN\n        self.stats.half_open_calls = 0\n        print(f\"Circuit '{self.name}' OPENED after {self.stats.failures} failures\")\n    \n    def _close(self):\n        self.state = CircuitState.CLOSED\n        self.stats = CircuitStats()\n        print(f\"Circuit '{self.name}' CLOSED - service recovered\")\n    \n    def _half_open(self):\n        self.state = CircuitState.HALF_OPEN\n        self.stats.successes = 0\n        self.stats.half_open_calls = 0\n        print(f\"Circuit '{self.name}' HALF_OPEN - testing recovery\")\n\n\n# Decorator version\ndef circuit_breaker(name: str, config: CircuitBreakerConfig = None):\n    cb = CircuitBreaker(name, config)\n    \n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await cb.call(func, *args, **kwargs)\n        wrapper.circuit = cb  # Expose for monitoring\n        return wrapper\n    return decorator\n\n\n# Anv√§ndning:\n@circuit_breaker(\"ollama-api\", CircuitBreakerConfig(\n    failure_threshold=3,\n    timeout_seconds=60.0\n))\nasync def call_ollama(prompt: str) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:11434/api/generate\",\n            json={\"model\": \"qwen2.5-coder:14b\", \"prompt\": prompt},\n            timeout=30.0\n        )\n        return response.json()[\"response\"]\n\n# Check circuit status\nprint(call_ollama.circuit.state)  # CircuitState.CLOSED\n```"}
{"instruction": "Skapa en n8n workflow f√∂r intelligent content repurposing across platforms", "output": "```json\n{\n  \"name\": \"AI Content Repurposing Engine\",\n  \"nodes\": [\n    {\n      \"name\": \"New Blog Post Trigger\",\n      \"type\": \"n8n-nodes-base.webhook\",\n      \"parameters\": {\n        \"path\": \"new-content\",\n        \"httpMethod\": \"POST\"\n      }\n    },\n    {\n      \"name\": \"Analyze Content\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Analysera inneh√•llet och extrahera:\\n1. Huvudbudskap (1 mening)\\n2. Nyckelpo√§nger (3-5 bullets)\\n3. M√•lgrupp\\n4. Ton (professional/casual/technical)\\n5. Hashtags (5 relevanta)\\n6. Hook (attention-grabbing opener)\\n\\nSvara med JSON.\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Titel: {{$json.title}}\\n\\nInneh√•ll:\\n{{$json.content}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate Twitter Thread\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa en Twitter-tr√•d (5-7 tweets) fr√•n inneh√•llet.\\n\\nRegler:\\n- Tweet 1: Hook med emoji\\n- Tweet 2-6: Nyckelpo√§nger (max 280 tecken/tweet)\\n- Tweet 7: Call-to-action + l√§nk\\n\\nFormat: JSON array med tweets\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Nyckelpo√§nger: {{$json.key_points}}\\nHook: {{$json.hook}}\\nHashtags: {{$json.hashtags}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate LinkedIn Post\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa ett LinkedIn-inl√§gg fr√•n inneh√•llet.\\n\\nStruktur:\\n1. Hook (f√∂rsta raden, attention-grabbing)\\n2. Problem/kontext (2-3 meningar)\\n3. L√∂sning/insight (huvudinneh√•ll)\\n4. Key takeaways (bullets)\\n5. Call-to-action\\n6. Hashtags\\n\\nTon: Professional men personlig\\nL√§ngd: 1000-1500 tecken\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analys: {{$node['Analyze Content'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Generate Newsletter Snippet\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"parameters\": {\n        \"model\": \"qwen2.5-coder:14b\",\n        \"messages\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"Skapa ett newsletter-snippet.\\n\\nStruktur:\\n- Subject line (max 50 tecken, nyfikenhetsskapande)\\n- Preview text (max 100 tecken)\\n- Intro paragraph (personlig ton)\\n- 3 bullet points\\n- Read more CTA\\n\\nFormat: JSON\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"Analys: {{$node['Analyze Content'].json}}\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"Post to Twitter\",\n      \"type\": \"n8n-nodes-base.twitter\",\n      \"parameters\": {\n        \"operation\": \"createThread\",\n        \"tweets\": \"={{$json.tweets}}\"\n      }\n    },\n    {\n      \"name\": \"Post to LinkedIn\",\n      \"type\": \"n8n-nodes-base.linkedIn\",\n      \"parameters\": {\n        \"operation\": \"share\",\n        \"text\": \"={{$json.linkedin_post}}\"\n      }\n    },\n    {\n      \"name\": \"Save to Notion\",\n      \"type\": \"n8n-nodes-base.notion\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"database\": \"Content Library\",\n        \"properties\": {\n          \"Title\": \"={{$json.title}}\",\n          \"Twitter\": \"={{$json.twitter_url}}\",\n          \"LinkedIn\": \"={{$json.linkedin_url}}\",\n          \"Newsletter\": \"={{$json.newsletter_snippet}}\"\n        }\n      }\n    },\n    {\n      \"name\": \"Notify Slack\",\n      \"type\": \"n8n-nodes-base.slack\",\n      \"parameters\": {\n        \"channel\": \"#content\",\n        \"text\": \"üì¢ *Content Repurposed!*\\n\\n*Original:* {{$json.title}}\\n\\n‚úÖ Twitter thread posted\\n‚úÖ LinkedIn post shared\\n‚úÖ Newsletter snippet saved\\n\\nView in Notion: {{$json.notion_url}}\"\n      }\n    }\n  ]\n}\n```\n\nDenna workflow:\\n1. Tar emot nytt blogginl√§gg via webhook\\n2. Analyserar inneh√•llet med AI\\n3. Genererar parallellt: Twitter-tr√•d, LinkedIn-post, Newsletter-snippet\\n4. Publicerar automatiskt till respektive plattform\\n5. Sparar allt i Notion content library\\n6. Notifierar teamet p√• Slack"}
{"instruction": "Implementera health check endpoint med detaljerad systemstatus", "output": "```python\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional\nfrom enum import Enum\nimport asyncio\nimport httpx\nimport psutil\nimport time\nfrom datetime import datetime\n\nclass HealthStatus(str, Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\nclass ComponentHealth(BaseModel):\n    name: str\n    status: HealthStatus\n    latency_ms: Optional[float] = None\n    message: Optional[str] = None\n    details: Optional[Dict] = None\n\nclass SystemHealth(BaseModel):\n    status: HealthStatus\n    timestamp: str\n    uptime_seconds: float\n    components: List[ComponentHealth]\n    system: Dict\n\napp = FastAPI()\nstart_time = time.time()\n\nasync def check_ollama() -> ComponentHealth:\n    \"\"\"Check Ollama API health.\"\"\"\n    start = time.perf_counter()\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                \"http://localhost:11434/api/tags\",\n                timeout=5.0\n            )\n            latency = (time.perf_counter() - start) * 1000\n            \n            if response.status_code == 200:\n                models = response.json().get(\"models\", [])\n                return ComponentHealth(\n                    name=\"ollama\",\n                    status=HealthStatus.HEALTHY,\n                    latency_ms=round(latency, 2),\n                    details={\"loaded_models\": len(models)}\n                )\n            else:\n                return ComponentHealth(\n                    name=\"ollama\",\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Status code: {response.status_code}\"\n                )\n    except Exception as e:\n        return ComponentHealth(\n            name=\"ollama\",\n            status=HealthStatus.UNHEALTHY,\n            message=str(e)\n        )\n\nasync def check_gpu() -> ComponentHealth:\n    \"\"\"Check GPU health and memory.\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total,utilization.gpu,temperature.gpu\",\n             \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True, timeout=5\n        )\n        \n        if result.returncode == 0:\n            parts = result.stdout.strip().split(\", \")\n            mem_used, mem_total = int(parts[0]), int(parts[1])\n            mem_percent = (mem_used / mem_total) * 100\n            \n            status = HealthStatus.HEALTHY\n            if mem_percent > 90:\n                status = HealthStatus.DEGRADED\n            if mem_percent > 98:\n                status = HealthStatus.UNHEALTHY\n            \n            return ComponentHealth(\n                name=\"gpu\",\n                status=status,\n                details={\n                    \"memory_used_mb\": mem_used,\n                    \"memory_total_mb\": mem_total,\n                    \"memory_percent\": round(mem_percent, 1),\n                    \"utilization_percent\": int(parts[2]),\n                    \"temperature_c\": int(parts[3])\n                }\n            )\n        else:\n            return ComponentHealth(\n                name=\"gpu\",\n                status=HealthStatus.UNHEALTHY,\n                message=\"nvidia-smi failed\"\n            )\n    except Exception as e:\n        return ComponentHealth(\n            name=\"gpu\",\n            status=HealthStatus.UNHEALTHY,\n            message=str(e)\n        )\n\nasync def check_disk() -> ComponentHealth:\n    \"\"\"Check disk space.\"\"\"\n    disk = psutil.disk_usage(\"/\")\n    percent_used = disk.percent\n    \n    status = HealthStatus.HEALTHY\n    if percent_used > 85:\n        status = HealthStatus.DEGRADED\n    if percent_used > 95:\n        status = HealthStatus.UNHEALTHY\n    \n    return ComponentHealth(\n        name=\"disk\",\n        status=status,\n        details={\n            \"total_gb\": round(disk.total / (1024**3), 1),\n            \"used_gb\": round(disk.used / (1024**3), 1),\n            \"free_gb\": round(disk.free / (1024**3), 1),\n            \"percent_used\": percent_used\n        }\n    )\n\n@app.get(\"/health\", response_model=SystemHealth)\nasync def health_check(response: Response):\n    \"\"\"Comprehensive health check endpoint.\"\"\"\n    \n    # Run all checks in parallel\n    components = await asyncio.gather(\n        check_ollama(),\n        check_gpu(),\n        check_disk()\n    )\n    \n    # Determine overall status\n    statuses = [c.status for c in components]\n    if HealthStatus.UNHEALTHY in statuses:\n        overall = HealthStatus.UNHEALTHY\n        response.status_code = 503\n    elif HealthStatus.DEGRADED in statuses:\n        overall = HealthStatus.DEGRADED\n        response.status_code = 200\n    else:\n        overall = HealthStatus.HEALTHY\n        response.status_code = 200\n    \n    # System metrics\n    system = {\n        \"cpu_percent\": psutil.cpu_percent(),\n        \"memory_percent\": psutil.virtual_memory().percent,\n        \"load_average\": list(psutil.getloadavg())\n    }\n    \n    return SystemHealth(\n        status=overall,\n        timestamp=datetime.utcnow().isoformat() + \"Z\",\n        uptime_seconds=round(time.time() - start_time, 2),\n        components=list(components),\n        system=system\n    )\n\n# Liveness probe (simple)\n@app.get(\"/health/live\")\nasync def liveness():\n    return {\"status\": \"alive\"}\n\n# Readiness probe (checks dependencies)\n@app.get(\"/health/ready\")\nasync def readiness(response: Response):\n    ollama = await check_ollama()\n    if ollama.status == HealthStatus.UNHEALTHY:\n        response.status_code = 503\n        return {\"status\": \"not ready\", \"reason\": \"ollama unavailable\"}\n    return {\"status\": \"ready\"}\n```"}
{"instruction": "Skapa en TypeScript utility f√∂r type-safe environment variables", "output": "```typescript\nimport { z } from 'zod';\n\n// Define environment schema\nconst envSchema = z.object({\n  // Server\n  NODE_ENV: z.enum(['development', 'production', 'test']).default('development'),\n  PORT: z.string().transform(Number).default('3000'),\n  HOST: z.string().default('0.0.0.0'),\n  \n  // Database\n  DATABASE_URL: z.string().url(),\n  DATABASE_POOL_SIZE: z.string().transform(Number).default('10'),\n  \n  // Ollama\n  OLLAMA_BASE_URL: z.string().url().default('http://localhost:11434'),\n  OLLAMA_MODEL: z.string().default('qwen2.5-coder:14b'),\n  OLLAMA_TIMEOUT_MS: z.string().transform(Number).default('60000'),\n  \n  // Auth (optional)\n  JWT_SECRET: z.string().min(32).optional(),\n  JWT_EXPIRES_IN: z.string().default('7d'),\n  \n  // Feature flags\n  ENABLE_STREAMING: z.string().transform(v => v === 'true').default('true'),\n  ENABLE_CACHE: z.string().transform(v => v === 'true').default('true'),\n  \n  // Debug\n  LOG_LEVEL: z.enum(['debug', 'info', 'warn', 'error']).default('info'),\n  DEBUG: z.string().transform(v => v === 'true').default('false'),\n});\n\ntype Env = z.infer<typeof envSchema>;\n\nclass EnvironmentManager {\n  private env: Env | null = null;\n  private validated = false;\n\n  /**\n   * Validate and parse environment variables.\n   * Call this at application startup.\n   */\n  validate(): Env {\n    if (this.validated && this.env) {\n      return this.env;\n    }\n\n    const result = envSchema.safeParse(process.env);\n\n    if (!result.success) {\n      const errors = result.error.issues\n        .map(issue => `  - ${issue.path.join('.')}: ${issue.message}`)\n        .join('\\n');\n      \n      console.error('‚ùå Environment validation failed:\\n' + errors);\n      \n      // In production, exit immediately\n      if (process.env.NODE_ENV === 'production') {\n        process.exit(1);\n      }\n      \n      throw new Error('Invalid environment configuration');\n    }\n\n    this.env = result.data;\n    this.validated = true;\n\n    // Log in development\n    if (this.env.NODE_ENV === 'development') {\n      console.log('‚úÖ Environment validated:', {\n        NODE_ENV: this.env.NODE_ENV,\n        PORT: this.env.PORT,\n        OLLAMA_MODEL: this.env.OLLAMA_MODEL,\n        LOG_LEVEL: this.env.LOG_LEVEL,\n      });\n    }\n\n    return this.env;\n  }\n\n  /**\n   * Get validated environment.\n   * Throws if validate() hasn't been called.\n   */\n  get(): Env {\n    if (!this.validated || !this.env) {\n      throw new Error(\n        'Environment not validated. Call env.validate() at startup.'\n      );\n    }\n    return this.env;\n  }\n\n  /**\n   * Check if running in production.\n   */\n  isProduction(): boolean {\n    return this.get().NODE_ENV === 'production';\n  }\n\n  /**\n   * Check if running in development.\n   */\n  isDevelopment(): boolean {\n    return this.get().NODE_ENV === 'development';\n  }\n\n  /**\n   * Get a safe subset for client-side (no secrets).\n   */\n  getPublicEnv() {\n    const env = this.get();\n    return {\n      NODE_ENV: env.NODE_ENV,\n      ENABLE_STREAMING: env.ENABLE_STREAMING,\n    };\n  }\n}\n\n// Singleton instance\nexport const env = new EnvironmentManager();\n\n// Type helper for components\nexport type { Env };\n\n// Usage:\n// 1. At app startup:\n//    import { env } from './env';\n//    env.validate();\n//\n// 2. Anywhere else:\n//    const config = env.get();\n//    console.log(config.OLLAMA_MODEL); // Type-safe!\n//\n// 3. In tests:\n//    process.env.DATABASE_URL = 'postgresql://test:test@localhost/test';\n//    env.validate();\n```"}
