{"instruction": "Create a rate limiter middleware for FastAPI that limits requests per IP to 100 per minute", "output": "Here's a production-ready rate limiter middleware:\n\n```python\nfrom fastapi import FastAPI, Request, HTTPException\nfrom collections import defaultdict\nimport time\nimport asyncio\n\nclass RateLimiter:\n    def __init__(self, requests_per_minute: int = 100):\n        self.requests_per_minute = requests_per_minute\n        self.requests = defaultdict(list)\n        self._cleanup_task = None\n    \n    async def start_cleanup(self):\n        while True:\n            await asyncio.sleep(60)\n            cutoff = time.time() - 60\n            for ip in list(self.requests.keys()):\n                self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n                if not self.requests[ip]:\n                    del self.requests[ip]\n    \n    def is_allowed(self, ip: str) -> bool:\n        now = time.time()\n        cutoff = now - 60\n        self.requests[ip] = [t for t in self.requests[ip] if t > cutoff]\n        \n        if len(self.requests[ip]) >= self.requests_per_minute:\n            return False\n        \n        self.requests[ip].append(now)\n        return True\n\nrate_limiter = RateLimiter(requests_per_minute=100)\n\n@app.middleware('http')\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n    \n    if not rate_limiter.is_allowed(client_ip):\n        raise HTTPException(\n            status_code=429,\n            detail='Rate limit exceeded. Max 100 requests per minute.'\n        )\n    \n    return await call_next(request)\n```\n\nKey features:\n1. In-memory storage with automatic cleanup\n2. Sliding window algorithm (more accurate than fixed windows)\n3. Per-IP tracking\n4. Async-compatible cleanup task\n5. Returns 429 with clear error message"}
{"instruction": "Write a Python decorator that retries failed async functions with exponential backoff", "output": "Here's a robust retry decorator with exponential backoff:\n\n```python\nimport asyncio\nimport functools\nimport logging\nfrom typing import Type, Tuple, Callable, Any\n\nlogger = logging.getLogger(__name__)\n\ndef async_retry(\n    max_attempts: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    exceptions: Tuple[Type[Exception], ...] = (Exception,)\n) -> Callable:\n    \"\"\"Retry decorator with exponential backoff for async functions.\n    \n    Args:\n        max_attempts: Maximum number of retry attempts\n        base_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay cap\n        exponential_base: Base for exponential calculation\n        exceptions: Tuple of exceptions to catch and retry\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> Any:\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_attempts - 1:\n                        logger.error(f'{func.__name__} failed after {max_attempts} attempts: {e}')\n                        raise\n                    \n                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n                    logger.warning(f'{func.__name__} attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s')\n                    await asyncio.sleep(delay)\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage example:\n@async_retry(max_attempts=5, base_delay=0.5, exceptions=(ConnectionError, TimeoutError))\nasync def fetch_data(url: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, timeout=10)\n        return response.json()\n```\n\nFeatures:\n1. Configurable retry count and delays\n2. Exponential backoff with max cap\n3. Selective exception handling\n4. Logging for debugging\n5. Preserves function metadata with functools.wraps"}
{"instruction": "Implement a simple pub/sub system in Python using asyncio", "output": "Here's a lightweight async pub/sub implementation:\n\n```python\nimport asyncio\nfrom collections import defaultdict\nfrom typing import Callable, Any, Dict, Set\nfrom dataclasses import dataclass, field\nimport uuid\n\n@dataclass\nclass Subscription:\n    id: str\n    topic: str\n    callback: Callable\n\nclass PubSub:\n    def __init__(self):\n        self._subscribers: Dict[str, Set[Subscription]] = defaultdict(set)\n        self._lock = asyncio.Lock()\n    \n    async def subscribe(self, topic: str, callback: Callable) -> str:\n        \"\"\"Subscribe to a topic. Returns subscription ID for unsubscribing.\"\"\"\n        sub_id = str(uuid.uuid4())\n        subscription = Subscription(id=sub_id, topic=topic, callback=callback)\n        \n        async with self._lock:\n            self._subscribers[topic].add(subscription)\n        \n        return sub_id\n    \n    async def unsubscribe(self, sub_id: str) -> bool:\n        \"\"\"Unsubscribe using subscription ID.\"\"\"\n        async with self._lock:\n            for topic, subs in self._subscribers.items():\n                for sub in subs:\n                    if sub.id == sub_id:\n                        subs.remove(sub)\n                        return True\n        return False\n    \n    async def publish(self, topic: str, message: Any) -> int:\n        \"\"\"Publish message to topic. Returns number of subscribers notified.\"\"\"\n        async with self._lock:\n            subscribers = list(self._subscribers.get(topic, []))\n        \n        if not subscribers:\n            return 0\n        \n        tasks = []\n        for sub in subscribers:\n            if asyncio.iscoroutinefunction(sub.callback):\n                tasks.append(sub.callback(message))\n            else:\n                tasks.append(asyncio.to_thread(sub.callback, message))\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n        return len(subscribers)\n    \n    async def publish_pattern(self, pattern: str, message: Any) -> int:\n        \"\"\"Publish to all topics matching pattern (simple * wildcard).\"\"\"\n        import fnmatch\n        count = 0\n        \n        async with self._lock:\n            matching_topics = [t for t in self._subscribers.keys() if fnmatch.fnmatch(t, pattern)]\n        \n        for topic in matching_topics:\n            count += await self.publish(topic, message)\n        \n        return count\n\n# Usage:\npubsub = PubSub()\n\nasync def handler(msg):\n    print(f'Received: {msg}')\n\nasync def main():\n    sub_id = await pubsub.subscribe('events.user.*', handler)\n    await pubsub.publish('events.user.login', {'user': 'simon'})\n    await pubsub.unsubscribe(sub_id)\n```\n\nFeatures:\n1. Async-safe with locks\n2. Supports both sync and async callbacks\n3. Subscription IDs for clean unsubscribe\n4. Wildcard pattern matching\n5. Thread-safe for sync callbacks"}
{"instruction": "Create a WebSocket connection manager for FastAPI that handles multiple rooms/channels", "output": "Here's a full-featured WebSocket manager with room support:\n\n```python\nfrom fastapi import WebSocket\nfrom typing import Dict, Set, Optional, Any\nimport json\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass Connection:\n    websocket: WebSocket\n    user_id: str\n    rooms: Set[str] = field(default_factory=set)\n    connected_at: datetime = field(default_factory=datetime.utcnow)\n\nclass ConnectionManager:\n    def __init__(self):\n        self._connections: Dict[str, Connection] = {}  # user_id -> Connection\n        self._rooms: Dict[str, Set[str]] = {}  # room_name -> set of user_ids\n        self._lock = asyncio.Lock()\n    \n    async def connect(self, websocket: WebSocket, user_id: str) -> Connection:\n        await websocket.accept()\n        conn = Connection(websocket=websocket, user_id=user_id)\n        \n        async with self._lock:\n            # Disconnect existing connection for same user\n            if user_id in self._connections:\n                await self._disconnect_internal(user_id)\n            self._connections[user_id] = conn\n        \n        return conn\n    \n    async def disconnect(self, user_id: str):\n        async with self._lock:\n            await self._disconnect_internal(user_id)\n    \n    async def _disconnect_internal(self, user_id: str):\n        if user_id not in self._connections:\n            return\n        \n        conn = self._connections[user_id]\n        \n        # Remove from all rooms\n        for room in conn.rooms.copy():\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n        \n        del self._connections[user_id]\n        \n        try:\n            await conn.websocket.close()\n        except:\n            pass\n    \n    async def join_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id not in self._connections:\n                return\n            \n            self._connections[user_id].rooms.add(room)\n            if room not in self._rooms:\n                self._rooms[room] = set()\n            self._rooms[room].add(user_id)\n    \n    async def leave_room(self, user_id: str, room: str):\n        async with self._lock:\n            if user_id in self._connections:\n                self._connections[user_id].rooms.discard(room)\n            if room in self._rooms:\n                self._rooms[room].discard(user_id)\n                if not self._rooms[room]:\n                    del self._rooms[room]\n    \n    async def send_personal(self, user_id: str, message: Any):\n        if user_id not in self._connections:\n            return False\n        \n        try:\n            data = json.dumps(message) if not isinstance(message, str) else message\n            await self._connections[user_id].websocket.send_text(data)\n            return True\n        except:\n            await self.disconnect(user_id)\n            return False\n    \n    async def broadcast_room(self, room: str, message: Any, exclude: Optional[str] = None):\n        if room not in self._rooms:\n            return 0\n        \n        tasks = []\n        for user_id in self._rooms[room]:\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    async def broadcast_all(self, message: Any, exclude: Optional[str] = None):\n        tasks = []\n        for user_id in list(self._connections.keys()):\n            if user_id != exclude:\n                tasks.append(self.send_personal(user_id, message))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return sum(1 for r in results if r is True)\n    \n    def get_room_users(self, room: str) -> Set[str]:\n        return self._rooms.get(room, set()).copy()\n    \n    def get_user_rooms(self, user_id: str) -> Set[str]:\n        if user_id in self._connections:\n            return self._connections[user_id].rooms.copy()\n        return set()\n\n# Usage in FastAPI:\nmanager = ConnectionManager()\n\n@app.websocket('/ws/{user_id}')\nasync def websocket_endpoint(websocket: WebSocket, user_id: str):\n    conn = await manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_json()\n            \n            if data.get('action') == 'join':\n                await manager.join_room(user_id, data['room'])\n            elif data.get('action') == 'leave':\n                await manager.leave_room(user_id, data['room'])\n            elif data.get('action') == 'message':\n                await manager.broadcast_room(data['room'], {\n                    'from': user_id,\n                    'content': data['content']\n                })\n    except:\n        await manager.disconnect(user_id)\n```\n\nFeatures:\n1. Multi-room support with join/leave\n2. Personal and broadcast messaging\n3. Automatic cleanup on disconnect\n4. Thread-safe with async locks\n5. Handles reconnection (kicks old connection)"}
{"instruction": "Write a Python script that monitors a directory for new files and processes them", "output": "Here's a robust file watcher with processing queue:\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler, FileCreatedEvent\nimport queue\nimport threading\nimport logging\nfrom typing import Callable, Optional\nfrom datetime import datetime\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass FileProcessor:\n    def __init__(\n        self,\n        watch_dir: str,\n        process_func: Callable[[Path], None],\n        patterns: Optional[list] = None,\n        debounce_seconds: float = 1.0\n    ):\n        self.watch_dir = Path(watch_dir)\n        self.process_func = process_func\n        self.patterns = patterns or ['*']\n        self.debounce_seconds = debounce_seconds\n        \n        self._queue = queue.Queue()\n        self._seen = {}  # path -> last_seen_time\n        self._running = False\n        self._observer = None\n        self._worker_thread = None\n    \n    def _matches_pattern(self, path: Path) -> bool:\n        import fnmatch\n        return any(fnmatch.fnmatch(path.name, p) for p in self.patterns)\n    \n    def _on_created(self, event):\n        if event.is_directory:\n            return\n        \n        path = Path(event.src_path)\n        if not self._matches_pattern(path):\n            return\n        \n        # Debounce: skip if we've seen this file recently\n        now = datetime.now().timestamp()\n        if path in self._seen:\n            if now - self._seen[path] < self.debounce_seconds:\n                return\n        \n        self._seen[path] = now\n        self._queue.put(path)\n        logger.info(f'Queued: {path.name}')\n    \n    def _worker(self):\n        while self._running:\n            try:\n                path = self._queue.get(timeout=1.0)\n            except queue.Empty:\n                continue\n            \n            # Wait for file to be fully written\n            asyncio.run(self._wait_for_stable(path))\n            \n            if not path.exists():\n                logger.warning(f'File disappeared: {path}')\n                continue\n            \n            try:\n                logger.info(f'Processing: {path.name}')\n                self.process_func(path)\n                logger.info(f'Completed: {path.name}')\n            except Exception as e:\n                logger.error(f'Error processing {path.name}: {e}')\n            finally:\n                self._queue.task_done()\n    \n    async def _wait_for_stable(self, path: Path, checks: int = 3, interval: float = 0.5):\n        \"\"\"Wait until file size stops changing.\"\"\"\n        prev_size = -1\n        stable_count = 0\n        \n        while stable_count < checks:\n            if not path.exists():\n                return\n            \n            size = path.stat().st_size\n            if size == prev_size:\n                stable_count += 1\n            else:\n                stable_count = 0\n                prev_size = size\n            \n            await asyncio.sleep(interval)\n    \n    def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        \n        # Start worker thread\n        self._worker_thread = threading.Thread(target=self._worker, daemon=True)\n        self._worker_thread.start()\n        \n        # Setup watchdog\n        handler = FileSystemEventHandler()\n        handler.on_created = self._on_created\n        \n        self._observer = Observer()\n        self._observer.schedule(handler, str(self.watch_dir), recursive=False)\n        self._observer.start()\n        \n        logger.info(f'Watching: {self.watch_dir}')\n    \n    def stop(self):\n        self._running = False\n        \n        if self._observer:\n            self._observer.stop()\n            self._observer.join()\n        \n        if self._worker_thread:\n            self._worker_thread.join(timeout=5.0)\n        \n        logger.info('Stopped watching')\n    \n    def process_existing(self):\n        \"\"\"Process files that already exist in the directory.\"\"\"\n        for path in self.watch_dir.iterdir():\n            if path.is_file() and self._matches_pattern(path):\n                self._queue.put(path)\n\n# Example usage:\ndef process_file(path: Path):\n    \"\"\"Your processing logic here.\"\"\"\n    content = path.read_text()\n    print(f'File {path.name} has {len(content)} characters')\n    # Move to processed folder, upload somewhere, etc.\n\nif __name__ == '__main__':\n    processor = FileProcessor(\n        watch_dir='/path/to/watch',\n        process_func=process_file,\n        patterns=['*.json', '*.csv'],\n        debounce_seconds=2.0\n    )\n    \n    processor.start()\n    processor.process_existing()  # Handle files already there\n    \n    try:\n        while True:\n            asyncio.run(asyncio.sleep(1))\n    except KeyboardInterrupt:\n        processor.stop()\n```\n\nFeatures:\n1. Pattern-based file filtering\n2. Debouncing to avoid duplicate processing\n3. Waits for files to be fully written before processing\n4. Queue-based processing with worker thread\n5. Can process existing files on startup\n6. Graceful shutdown"}
{"instruction": "Create a caching decorator that supports both sync and async functions with TTL", "output": "Here's a universal caching decorator with TTL support:\n\n```python\nimport asyncio\nimport functools\nimport time\nimport hashlib\nimport json\nfrom typing import Any, Callable, Optional, Union\nfrom collections import OrderedDict\nimport threading\n\nclass Cache:\n    def __init__(self, maxsize: int = 1000):\n        self._cache: OrderedDict = OrderedDict()\n        self._maxsize = maxsize\n        self._lock = threading.Lock()\n    \n    def get(self, key: str) -> tuple[bool, Any]:\n        with self._lock:\n            if key not in self._cache:\n                return False, None\n            \n            value, expiry = self._cache[key]\n            if expiry and time.time() > expiry:\n                del self._cache[key]\n                return False, None\n            \n            # Move to end (LRU)\n            self._cache.move_to_end(key)\n            return True, value\n    \n    def set(self, key: str, value: Any, ttl: Optional[float] = None):\n        with self._lock:\n            expiry = time.time() + ttl if ttl else None\n            \n            if key in self._cache:\n                del self._cache[key]\n            \n            self._cache[key] = (value, expiry)\n            \n            # Evict oldest if over maxsize\n            while len(self._cache) > self._maxsize:\n                self._cache.popitem(last=False)\n    \n    def delete(self, key: str):\n        with self._lock:\n            self._cache.pop(key, None)\n    \n    def clear(self):\n        with self._lock:\n            self._cache.clear()\n\n# Global cache instance\n_default_cache = Cache()\n\ndef _make_key(func: Callable, args: tuple, kwargs: dict) -> str:\n    \"\"\"Create a hashable cache key from function and arguments.\"\"\"\n    key_parts = [\n        func.__module__,\n        func.__qualname__,\n        str(args),\n        str(sorted(kwargs.items()))\n    ]\n    key_string = '|'.join(key_parts)\n    return hashlib.md5(key_string.encode()).hexdigest()\n\ndef cached(\n    ttl: Optional[float] = None,\n    maxsize: int = 1000,\n    key_func: Optional[Callable] = None,\n    cache: Optional[Cache] = None\n) -> Callable:\n    \"\"\"Universal caching decorator for sync and async functions.\n    \n    Args:\n        ttl: Time to live in seconds. None = no expiry.\n        maxsize: Maximum cache entries.\n        key_func: Custom function to generate cache key.\n        cache: Custom Cache instance. Uses global cache if None.\n    \"\"\"\n    _cache = cache or _default_cache\n    \n    def decorator(func: Callable) -> Callable:\n        is_async = asyncio.iscoroutinefunction(func)\n        \n        if is_async:\n            @functools.wraps(func)\n            async def async_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = await func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            async_wrapper.cache_clear = _cache.clear\n            async_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return async_wrapper\n        else:\n            @functools.wraps(func)\n            def sync_wrapper(*args, **kwargs):\n                if key_func:\n                    key = key_func(*args, **kwargs)\n                else:\n                    key = _make_key(func, args, kwargs)\n                \n                found, value = _cache.get(key)\n                if found:\n                    return value\n                \n                result = func(*args, **kwargs)\n                _cache.set(key, result, ttl)\n                return result\n            \n            sync_wrapper.cache_clear = _cache.clear\n            sync_wrapper.cache_delete = lambda *a, **kw: _cache.delete(\n                key_func(*a, **kw) if key_func else _make_key(func, a, kw)\n            )\n            return sync_wrapper\n    \n    return decorator\n\n# Usage examples:\n\n@cached(ttl=60)  # Cache for 60 seconds\ndef get_user(user_id: int) -> dict:\n    # Expensive database query\n    return {'id': user_id, 'name': 'Simon'}\n\n@cached(ttl=300, key_func=lambda url, **kw: url)  # Custom key\nasync def fetch_url(url: str, headers: dict = None) -> str:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        return response.text\n\n# Manual cache control\nget_user.cache_clear()  # Clear all\nget_user.cache_delete(42)  # Delete specific key\n```\n\nFeatures:\n1. Works with both sync and async functions\n2. TTL (time-to-live) support\n3. LRU eviction when maxsize reached\n4. Custom key functions\n5. Thread-safe\n6. Manual cache control methods\n7. Shared or isolated cache instances"}
{"instruction": "Build a simple task queue with priority support using asyncio", "output": "Here's a priority task queue implementation:\n\n```python\nimport asyncio\nimport heapq\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Coroutine, Optional\nfrom datetime import datetime\nimport uuid\nimport logging\nfrom enum import IntEnum\n\nlogger = logging.getLogger(__name__)\n\nclass Priority(IntEnum):\n    LOW = 3\n    NORMAL = 2\n    HIGH = 1\n    CRITICAL = 0\n\n@dataclass(order=True)\nclass Task:\n    priority: int\n    created_at: float = field(compare=True)\n    id: str = field(default_factory=lambda: str(uuid.uuid4()), compare=False)\n    func: Callable = field(compare=False)\n    args: tuple = field(default=(), compare=False)\n    kwargs: dict = field(default_factory=dict, compare=False)\n    result: Any = field(default=None, compare=False)\n    error: Optional[Exception] = field(default=None, compare=False)\n    completed: bool = field(default=False, compare=False)\n\nclass TaskQueue:\n    def __init__(self, num_workers: int = 3):\n        self._heap: list[Task] = []\n        self._num_workers = num_workers\n        self._workers: list[asyncio.Task] = []\n        self._running = False\n        self._task_added = asyncio.Event()\n        self._results: dict[str, Task] = {}\n        self._lock = asyncio.Lock()\n    \n    async def add(\n        self,\n        func: Callable[..., Coroutine],\n        *args,\n        priority: Priority = Priority.NORMAL,\n        **kwargs\n    ) -> str:\n        \"\"\"Add a task to the queue. Returns task ID.\"\"\"\n        task = Task(\n            priority=priority,\n            created_at=datetime.now().timestamp(),\n            func=func,\n            args=args,\n            kwargs=kwargs\n        )\n        \n        async with self._lock:\n            heapq.heappush(self._heap, task)\n            self._results[task.id] = task\n        \n        self._task_added.set()\n        logger.debug(f'Added task {task.id} with priority {priority.name}')\n        return task.id\n    \n    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Wait for task completion and return result.\"\"\"\n        start = asyncio.get_event_loop().time()\n        \n        while True:\n            if task_id in self._results:\n                task = self._results[task_id]\n                if task.completed:\n                    if task.error:\n                        raise task.error\n                    return task.result\n            \n            if timeout:\n                elapsed = asyncio.get_event_loop().time() - start\n                if elapsed >= timeout:\n                    raise TimeoutError(f'Task {task_id} did not complete in {timeout}s')\n            \n            await asyncio.sleep(0.1)\n    \n    async def _worker(self, worker_id: int):\n        logger.info(f'Worker {worker_id} started')\n        \n        while self._running:\n            # Wait for tasks\n            task = None\n            async with self._lock:\n                if self._heap:\n                    task = heapq.heappop(self._heap)\n            \n            if not task:\n                self._task_added.clear()\n                try:\n                    await asyncio.wait_for(self._task_added.wait(), timeout=1.0)\n                except asyncio.TimeoutError:\n                    pass\n                continue\n            \n            # Execute task\n            logger.debug(f'Worker {worker_id} executing task {task.id}')\n            try:\n                if asyncio.iscoroutinefunction(task.func):\n                    task.result = await task.func(*task.args, **task.kwargs)\n                else:\n                    task.result = await asyncio.to_thread(task.func, *task.args, **task.kwargs)\n            except Exception as e:\n                task.error = e\n                logger.error(f'Task {task.id} failed: {e}')\n            finally:\n                task.completed = True\n        \n        logger.info(f'Worker {worker_id} stopped')\n    \n    async def start(self):\n        if self._running:\n            return\n        \n        self._running = True\n        self._workers = [\n            asyncio.create_task(self._worker(i))\n            for i in range(self._num_workers)\n        ]\n        logger.info(f'TaskQueue started with {self._num_workers} workers')\n    \n    async def stop(self, wait_for_completion: bool = True):\n        if wait_for_completion:\n            # Wait for queue to drain\n            while self._heap:\n                await asyncio.sleep(0.1)\n        \n        self._running = False\n        self._task_added.set()  # Wake up workers\n        \n        await asyncio.gather(*self._workers, return_exceptions=True)\n        logger.info('TaskQueue stopped')\n    \n    @property\n    def pending_count(self) -> int:\n        return len(self._heap)\n    \n    def get_status(self, task_id: str) -> dict:\n        if task_id not in self._results:\n            return {'status': 'not_found'}\n        \n        task = self._results[task_id]\n        return {\n            'id': task.id,\n            'status': 'completed' if task.completed else 'pending',\n            'priority': Priority(task.priority).name,\n            'error': str(task.error) if task.error else None\n        }\n\n# Example usage:\nasync def slow_task(name: str, delay: float) -> str:\n    await asyncio.sleep(delay)\n    return f'Task {name} completed after {delay}s'\n\nasync def main():\n    queue = TaskQueue(num_workers=2)\n    await queue.start()\n    \n    # Add tasks with different priorities\n    id1 = await queue.add(slow_task, 'low', 2.0, priority=Priority.LOW)\n    id2 = await queue.add(slow_task, 'critical', 1.0, priority=Priority.CRITICAL)\n    id3 = await queue.add(slow_task, 'normal', 1.5, priority=Priority.NORMAL)\n    \n    # Critical will run first despite being added second\n    result = await queue.get_result(id2)\n    print(result)  # 'Task critical completed after 1.0s'\n    \n    await queue.stop()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nFeatures:\n1. Priority levels (CRITICAL, HIGH, NORMAL, LOW)\n2. Multiple worker support\n3. Async and sync function support\n4. Result retrieval with timeout\n5. Graceful shutdown\n6. Task status tracking\n7. Thread-safe operations"}
{"instruction": "Implement a circuit breaker pattern for external API calls", "output": "Here's a robust circuit breaker implementation:\n\n```python\nimport asyncio\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Optional, Any, Type\nimport functools\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = 'closed'      # Normal operation\n    OPEN = 'open'          # Failing, reject calls\n    HALF_OPEN = 'half_open'  # Testing recovery\n\n@dataclass\nclass CircuitStats:\n    failures: int = 0\n    successes: int = 0\n    last_failure_time: Optional[float] = None\n    last_success_time: Optional[float] = None\n    consecutive_failures: int = 0\n    consecutive_successes: int = 0\n\nclass CircuitBreakerOpen(Exception):\n    \"\"\"Raised when circuit breaker is open.\"\"\"\n    def __init__(self, name: str, retry_after: float):\n        self.name = name\n        self.retry_after = retry_after\n        super().__init__(f'Circuit {name} is open. Retry after {retry_after:.1f}s')\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        name: str,\n        failure_threshold: int = 5,\n        success_threshold: int = 3,\n        timeout: float = 30.0,\n        exceptions: tuple[Type[Exception], ...] = (Exception,),\n        exclude: tuple[Type[Exception], ...] = ()\n    ):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.success_threshold = success_threshold\n        self.timeout = timeout\n        self.exceptions = exceptions\n        self.exclude = exclude\n        \n        self._state = CircuitState.CLOSED\n        self._stats = CircuitStats()\n        self._opened_at: Optional[float] = None\n        self._lock = asyncio.Lock()\n    \n    @property\n    def state(self) -> CircuitState:\n        if self._state == CircuitState.OPEN:\n            if time.time() - self._opened_at >= self.timeout:\n                return CircuitState.HALF_OPEN\n        return self._state\n    \n    async def _on_success(self):\n        async with self._lock:\n            self._stats.successes += 1\n            self._stats.last_success_time = time.time()\n            self._stats.consecutive_successes += 1\n            self._stats.consecutive_failures = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                if self._stats.consecutive_successes >= self.success_threshold:\n                    logger.info(f'Circuit {self.name}: HALF_OPEN -> CLOSED')\n                    self._state = CircuitState.CLOSED\n                    self._stats.consecutive_successes = 0\n    \n    async def _on_failure(self, error: Exception):\n        async with self._lock:\n            self._stats.failures += 1\n            self._stats.last_failure_time = time.time()\n            self._stats.consecutive_failures += 1\n            self._stats.consecutive_successes = 0\n            \n            if self._state == CircuitState.HALF_OPEN:\n                logger.warning(f'Circuit {self.name}: HALF_OPEN -> OPEN (test failed)')\n                self._state = CircuitState.OPEN\n                self._opened_at = time.time()\n            \n            elif self._state == CircuitState.CLOSED:\n                if self._stats.consecutive_failures >= self.failure_threshold:\n                    logger.warning(f'Circuit {self.name}: CLOSED -> OPEN (threshold reached)')\n                    self._state = CircuitState.OPEN\n                    self._opened_at = time.time()\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        state = self.state  # Check with timeout logic\n        \n        if state == CircuitState.OPEN:\n            retry_after = self.timeout - (time.time() - self._opened_at)\n            raise CircuitBreakerOpen(self.name, retry_after)\n        \n        try:\n            if asyncio.iscoroutinefunction(func):\n                result = await func(*args, **kwargs)\n            else:\n                result = await asyncio.to_thread(func, *args, **kwargs)\n            \n            await self._on_success()\n            return result\n        \n        except self.exclude:\n            # These exceptions don't count as failures\n            raise\n        except self.exceptions as e:\n            await self._on_failure(e)\n            raise\n    \n    def get_stats(self) -> dict:\n        return {\n            'name': self.name,\n            'state': self.state.value,\n            'failures': self._stats.failures,\n            'successes': self._stats.successes,\n            'consecutive_failures': self._stats.consecutive_failures,\n            'consecutive_successes': self._stats.consecutive_successes\n        }\n    \n    async def reset(self):\n        async with self._lock:\n            self._state = CircuitState.CLOSED\n            self._stats = CircuitStats()\n            self._opened_at = None\n\n# Decorator version\ndef circuit_breaker(\n    name: str,\n    failure_threshold: int = 5,\n    success_threshold: int = 3,\n    timeout: float = 30.0,\n    exceptions: tuple = (Exception,)\n) -> Callable:\n    breaker = CircuitBreaker(\n        name=name,\n        failure_threshold=failure_threshold,\n        success_threshold=success_threshold,\n        timeout=timeout,\n        exceptions=exceptions\n    )\n    \n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await breaker.call(func, *args, **kwargs)\n        \n        wrapper.circuit_breaker = breaker\n        return wrapper\n    \n    return decorator\n\n# Usage examples:\n\n# As decorator\n@circuit_breaker('external_api', failure_threshold=3, timeout=60)\nasync def call_external_api(endpoint: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f'https://api.example.com/{endpoint}')\n        response.raise_for_status()\n        return response.json()\n\n# Manual usage\napi_breaker = CircuitBreaker('payment_gateway', failure_threshold=2, timeout=120)\n\nasync def process_payment(amount: float):\n    try:\n        result = await api_breaker.call(payment_api.charge, amount)\n        return result\n    except CircuitBreakerOpen as e:\n        # Handle gracefully - maybe queue for later\n        logger.warning(f'Payment service unavailable: {e}')\n        return {'status': 'queued', 'retry_after': e.retry_after}\n\n# Check stats\nprint(call_external_api.circuit_breaker.get_stats())\n```\n\nFeatures:\n1. Three states: CLOSED (normal), OPEN (failing), HALF_OPEN (testing)\n2. Configurable failure/success thresholds\n3. Automatic timeout and recovery testing\n4. Exception filtering (track specific errors)\n5. Both decorator and manual usage patterns\n6. Statistics tracking\n7. Async-safe with locks\n8. Clear error messages with retry timing"}
